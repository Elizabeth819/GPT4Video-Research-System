#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆGPT-4.1è„šæœ¬ - ä»…ä¿®æ”¹promptä»¥å‡å°‘è¯¯æŠ¥
ä¸“é—¨ç”¨äºæµ‹è¯•images_5_054
"""

import re
from functools import partial
import multiprocessing
import datetime
import traceback
import tqdm
import numpy as np
from jinja2 import Environment, FileSystemLoader
import video_utilities as vu
from dotenv import load_dotenv
from functools import wraps
import time
import cv2
import os
import base64
import requests
from moviepy.editor import VideoFileClip
import logging
import json
import sys
import openai
import threading
from retrying import retry
logging.getLogger('moviepy').setLevel(logging.ERROR)

# å…¨å±€å˜é‡ç”¨äºå¤šè¿›ç¨‹å­˜å‚¨å½“å‰è¿›ç¨‹ç¼–å·
CURRENT_PROCESS_ID = 0

# è·å–å½“å‰è¿›ç¨‹ä¸“ç”¨çš„å¸§ç›®å½•
def get_process_frame_dir(process_id=None):
    if process_id is None:
        process_id = CURRENT_PROCESS_ID
    return f'frames_process_{process_id}'

# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
def initialize_logger(log_level='INFO'):
    logger = logging.getLogger('video_processor')
    logger.setLevel(getattr(logging, log_level.upper()))
    
    console_handler = logging.StreamHandler()
    console_handler.setLevel(getattr(logging, log_level.upper()))
    
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    
    logger.addHandler(console_handler)
    
    return logger

# è®¾ç½®è¯¦ç»†æ—¥å¿—è®°å½•å™¨
detailed_logger = initialize_logger('INFO')

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®Azure OpenAI API
azure_speech_key = os.environ["AZURE_SPEECH_KEY"]
azure_whisper_key = os.environ["AZURE_WHISPER_KEY"]
azure_whisper_deployment = os.environ["AZURE_WHISPER_DEPLOYMENT"]
azure_whisper_endpoint = os.environ["AZURE_WHISPER_ENDPOINT"]

# Audio API type (OpenAI, Azure)*
audio_api_type = os.environ["AUDIO_API_TYPE"]

# GPT4 vision APi type (OpenAI, Azure)*
vision_api_type = os.environ["VISION_API_TYPE"]

# OpenAI API Key*
openai_api_key = os.environ["OPENAI_API_KEY"]

# GPT-4.1 Azure vision API Deployment Name*
vision_deployment = os.environ.get("VISION_ENDPOINT_4.1", "gpt-4.1")

# GPT endpoint  
vision_endpoint = os.environ["VISION_ENDPOINT"]

def log_execution_time(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            end_time = time.time()
            execution_time = end_time - start_time
            detailed_logger.info(f"å‡½æ•° {func.__name__} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            return result
        except Exception as e:
            end_time = time.time()
            execution_time = end_time - start_time
            detailed_logger.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥ï¼Œè€—æ—¶: {execution_time:.2f}ç§’ï¼Œé”™è¯¯: {str(e)}")
            raise
    return wrapper

class Spinner:
    def __init__(self, message="å¤„ç†ä¸­..."):
        self.message = message
        self.spinner_chars = "|/-\\"
        self.stop_spinner = False
        self.thread = None
        self.logger = detailed_logger

    def spin(self):
        i = 0
        while not self.stop_spinner:
            sys.stdout.write(
                f'\r{self.message} {self.spinner_chars[i % len(self.spinner_chars)]}')
            sys.stdout.flush()
            time.sleep(0.1)
            i += 1

    def start(self):
        self.stop_spinner = False
        self.thread = threading.Thread(target=self.spin)
        self.thread.daemon = True
        self.thread.start()
        if self.logger:
            self.logger.debug(f"Spinner thread started for: {self.message}")

    def stop(self):
        self.stop_spinner = True
        try:
            if self.thread.is_alive():
                self.thread.join(timeout=1.0)
            sys.stdout.write(
                '\r' + ' '*(len(self.message)+2) + '\r')
            sys.stdout.flush()
            if self.logger:
                self.logger.debug(f"Spinner stopped: {self.message}")
        except Exception as e:
            if self.logger:
                self.logger.error(f"Error stopping spinner: {str(e)}")
            try:
                sys.stdout.write('\r' + ' '*(len(self.message)+2) + '\r')
                sys.stdout.flush()
            except:
                pass

chapter_summary = {}
miss_arr = []

def extract_video_id(filename, logger=None):
    if logger:
        logger.debug(f"æå–è§†é¢‘IDï¼Œæ–‡ä»¶å: {filename}")

    try:
        if not filename or not isinstance(filename, str):
            error_msg = f"æ— æ•ˆçš„æ–‡ä»¶å: {filename}"
            if logger:
                logger.error(error_msg)
            return f"unknown_{int(time.time())}"

        dada_match = re.match(r"images_(\\d+)_(\\d+)\\.avi", filename)
        if dada_match:
            video_id = f"dada_{dada_match.group(1)}_{dada_match.group(2)}"
            if logger:
                logger.debug(f"åŒ¹é…DADAæ ¼å¼æˆåŠŸ: {video_id}")
            return video_id

        num_prefix_match = re.match(r"(\\d+)_", filename)
        if num_prefix_match:
            video_id = f"vid_{num_prefix_match.group(1)}"
            if logger:
                logger.debug(f"åŒ¹é…æ•°å­—å‰ç¼€æ ¼å¼æˆåŠŸ: {video_id}")
            return video_id

        base_name = os.path.splitext(filename)[0]
        video_id = re.sub(r'[^\\w-]', '_', base_name)
        if logger:
            logger.debug(f"ä½¿ç”¨é€šç”¨å¤‡é€‰æ–¹æ¡ˆ: {video_id}")
        return video_id
    except Exception as e:
        error_msg = f"æå–è§†é¢‘IDæ—¶å‡ºé”™: {str(e)}, å›é€€åˆ°å®‰å…¨ID"
        if logger:
            logger.error(error_msg)
            logger.error(traceback.format_exc())

        safe_id = f"unknown_{int(time.time())}_{hash(filename) % 10000 if filename else 0}"
        return safe_id

@log_execution_time
def AnalyzeVideo(vp, fi, fpi, speed_mode=False, output_dir='result/gpt41-improved'):
    video_filename = os.path.basename(vp)
    video_id = extract_video_id(video_filename)
    print(f"å¤„ç†è§†é¢‘: {video_filename}, è§†é¢‘ID: {video_id}")

    global CURRENT_PROCESS_ID
    if multiprocessing.current_process().name != 'MainProcess':
        frames_dir = get_process_frame_dir()
    else:
        frames_dir = 'frames'

    if not os.path.exists(frames_dir):
        os.makedirs(frames_dir)

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    video_clip = VideoFileClip(vp)
    duration = video_clip.duration
    detailed_logger.info(f"è§†é¢‘æ—¶é•¿: {duration:.2f}ç§’")

    total_intervals = int(duration / fi)
    if duration % fi > 0:
        total_intervals += 1

    detailed_logger.info(f"å°†åˆ›å»º {total_intervals} ä¸ªé—´éš”ï¼Œæ¯é—´éš” {fi} ç§’ï¼Œæ¯é—´éš” {fpi} å¸§")

    progress_bar = tqdm.tqdm(total=total_intervals, desc="å¤„ç†è§†é¢‘é—´éš”", unit="interval")
    all_segments = []

    for interval_index in range(total_intervals):
        segment_id = f"segment_{interval_index:03d}"
        start_time = interval_index * fi
        end_time = min(start_time + fi, duration)
        
        detailed_logger.info(f"å¤„ç†é—´éš” {interval_index + 1}/{total_intervals}: {start_time:.1f}s - {end_time:.1f}s")

        packet = []
        for frame_index in range(fpi):
            frame_time = start_time + (frame_index * (end_time - start_time) / fpi)
            if frame_time >= duration:
                break
            
            frame_filename = f"{frames_dir}/frame_at_{frame_time:.1f}s.jpg"
            
            frame = video_clip.get_frame(frame_time)
            cv2.imwrite(frame_filename, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
            packet.append(frame_filename)
            detailed_logger.debug(f"æå–å¸§: {frame_filename}")

        if not packet:
            detailed_logger.warning(f"é—´éš” {segment_id} æ²¡æœ‰æå–åˆ°å¸§ï¼Œè·³è¿‡")
            continue

        current_transcription = ""
        if video_clip.audio is not None:
            spinner = Spinner(f"æ­£åœ¨è½¬å½•é—´éš” {segment_id} çš„éŸ³é¢‘...")
            spinner.start()
            
            try:
                audio_filename = f"{frames_dir}/audio_segment_{segment_id}.wav"
                audio_clip = video_clip.subclip(start_time, end_time)
                audio_clip.audio.write_audiofile(audio_filename, verbose=False, logger=None)
                
                tscribe = transcribe_audio(audio_filename, azure_whisper_key, azure_whisper_deployment, azure_whisper_endpoint)
                current_transcription = tscribe
                
                if os.path.exists(audio_filename):
                    os.remove(audio_filename)
                    
            except Exception as e:
                detailed_logger.error(f"éŸ³é¢‘è½¬å½•å¤±è´¥: {str(e)}")
                current_transcription = ""
            finally:
                spinner.stop()
        else:
            print("è§†é¢‘æ²¡æœ‰éŸ³è½¨ï¼Œè·³è¿‡éŸ³é¢‘æå–å’Œè½¬å½•")

        spinner = Spinner(f"æ­£åœ¨åˆ†æé—´éš” {segment_id} çš„ {len(packet)} å¸§å›¾åƒ...")
        spinner.start()

        vision_response = gpt41_vision_analysis_improved(
            packet, openai_api_key, "", current_transcription, video_id, segment_id, speed_mode, start_time, end_time)

        spinner.stop()

        if vision_response == -1:
            print(f"è­¦å‘Š: é—´éš” {segment_id} çš„è§†è§‰åˆ†æå¤±è´¥ï¼Œè·³è¿‡")
            continue

        try:
            segment_data = json.loads(vision_response)
            if isinstance(segment_data, dict):
                if 'Start_Timestamp' not in segment_data:
                    segment_data['Start_Timestamp'] = f"{start_time:.1f}s"
                if 'End_Timestamp' not in segment_data:
                    segment_data['End_Timestamp'] = f"{end_time:.1f}s"
                all_segments.append(segment_data)
            else:
                detailed_logger.warning(f"é—´éš” {segment_id} è¿”å›äº†æ— æ•ˆçš„JSONæ ¼å¼")
        except json.JSONDecodeError as e:
            detailed_logger.error(f"è§£æé—´éš” {segment_id} çš„JSONå“åº”å¤±è´¥: {str(e)}")
            detailed_logger.error(f"åŸå§‹å“åº”: {vision_response[:200]}...")
            continue

        for frame_path in packet:
            if os.path.exists(frame_path):
                os.remove(frame_path)

        progress_bar.update(1)

    progress_bar.close()
    video_clip.close()

    result_filename = f'actionSummary_{video_filename.split(".")[0]}.json'
    result_path = os.path.join(output_dir, result_filename)
    
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(all_segments, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {result_path}")
    return result_path

def transcribe_audio(audio_path, api_key, deployment, endpoint):
    try:
        url = f"{endpoint}/openai/deployments/{deployment}/audio/transcriptions?api-version=2024-02-01"
        headers = {
            "api-key": api_key,
        }
        
        with open(audio_path, 'rb') as audio_file:
            files = {
                'file': audio_file,
            }
            data = {
                'model': 'whisper-1',
                'response_format': 'text'
            }
            
            response = requests.post(url, headers=headers, files=files, data=data)
            
            if response.status_code == 200:
                return response.text.strip()
            else:
                detailed_logger.error(f"éŸ³é¢‘è½¬å½•å¤±è´¥: {response.status_code} - {response.text}")
                return ""
                
    except Exception as e:
        detailed_logger.error(f"éŸ³é¢‘è½¬å½•å¼‚å¸¸: {str(e)}")
        return ""

def retry_if_connection_error(exception):
    is_connection_error = isinstance(exception, (requests.exceptions.ConnectionError,
                                                 requests.exceptions.Timeout))
    if is_connection_error:
        detailed_logger.warning(f"APIè¿æ¥é”™è¯¯ï¼Œå°†é‡è¯•: {str(exception)}")
    return is_connection_error

# æ”¹è¿›ç‰ˆGPT-4.1 vision analysis function - ä»…ä¿®æ”¹prompt
@retry(stop_max_attempt_number=2, wait_exponential_multiplier=2000, wait_exponential_max=60000,
       retry_on_exception=retry_if_connection_error)
def gpt41_vision_analysis_improved(image_path, api_key, summary, trans, video_id, segment_id=None, speed_mode=False, start_time=0, end_time=10):
    detailed_logger.info(f"å¼€å§‹æ”¹è¿›ç‰ˆGPT-4.1è§†è§‰åˆ†æ, è§†é¢‘ID: {video_id}, æ®µè½ID: {segment_id}, å›¾åƒæ•°é‡: {len(image_path)}")
    
    encoded_images = []
    for path in image_path:
        with open(path, 'rb') as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
            encoded_images.append(encoded_string)
    
    frame_interval = 10
    frames_per_interval = len(image_path)
    
    if vision_api_type == "Azure":
        detailed_logger.info("ä½¿ç”¨æ”¹è¿›ç‰ˆAzure OpenAI GPT-4.1è¿›è¡Œè§†è§‰åˆ†æ")
        segment_id_str = segment_id if segment_id else f"Segment_{0:03d}"
        
        # ğŸ”§ æ”¹è¿›ç‰ˆPROMPT - å‡å°‘è¯¯æŠ¥
        system_content = f"""You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

Your job is to take in as an input a transcription of {frame_interval} seconds of audio from a video,
as well as {frames_per_interval} frames split evenly throughout {frame_interval} seconds.
You are to generate and provide a Current Action Summary of the video you are considering ({frames_per_interval}
frames over {frame_interval} seconds), which is generated from your analysis of each frame ({frames_per_interval} in total),
as well as the in-between audio, until we have a full action summary of the video.

IMPORTANT: For GENUINE ghost probing detection, ALL of the following criteria must be met:

1. **ENVIRONMENT CHECK**:
   - HIGH-RISK environments: Highways, rural roads, parking lots, residential streets without crosswalks
   - LOW-RISK environments: Intersections, crosswalks, traffic lights, busy urban areas (normal pedestrian/vehicle behavior expected here)

2. **PROXIMITY & TIMING**:
   - Object appears EXTREMELY close (within 1-2 vehicle lengths, <5 meters)
   - Appearance is INSTANTANEOUS (not gradual approach over multiple frames)
   - Requires IMMEDIATE emergency braking/swerving to avoid collision

3. **BEHAVIOR PATTERN**:
   - Object emerges from TRUE blind spots (not visible in any previous frames)
   - Movement is UNPREDICTABLE and violates traffic norms
   - Creates UNAVOIDABLE emergency situation

4. **STRICT EXCLUSIONS** (DO NOT mark as ghost probing):
   - Pedestrians crossing at intersections, crosswalks, or traffic lights
   - Vehicles making normal lane changes, turns, or merging with signals
   - Cyclists following traffic patterns in urban areas or bike lanes
   - Any movement that is PREDICTABLE or EXPECTED given the traffic environment
   - Gradual approach of any object (must be instantaneous appearance)

5. **FINAL CONFIRMATION**:
   Only use "ghost probing" if this creates a TRUE life-threatening emergency that violates all traffic expectations.

Use "ghost probing" in key_actions ONLY when ALL above criteria are satisfied.
For other traffic situations, use descriptive terms like:
- "emergency braking due to pedestrian crossing"
- "evasive action for vehicle maneuver"
- "sudden cyclist appearance requiring caution"
- "unexpected pedestrian movement"

Your response should be a valid JSON object with the following EXACT structure (match this format precisely):
{{
    "video_id": "{video_id}",
    "segment_id": "{segment_id_str}",
    "Start_Timestamp": "{start_time:.1f}s",
    "End_Timestamp": "{end_time:.1f}s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene",
    "summary": "comprehensive summary of the scene and what happens",
    "actions": "actions taken by the vehicle and driver responses",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "brief description of most important actions (use 'ghost probing' ONLY if ALL criteria above are met)",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

Audio Transcription: {trans}
"""
        
        content = [{"type": "text", "text": system_content}]
        
        for encoded_image in encoded_images:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}"
                }
            })
        
        data = {
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ],
            "max_tokens": 2000,
            "temperature": 0.3
        }
        
        response = send_post_request(vision_endpoint, 
                                   vision_deployment, 
                                   openai_api_key, 
                                   data)
        return response
    
    else:  # OpenAI API
        detailed_logger.info("ä½¿ç”¨æ”¹è¿›ç‰ˆOpenAI GPT-4.1è¿›è¡Œè§†è§‰åˆ†æ")
        
        content = [
            {
                "type": "text", 
                "text": f"""You are VideoAnalyzerGPT. Your job is to take in as an input a transcription of {frame_interval} seconds of audio from a video,
as well as {frames_per_interval} frames split evenly throughout {frame_interval} seconds.

IMPORTANT: For GENUINE ghost probing detection, ALL of the following criteria must be met:

1. **ENVIRONMENT CHECK**:
   - HIGH-RISK environments: Highways, rural roads, parking lots
   - LOW-RISK environments: Intersections, crosswalks, traffic lights (normal behavior expected)

2. **PROXIMITY & TIMING**:
   - Object appears EXTREMELY close (within 1-2 vehicle lengths)
   - Appearance is INSTANTANEOUS (not gradual approach)
   - Requires IMMEDIATE emergency braking/swerving

3. **STRICT EXCLUSIONS** (DO NOT mark as ghost probing):
   - Pedestrians crossing at intersections/crosswalks
   - Vehicles making normal lane changes or turns
   - Cyclists following traffic patterns
   - Any PREDICTABLE movement given the environment

Use "ghost probing" ONLY when ALL criteria are met.

Your response should be a valid JSON object with the following EXACT structure:
{{
    "video_id": "{video_id}",
    "segment_id": "{segment_id}",
    "Start_Timestamp": "{start_time:.1f}s",
    "End_Timestamp": "{end_time:.1f}s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene",
    "summary": "comprehensive summary of the scene and what happens",
    "actions": "actions taken by the vehicle and driver responses",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "brief description of most important actions (use 'ghost probing' ONLY if ALL criteria above are met)",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

Audio Transcription: {trans}
"""
            }
        ]
        
        for encoded_image in encoded_images:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}"
                }
            })
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        payload = {
            "model": "gpt-4.1",
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ],
            "max_tokens": 2000,
            "temperature": 0.3
        }
        
        response = requests.post("https://api.openai.com/v1/chat/completions", 
                               headers=headers, 
                               json=payload)
        
        if response.status_code == 200:
            response_data = response.json()
            return response_data['choices'][0]['message']['content']
        else:
            detailed_logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return -1

def send_post_request(endpoint, deployment_name, api_key, data):
    if not endpoint.startswith('https://'):
        endpoint = f"https://{endpoint}.openai.azure.com"
    
    url = f"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-02-15-preview"
    headers = {
        "Content-Type": "application/json",
        "api-key": api_key
    }
    
    try:
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            response_data = response.json()
            return response_data['choices'][0]['message']['content']
        else:
            detailed_logger.error(f"Azure APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            return -1
    except Exception as e:
        detailed_logger.error(f"å‘é€è¯·æ±‚æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        return -1

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='æ”¹è¿›ç‰ˆGPT-4.1è§†é¢‘åˆ†æ - å‡å°‘è¯¯æŠ¥')
    parser.add_argument('--single', type=str, help='å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶')
    parser.add_argument('--interval', type=int, default=10, help='å¸§é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--frames', type=int, default=10, help='æ¯é—´éš”çš„å¸§æ•°')
    parser.add_argument('--output-dir', type=str, default='result/gpt41-improved', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    if args.single:
        print(f"ğŸ§ª æµ‹è¯•æ”¹è¿›ç‰ˆGPT-4.1 prompt")
        print(f"ğŸ“ è§†é¢‘: {args.single}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output_dir}")
        print("=" * 50)
        
        result = AnalyzeVideo(args.single, args.interval, args.frames, False, args.output_dir)
        if result:
            print(f"âœ… æ”¹è¿›ç‰ˆå¤„ç†æˆåŠŸ: {result}")
        else:
            print("âŒ æ”¹è¿›ç‰ˆå¤„ç†å¤±è´¥")

if __name__ == "__main__":
    main()