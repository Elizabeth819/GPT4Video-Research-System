#!/usr/bin/env python3
"""
è®¡ç®—GPT-4oå’ŒGeminiçš„ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1ç­‰ç»Ÿè®¡æŒ‡æ ‡
"""

import json
import os
import pandas as pd
import numpy as np
import re
from datetime import datetime
from collections import defaultdict
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score
import csv

class PerformanceCalculator:
    def __init__(self):
        self.ground_truth_path = "result/groundtruth_labels.csv"
        self.gpt4o_dir = "result/gpt-4o"
        self.gemini_dir = "result/gemini-testinterval"
        self.output_dir = "result/comparison"
        
        self.ground_truth = {}
        self.gpt4o_predictions = {}
        self.gemini_predictions = {}
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
    
    def load_ground_truth(self):
        """åŠ è½½ground truthæ ‡ç­¾"""
        print("ğŸ“Š åŠ è½½ground truthæ ‡ç­¾...")
        
        if not os.path.exists(self.ground_truth_path):
            print(f"âŒ Ground truthæ–‡ä»¶ä¸å­˜åœ¨: {self.ground_truth_path}")
            return False
        
        with open(self.ground_truth_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f, delimiter='\t')
            for row in reader:
                video_id = row['video_id'].replace('.avi', '')
                label = row['ground_truth_label']
                
                # è§£ææ ‡ç­¾ï¼šæå–æ˜¯å¦åŒ…å«ghost probing
                if 'ghost probing' in label.lower():
                    self.ground_truth[video_id] = 1  # æ­£ä¾‹ï¼šåŒ…å«ghost probing
                else:
                    self.ground_truth[video_id] = 0  # è´Ÿä¾‹ï¼šä¸åŒ…å«ghost probing
        
        print(f"âœ… åŠ è½½äº† {len(self.ground_truth)} ä¸ªground truthæ ‡ç­¾")
        
        # ç»Ÿè®¡æ­£è´Ÿæ ·æœ¬
        positive_count = sum(1 for v in self.ground_truth.values() if v == 1)
        negative_count = len(self.ground_truth) - positive_count
        print(f"   æ­£æ ·æœ¬(ghost probing): {positive_count}")
        print(f"   è´Ÿæ ·æœ¬(normal): {negative_count}")
        
        return True
    
    def extract_ghost_probing_prediction(self, result_data, video_name):
        """ä»æ¨¡å‹ç»“æœä¸­æå–ghost probingé¢„æµ‹"""
        if not isinstance(result_data, list):
            return 0
        
        # æ£€æŸ¥æ‰€æœ‰æ®µè½çš„åˆ†æç»“æœ
        for segment in result_data:
            if not isinstance(segment, dict):
                continue
                
            # æ£€æŸ¥å¤šä¸ªå­—æ®µä¸­æ˜¯å¦æåˆ°ghost probingç›¸å…³å†…å®¹
            text_fields = []
            for field in ['summary', 'actions', 'key_actions', 'next_action', 'key_objects']:
                if field in segment and segment[field]:
                    text_fields.append(str(segment[field]).lower())
            
            combined_text = ' '.join(text_fields)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ghost probingç›¸å…³å…³é”®è¯
            ghost_keywords = [
                'ghost probing', 'ghost', 'probing', 
                'sudden appearance', 'unexpected', 'emerging',
                'appearing suddenly', 'cuts in', 'cut in',
                'overtaking', 'lane change', 'dangerous',
                'risky maneuver', 'unsafe'
            ]
            
            for keyword in ghost_keywords:
                if keyword in combined_text:
                    return 1  # é¢„æµ‹ä¸ºæ­£ä¾‹
        
        return 0  # é¢„æµ‹ä¸ºè´Ÿä¾‹
    
    def load_model_predictions(self, model_dir, model_name):
        """åŠ è½½æ¨¡å‹é¢„æµ‹ç»“æœ"""
        print(f"ğŸ“Š åŠ è½½{model_name}é¢„æµ‹ç»“æœ...")
        
        predictions = {}
        processed_count = 0
        
        for video_name in self.ground_truth.keys():
            result_file = os.path.join(model_dir, f"actionSummary_{video_name}.json")
            
            if os.path.exists(result_file):
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result_data = json.load(f)
                        predictions[video_name] = self.extract_ghost_probing_prediction(result_data, video_name)
                        processed_count += 1
                except Exception as e:
                    print(f"âŒ åŠ è½½{model_name}ç»“æœå¤±è´¥: {video_name} - {e}")
                    predictions[video_name] = 0  # é»˜è®¤ä¸ºè´Ÿä¾‹
            else:
                print(f"âš ï¸  {model_name}ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {video_name}")
                predictions[video_name] = 0  # é»˜è®¤ä¸ºè´Ÿä¾‹
        
        print(f"âœ… {model_name}å¤„ç†äº† {processed_count}/{len(self.ground_truth)} ä¸ªè§†é¢‘")
        
        # ç»Ÿè®¡é¢„æµ‹ç»“æœ
        positive_pred = sum(1 for v in predictions.values() if v == 1)
        negative_pred = len(predictions) - positive_pred
        print(f"   é¢„æµ‹æ­£ä¾‹: {positive_pred}")
        print(f"   é¢„æµ‹è´Ÿä¾‹: {negative_pred}")
        
        return predictions
    
    def calculate_metrics(self, y_true, y_pred, model_name):
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        print(f"ğŸ“ˆ è®¡ç®—{model_name}æ€§èƒ½æŒ‡æ ‡...")
        
        # åŸºæœ¬æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # ç‰¹å¼‚æ€§ï¼ˆSpecificityï¼‰
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        # å¹³è¡¡å‡†ç¡®ç‡ï¼ˆBalanced Accuracyï¼‰
        balanced_accuracy = (recall + specificity) / 2
        
        metrics = {
            'model': model_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'specificity': specificity,
            'balanced_accuracy': balanced_accuracy,
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'total_samples': len(y_true)
        }
        
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   ç²¾ç¡®åº¦: {precision:.4f}")
        print(f"   å¬å›ç‡: {recall:.4f}")
        print(f"   F1åˆ†æ•°: {f1:.4f}")
        print(f"   ç‰¹å¼‚æ€§: {specificity:.4f}")
        print(f"   å¹³è¡¡å‡†ç¡®ç‡: {balanced_accuracy:.4f}")
        print(f"   æ··æ·†çŸ©é˜µ: TP={tp}, TN={tn}, FP={fp}, FN={fn}")
        
        return metrics
    
    def create_detailed_analysis(self, gpt4o_metrics, gemini_metrics):
        """åˆ›å»ºè¯¦ç»†åˆ†æ"""
        print("ğŸ“‹ åˆ›å»ºè¯¦ç»†æ€§èƒ½åˆ†æ...")
        
        # å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹
        comparison = {}
        
        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'balanced_accuracy']:
            gpt4o_val = gpt4o_metrics[metric]
            gemini_val = gemini_metrics[metric]
            
            comparison[metric] = {
                'gpt4o': gpt4o_val,
                'gemini': gemini_val,
                'difference': gemini_val - gpt4o_val,
                'percentage_change': ((gemini_val - gpt4o_val) / gpt4o_val * 100) if gpt4o_val > 0 else float('inf')
            }
        
        # æ‰¾å‡ºè¡¨ç°æ›´å¥½çš„æ¨¡å‹
        better_model_count = {
            'gpt4o': 0,
            'gemini': 0,
            'tie': 0
        }
        
        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'balanced_accuracy']:
            gpt4o_val = gpt4o_metrics[metric]
            gemini_val = gemini_metrics[metric]
            
            if gpt4o_val > gemini_val:
                better_model_count['gpt4o'] += 1
            elif gemini_val > gpt4o_val:
                better_model_count['gemini'] += 1
            else:
                better_model_count['tie'] += 1
        
        return comparison, better_model_count
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        # åŠ è½½æ•°æ®
        if not self.load_ground_truth():
            return None
        
        self.gpt4o_predictions = self.load_model_predictions(self.gpt4o_dir, "GPT-4o")
        self.gemini_predictions = self.load_model_predictions(self.gemini_dir, "Gemini")
        
        # å‡†å¤‡æ•°æ®
        video_names = list(self.ground_truth.keys())
        y_true = [self.ground_truth[name] for name in video_names]
        y_pred_gpt4o = [self.gpt4o_predictions[name] for name in video_names]
        y_pred_gemini = [self.gemini_predictions[name] for name in video_names]
        
        # è®¡ç®—æŒ‡æ ‡
        gpt4o_metrics = self.calculate_metrics(y_true, y_pred_gpt4o, "GPT-4o")
        gemini_metrics = self.calculate_metrics(y_true, y_pred_gemini, "Gemini")
        
        # è¯¦ç»†åˆ†æ
        comparison, better_model_count = self.create_detailed_analysis(gpt4o_metrics, gemini_metrics)
        
        # ç”ŸæˆæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        performance_report = {
            "analysis_timestamp": datetime.now().isoformat(),
            "dataset_info": {
                "ground_truth_file": self.ground_truth_path,
                "total_videos": len(self.ground_truth),
                "positive_samples": sum(y_true),
                "negative_samples": len(y_true) - sum(y_true)
            },
            "gpt4o_metrics": gpt4o_metrics,
            "gemini_metrics": gemini_metrics,
            "comparison": comparison,
            "model_comparison_summary": better_model_count
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.output_dir, f"performance_metrics_{timestamp}.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(performance_report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return performance_report
    
    def print_summary(self, report):
        """æ‰“å°æ€§èƒ½æ€»ç»“"""
        print("\n" + "="*80)
        print("ğŸ“Š GPT-4o vs Gemini æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”æ€»ç»“")
        print("="*80)
        
        gpt4o = report["gpt4o_metrics"]
        gemini = report["gemini_metrics"]
        comparison = report["comparison"]
        
        print(f"ğŸ“ æ•°æ®é›†ä¿¡æ¯:")
        print(f"   æ€»è§†é¢‘æ•°: {report['dataset_info']['total_videos']}")
        print(f"   æ­£æ ·æœ¬æ•°: {report['dataset_info']['positive_samples']}")
        print(f"   è´Ÿæ ·æœ¬æ•°: {report['dataset_info']['negative_samples']}")
        
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<15} {'GPT-4o':<10} {'Gemini':<10} {'å·®å€¼':<10} {'æå‡%':<10}")
        print("-" * 65)
        
        for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'balanced_accuracy']:
            gpt4o_val = gpt4o[metric]
            gemini_val = gemini[metric]
            diff = comparison[metric]['difference']
            pct_change = comparison[metric]['percentage_change']
            
            pct_str = f"{pct_change:+.1f}%" if abs(pct_change) != float('inf') else "N/A"
            
            print(f"{metric:<15} {gpt4o_val:<10.4f} {gemini_val:<10.4f} {diff:<+10.4f} {pct_str:<10}")
        
        print(f"\nğŸ¯ æ··æ·†çŸ©é˜µå¯¹æ¯”:")
        print(f"GPT-4o: TP={gpt4o['true_positives']}, TN={gpt4o['true_negatives']}, FP={gpt4o['false_positives']}, FN={gpt4o['false_negatives']}")
        print(f"Gemini: TP={gemini['true_positives']}, TN={gemini['true_negatives']}, FP={gemini['false_positives']}, FN={gemini['false_negatives']}")
        
        summary = report["model_comparison_summary"]
        print(f"\nğŸ† æ€»ä½“è¡¨ç°:")
        print(f"   GPT-4oä¼˜åŠ¿æŒ‡æ ‡: {summary['gpt4o']}/6")
        print(f"   Geminiä¼˜åŠ¿æŒ‡æ ‡: {summary['gemini']}/6")
        print(f"   å¹³å±€æŒ‡æ ‡: {summary['tie']}/6")
        
        if summary['gpt4o'] > summary['gemini']:
            print(f"   ğŸ¥‡ æ•´ä½“è¡¨ç°æ›´å¥½: GPT-4o")
        elif summary['gemini'] > summary['gpt4o']:
            print(f"   ğŸ¥‡ æ•´ä½“è¡¨ç°æ›´å¥½: Gemini")
        else:
            print(f"   ğŸ¤ æ•´ä½“è¡¨ç°: å¹³å±€")
        
        print("\n" + "="*80)

def main():
    calculator = PerformanceCalculator()
    report = calculator.run_analysis()
    
    if report:
        calculator.print_summary(report)
    else:
        print("âŒ æ€§èƒ½åˆ†æå¤±è´¥")

if __name__ == "__main__":
    main()