#!/usr/bin/env python3
"""
è¯„ä¼°å¹³è¡¡ç‰ˆGPT-4.1çš„è¿›å±• - å¯¹æ¯”ä¸‰ä¸ªç‰ˆæœ¬çš„æ€§èƒ½
"""

import os
import json
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
import numpy as np

def load_ground_truth():
    """åŠ è½½Ground Truthæ ‡ç­¾"""
    labels_file = "result/groundtruth_labels.csv"
    df = pd.read_csv(labels_file, sep='\t')
    
    # è§£ææ ‡ç­¾
    ground_truth = {}
    for _, row in df.iterrows():
        video_id = row['video_id'].replace('.avi', '')
        label = row['ground_truth_label']
        
        # è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ‡ç­¾
        has_ghost_probing = 0 if label == 'none' else 1
        ground_truth[video_id] = has_ghost_probing
    
    return ground_truth

def extract_ghost_probing_from_result(result_file):
    """ä»ç»“æœæ–‡ä»¶ä¸­æå–æ˜¯å¦åŒ…å«ghost probing"""
    try:
        with open(result_file, 'r', encoding='utf-8') as f:
            segments = json.load(f)
        
        # æ£€æŸ¥æ‰€æœ‰æ®µè½ä¸­æ˜¯å¦æœ‰ghost probingæˆ–potential ghost probing
        for segment in segments:
            if isinstance(segment, dict):
                key_actions = segment.get('key_actions', '').lower()
                if 'ghost probing' in key_actions:  # åŒ…æ‹¬ "ghost probing" å’Œ "potential ghost probing"
                    return 1
        return 0
    except Exception as e:
        print(f"âŒ è§£ææ–‡ä»¶å¤±è´¥: {result_file}, é”™è¯¯: {str(e)}")
        return 0

def evaluate_three_models(original_dir, improved_dir, balanced_dir, ground_truth):
    """å¯¹æ¯”ä¸‰ä¸ªç‰ˆæœ¬æ¨¡å‹çš„æ€§èƒ½"""
    
    # æ‰¾åˆ°ä¸‰ä¸ªç›®å½•éƒ½æœ‰çš„è§†é¢‘
    original_files = set(f.replace('actionSummary_', '').replace('.json', '') 
                        for f in os.listdir(original_dir) if f.endswith('.json'))
    improved_files = set(f.replace('actionSummary_', '').replace('.json', '') 
                        for f in os.listdir(improved_dir) if f.endswith('.json'))
    balanced_files = set(f.replace('actionSummary_', '').replace('.json', '') 
                        for f in os.listdir(balanced_dir) if f.endswith('.json'))
    
    # æ‰¾åˆ°æ‰€æœ‰ä¸‰ä¸ªç‰ˆæœ¬éƒ½å¤„ç†è¿‡çš„è§†é¢‘
    common_videos = original_files.intersection(improved_files).intersection(balanced_files)
    
    # åªè¯„ä¼°æœ‰Ground Truthæ ‡ç­¾çš„è§†é¢‘
    valid_videos = [vid for vid in common_videos if vid in ground_truth]
    
    print(f"ğŸ“Š ä¸‰ä¸ªç‰ˆæœ¬éƒ½æœ‰çš„å¯è¯„ä¼°è§†é¢‘æ•°é‡: {len(valid_videos)}")
    
    if len(valid_videos) < 10:
        print("âš ï¸  å¯è¯„ä¼°è§†é¢‘æ•°é‡å¤ªå°‘ï¼Œç»“æœå¯èƒ½ä¸å…·æœ‰ä»£è¡¨æ€§")
    
    # æå–é¢„æµ‹ç»“æœ
    original_predictions = []
    improved_predictions = []
    balanced_predictions = []
    true_labels = []
    
    detailed_results = []
    
    for video_id in valid_videos:
        original_file = os.path.join(original_dir, f"actionSummary_{video_id}.json")
        improved_file = os.path.join(improved_dir, f"actionSummary_{video_id}.json")
        balanced_file = os.path.join(balanced_dir, f"actionSummary_{video_id}.json")
        
        original_pred = extract_ghost_probing_from_result(original_file)
        improved_pred = extract_ghost_probing_from_result(improved_file)
        balanced_pred = extract_ghost_probing_from_result(balanced_file)
        true_label = ground_truth[video_id]
        
        original_predictions.append(original_pred)
        improved_predictions.append(improved_pred)
        balanced_predictions.append(balanced_pred)
        true_labels.append(true_label)
        
        # è®°å½•è¯¦ç»†ç»“æœ
        detailed_results.append({
            'video_id': video_id,
            'ground_truth': true_label,
            'original_pred': original_pred,
            'improved_pred': improved_pred,
            'balanced_pred': balanced_pred,
            'original_correct': original_pred == true_label,
            'improved_correct': improved_pred == true_label,
            'balanced_correct': balanced_pred == true_label
        })
    
    # è®¡ç®—æŒ‡æ ‡
    original_metrics = calculate_metrics(true_labels, original_predictions, "åŸç‰ˆGPT-4.1")
    improved_metrics = calculate_metrics(true_labels, improved_predictions, "æ”¹è¿›ç‰ˆGPT-4.1")
    balanced_metrics = calculate_metrics(true_labels, balanced_predictions, "å¹³è¡¡ç‰ˆGPT-4.1")
    
    return original_metrics, improved_metrics, balanced_metrics, detailed_results

def calculate_metrics(true_labels, predictions, model_name):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    if len(set(true_labels)) == 1:
        print(f"âš ï¸  {model_name}: æ‰€æœ‰çœŸå®æ ‡ç­¾éƒ½ç›¸åŒï¼ŒæŸäº›æŒ‡æ ‡å¯èƒ½ä¸å‡†ç¡®")
    
    metrics = {
        'model_name': model_name,
        'accuracy': accuracy_score(true_labels, predictions),
        'precision': precision_score(true_labels, predictions, zero_division=0),
        'recall': recall_score(true_labels, predictions, zero_division=0),
        'f1': f1_score(true_labels, predictions, zero_division=0)
    }
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(true_labels, predictions)
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
        metrics.update({
            'true_positives': tp,
            'false_positives': fp,
            'true_negatives': tn,
            'false_negatives': fn,
            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0
        })
    
    return metrics

def print_three_way_comparison(original_metrics, improved_metrics, balanced_metrics, detailed_results):
    """æ‰“å°ä¸‰ä¸ªç‰ˆæœ¬çš„å¯¹æ¯”ç»“æœ"""
    print("\n" + "=" * 100)
    print("ğŸ“Š GPT-4.1 ä¸‰ä¸ªç‰ˆæœ¬æ€§èƒ½å¯¹æ¯”")
    print("=" * 100)
    
    # æŒ‡æ ‡å¯¹æ¯”è¡¨
    metrics_names = ['accuracy', 'precision', 'recall', 'f1', 'specificity']
    
    print(f"\n{'æŒ‡æ ‡':<15} {'åŸç‰ˆGPT-4.1':<15} {'æ”¹è¿›ç‰ˆGPT-4.1':<15} {'å¹³è¡¡ç‰ˆGPT-4.1':<15} {'æœ€ä½³ç‰ˆæœ¬':<15}")
    print("-" * 85)
    
    best_counts = {'original': 0, 'improved': 0, 'balanced': 0}
    
    for metric in metrics_names:
        if metric in original_metrics and metric in improved_metrics and metric in balanced_metrics:
            original_val = original_metrics[metric]
            improved_val = improved_metrics[metric]
            balanced_val = balanced_metrics[metric]
            
            # æ‰¾å‡ºæœ€ä½³å€¼
            values = {'original': original_val, 'improved': improved_val, 'balanced': balanced_val}
            best_key = max(values, key=values.get)
            best_counts[best_key] += 1
            
            best_marker = {
                'original': 'ğŸ¥‡' if best_key == 'original' else '',
                'improved': 'ğŸ¥‡' if best_key == 'improved' else '', 
                'balanced': 'ğŸ¥‡' if best_key == 'balanced' else ''
            }
            
            print(f"{metric:<15} {original_val:<15.3f} {improved_val:<15.3f} {balanced_val:<15.3f} {best_marker[best_key]:<15}")
    
    # æ··æ·†çŸ©é˜µå¯¹æ¯”
    if 'true_positives' in original_metrics:
        print(f"\nğŸ“ˆ æ··æ·†çŸ©é˜µå¯¹æ¯”:")
        
        models = [
            ("åŸç‰ˆGPT-4.1", original_metrics),
            ("æ”¹è¿›ç‰ˆGPT-4.1", improved_metrics), 
            ("å¹³è¡¡ç‰ˆGPT-4.1", balanced_metrics)
        ]
        
        for model_name, metrics in models:
            print(f"\n{model_name}:")
            print(f"  TP: {metrics['true_positives']}, FP: {metrics['false_positives']}")
            print(f"  FN: {metrics['false_negatives']}, TN: {metrics['true_negatives']}")
            print(f"  è¯¯æŠ¥ç‡: {metrics['false_positives']/(metrics['false_positives']+metrics['true_negatives'])*100:.1f}%")
            print(f"  æ¼æŠ¥ç‡: {metrics['false_negatives']/(metrics['false_negatives']+metrics['true_positives'])*100:.1f}%")
    
    # ç»¼åˆè¯„ä¼°
    print(f"\nğŸ† ç»¼åˆè¯„ä¼° (åŸºäº {len(detailed_results)} ä¸ªè§†é¢‘):")
    print(f"   åŸç‰ˆGPT-4.1 è·èƒœæŒ‡æ ‡: {best_counts['original']} ä¸ª")
    print(f"   æ”¹è¿›ç‰ˆGPT-4.1 è·èƒœæŒ‡æ ‡: {best_counts['improved']} ä¸ª") 
    print(f"   å¹³è¡¡ç‰ˆGPT-4.1 è·èƒœæŒ‡æ ‡: {best_counts['balanced']} ä¸ª")
    
    # å…³é”®æ”¹è¿›ç‚¹åˆ†æ
    improved_cases = []
    for result in detailed_results:
        if result['balanced_correct'] and not result['original_correct']:
            improved_cases.append(f"{result['video_id']}: å¹³è¡¡ç‰ˆä¿®æ­£äº†åŸç‰ˆçš„é”™è¯¯")
        elif result['balanced_correct'] and not result['improved_correct']:
            improved_cases.append(f"{result['video_id']}: å¹³è¡¡ç‰ˆä¿®æ­£äº†æ”¹è¿›ç‰ˆçš„é”™è¯¯")
    
    if improved_cases:
        print(f"\nâœ… å¹³è¡¡ç‰ˆçš„å…³é”®æ”¹è¿› ({len(improved_cases)} ä¸ªæ¡ˆä¾‹):")
        for case in improved_cases[:5]:
            print(f"   â€¢ {case}")

def main():
    print("ğŸ”§ è¯„ä¼°å¹³è¡¡ç‰ˆGPT-4.1çš„ä¸‰æ–¹å¯¹æ¯”")
    print("=" * 60)
    
    # åŠ è½½Ground Truth
    ground_truth = load_ground_truth()
    print(f"ğŸ“‹ Ground Truthæ ‡ç­¾æ•°é‡: {len(ground_truth)}")
    
    # è®¾ç½®ç›®å½•
    original_dir = "result/gpt41-gt-final"
    improved_dir = "result/gpt41-improved-full"
    balanced_dir = "result/gpt41-balanced-full"
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    directories = [
        (original_dir, "åŸç‰ˆ"),
        (improved_dir, "æ”¹è¿›ç‰ˆ"),
        (balanced_dir, "å¹³è¡¡ç‰ˆ")
    ]
    
    for dir_path, dir_name in directories:
        if not os.path.exists(dir_path):
            print(f"âŒ {dir_name}ç»“æœç›®å½•ä¸å­˜åœ¨: {dir_path}")
            return
        
        count = len([f for f in os.listdir(dir_path) if f.endswith('.json')])
        print(f"ğŸ“ {dir_name}ç»“æœæ•°é‡: {count}")
    
    # è¿›è¡Œä¸‰æ–¹è¯„ä¼°
    original_metrics, improved_metrics, balanced_metrics, detailed_results = evaluate_three_models(
        original_dir, improved_dir, balanced_dir, ground_truth
    )
    
    # æ‰“å°ç»“æœ
    print_three_way_comparison(original_metrics, improved_metrics, balanced_metrics, detailed_results)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_df = pd.DataFrame(detailed_results)
    results_file = "result/gpt41_three_way_comparison.csv"
    results_df.to_csv(results_file, index=False)
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # å¦‚æœå¹³è¡¡ç‰ˆæ•°é‡ä¸å¤Ÿï¼Œæç¤ºç»§ç»­å¤„ç†
    balanced_count = len([f for f in os.listdir(balanced_dir) if f.endswith('.json')])
    if balanced_count < len(ground_truth) * 0.9:
        print(f"\nâš ï¸  å¹³è¡¡ç‰ˆè¿˜éœ€è¦å¤„ç†æ›´å¤šè§†é¢‘ (å½“å‰: {balanced_count}/{len(ground_truth)})")
        print("ğŸ’¡ å»ºè®®ç»§ç»­è¿è¡Œæ‰¹å¤„ç†è„šæœ¬ä»¥è·å¾—æ›´å®Œæ•´çš„è¯„ä¼°")
    else:
        print(f"\nğŸ‰ å¹³è¡¡ç‰ˆå¤„ç†å®Œæˆåº¦: {balanced_count}/{len(ground_truth)} ({balanced_count/len(ground_truth)*100:.1f}%)")

if __name__ == "__main__":
    main()