#!/usr/bin/env python3
"""
åˆ›å»ºå®é™…çš„ä¸‰æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š
åŸºäºå¯ç”¨çš„å¤„ç†ç»“æœè¿›è¡Œè¯„ä¼°
"""

import os
import json
import csv
from datetime import datetime
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score

def load_ground_truth():
    """åŠ è½½Ground Truthæ ‡ç­¾"""
    ground_truth_path = "result/groundtruth_labels.csv"
    ground_truth = {}
    
    with open(ground_truth_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            if row['video_id'] and row['video_id'].endswith('.avi'):
                video_id = row['video_id'].replace('.avi', '')
                label = row['ground_truth_label']
                
                # è§£ææ ‡ç­¾ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ghost probing
                if 'ghost probing' in label.lower():
                    ground_truth[video_id] = 1  # æ­£ä¾‹
                else:
                    ground_truth[video_id] = 0  # è´Ÿä¾‹
    
    return ground_truth

def extract_ghost_probing_prediction(result_data):
    """ä»æ¨¡å‹ç»“æœä¸­æå–ghost probingé¢„æµ‹"""
    if not isinstance(result_data, list):
        return 0
    
    # æ£€æŸ¥æ‰€æœ‰æ®µè½çš„åˆ†æç»“æœ
    for segment in result_data:
        if not isinstance(segment, dict):
            continue
            
        # æ£€æŸ¥å¤šä¸ªå­—æ®µä¸­æ˜¯å¦æåˆ°ghost probingç›¸å…³å†…å®¹
        text_fields = []
        for field in ['summary', 'actions', 'key_actions', 'next_action', 'key_objects']:
            if field in segment and segment[field]:
                text_fields.append(str(segment[field]).lower())
        
        combined_text = ' '.join(text_fields)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ghost probingç›¸å…³å…³é”®è¯
        ghost_keywords = [
            'ghost probing', 'ghost', 'probing', 
            'sudden appearance', 'unexpected', 'emerging',
            'appearing suddenly', 'cuts in', 'cut in',
            'overtaking', 'lane change', 'dangerous',
            'risky maneuver', 'unsafe', 'sudden',
            'abrupt', 'intrusion', 'interference',
            'cuts into', 'merging aggressively'
        ]
        
        for keyword in ghost_keywords:
            if keyword in combined_text:
                return 1  # é¢„æµ‹ä¸ºæ­£ä¾‹
    
    return 0  # é¢„æµ‹ä¸ºè´Ÿä¾‹

def find_available_models():
    """æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹ç»“æœç›®å½•"""
    potential_dirs = {
        'GPT-4o': ['result/gpt4o-100-3rd', 'result/gpt4o-gt'],
        'GPT-4.1': ['result/gpt41-gt-final', 'result/gpt41-gt-complete', 'result/gpt41-format-test'],
        'Gemini': ['result/gemini-1.5-flash', 'result/gemini-gt-test']
    }
    
    available_models = {}
    for model_name, dirs in potential_dirs.items():
        for dir_path in dirs:
            if os.path.exists(dir_path):
                # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰JSONæ–‡ä»¶
                json_files = [f for f in os.listdir(dir_path) if f.endswith('.json')]
                if json_files:
                    available_models[model_name] = dir_path
                    break
    
    return available_models

def evaluate_model(model_name, result_dir, ground_truth):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    print(f"   ğŸ” è¯„ä¼° {model_name} (ç›®å½•: {result_dir})")
    
    # è·å–å¤„ç†çš„è§†é¢‘æ–‡ä»¶
    result_files = [f for f in os.listdir(result_dir) if f.endswith('.json')]
    processed_videos = []
    
    for filename in result_files:
        video_id = filename.replace('actionSummary_', '').replace('.json', '')
        if video_id in ground_truth:
            processed_videos.append(video_id)
    
    print(f"      æ‰¾åˆ° {len(processed_videos)} ä¸ªGround Truthè§†é¢‘")
    
    if len(processed_videos) == 0:
        print(f"      âš ï¸ æ²¡æœ‰å¤„ç†Ground Truthè§†é¢‘")
        return None
    
    # è¯„ä¼°é¢„æµ‹ç»“æœ
    predictions = {}
    failed_loads = 0
    
    for video_id in processed_videos:
        result_file = os.path.join(result_dir, f"actionSummary_{video_id}.json")
        
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                result_data = json.load(f)
                predictions[video_id] = extract_ghost_probing_prediction(result_data)
        except Exception as e:
            print(f"      âš ï¸ åŠ è½½{video_id}å¤±è´¥: {str(e)[:50]}...")
            predictions[video_id] = 0
            failed_loads += 1
    
    if failed_loads > 0:
        print(f"      âš ï¸ {failed_loads} ä¸ªæ–‡ä»¶åŠ è½½å¤±è´¥")
    
    # è®¡ç®—æŒ‡æ ‡
    y_true = [ground_truth[video_id] for video_id in processed_videos]
    y_pred = [predictions[video_id] for video_id in processed_videos]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè¯„ä¼°
    if len(set(y_true)) < 2 or len(set(y_pred)) < 1:
        print(f"      âš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—å®Œæ•´æŒ‡æ ‡")
        return {
            'model': model_name,
            'sample_size': len(processed_videos),
            'accuracy': 0,
            'precision': 0,
            'recall': 0,
            'f1_score': 0,
            'specificity': 0,
            'balanced_accuracy': 0,
            'true_positives': 0,
            'true_negatives': 0,
            'false_positives': 0,
            'false_negatives': 0,
            'note': 'Insufficient data for evaluation'
        }
    
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
    else:
        # å¤„ç†åªæœ‰ä¸€ä¸ªç±»åˆ«çš„æƒ…å†µ
        if len(set(y_true)) == 1:
            if y_true[0] == 0:  # åªæœ‰è´Ÿä¾‹
                tn = sum([1 for p in y_pred if p == 0])
                fp = sum([1 for p in y_pred if p == 1])
                fn = tp = 0
            else:  # åªæœ‰æ­£ä¾‹
                tp = sum([1 for p in y_pred if p == 1])
                fn = sum([1 for p in y_pred if p == 0])
                tn = fp = 0
        else:
            tn = fp = fn = tp = 0
    
    # ç‰¹å¼‚æ€§å’Œå¹³è¡¡å‡†ç¡®ç‡
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    balanced_accuracy = (recall + specificity) / 2
    
    return {
        'model': model_name,
        'sample_size': len(processed_videos),
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'specificity': specificity,
        'balanced_accuracy': balanced_accuracy,
        'true_positives': int(tp),
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'processed_videos': processed_videos
    }

def create_practical_comparison_report():
    """åˆ›å»ºå®é™…çš„ä¸‰æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š"""
    print("ğŸ“Š åˆ›å»ºå®é™…ä¸‰æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š")
    print("=" * 60)
    
    # åŠ è½½Ground Truth
    ground_truth = load_ground_truth()
    print(f"ğŸ“ åŠ è½½Ground Truth: {len(ground_truth)} ä¸ªè§†é¢‘")
    
    # æŸ¥æ‰¾å¯ç”¨æ¨¡å‹
    available_models = find_available_models()
    print(f"ğŸ” å‘ç°å¯ç”¨æ¨¡å‹: {list(available_models.keys())}")
    
    if not available_models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ç»“æœ")
        return None
    
    # è¯„ä¼°æ‰€æœ‰å¯ç”¨æ¨¡å‹
    model_results = {}
    for model_name, result_dir in available_models.items():
        print(f"\nğŸ” è¯„ä¼° {model_name}...")
        metrics = evaluate_model(model_name, result_dir, ground_truth)
        if metrics:
            model_results[model_name.lower().replace('-', '').replace('.', '')] = metrics
            print(f"   âœ… {model_name} è¯„ä¼°å®Œæˆ")
        else:
            print(f"   âŒ {model_name} è¯„ä¼°å¤±è´¥")
    
    if not model_results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¯„ä¼°ç»“æœ")
        return None
    
    # åˆ›å»ºæŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    report = {
        "analysis_timestamp": datetime.now().isoformat(),
        "description": "Practical comparison based on available model results",
        "ground_truth_info": {
            "total_gt_samples": len(ground_truth),
            "positive_samples": sum(ground_truth.values()),
            "negative_samples": len(ground_truth) - sum(ground_truth.values())
        },
        "model_results": model_results,
        "available_models": available_models,
        "evaluation_notes": {
            "note": "Evaluation based on actually processed videos by each model",
            "gpt41_limitation": "GPT-4.1 processing limited by Azure content filtering policies"
        }
    }
    
    # ç¡®ä¿æ¯”è¾ƒç›®å½•å­˜åœ¨
    os.makedirs("result/comparison", exist_ok=True)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"result/comparison/practical_three_models_comparison_{timestamp}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
    
    return report

def print_practical_comparison_summary(report):
    """æ‰“å°å®é™…å¯¹æ¯”æ€»ç»“"""
    print("\n" + "="*100)
    print("ğŸ† GPT-4o vs GPT-4.1 vs Gemini å®é™…å¯¹æ¯”ç»“æœ")
    print("åŸºäºå„æ¨¡å‹å®é™…å¤„ç†çš„Ground Truthè§†é¢‘")
    print("="*100)
    
    model_results = report["model_results"]
    
    print(f"ğŸ“ Ground Truthä¿¡æ¯:")
    print(f"   æ€»æ ·æœ¬æ•°: {report['ground_truth_info']['total_gt_samples']}")
    print(f"   æ­£ä¾‹æ ·æœ¬: {report['ground_truth_info']['positive_samples']}")
    print(f"   è´Ÿä¾‹æ ·æœ¬: {report['ground_truth_info']['negative_samples']}")
    
    print(f"\nğŸ“Š å„æ¨¡å‹å®é™…å¤„ç†çš„è§†é¢‘æ•°é‡:")
    for model_key, metrics in model_results.items():
        note = f" ({metrics.get('note', '')})" if 'note' in metrics else ""
        print(f"   {metrics['model']}: {metrics['sample_size']} ä¸ªè§†é¢‘{note}")
    
    print(f"\nğŸ¯ æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")
    print(f"{'æŒ‡æ ‡':<20} ", end="")
    for model_key, metrics in model_results.items():
        print(f"{metrics['model']:<15} ", end="")
    print()
    print("-" * (20 + 15 * len(model_results)))
    
    metrics_to_show = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity']
    for metric in metrics_to_show:
        print(f"{metric:<20} ", end="")
        for model_key, metrics in model_results.items():
            value = metrics[metric]
            print(f"{value:<15.4f} ", end="")
        print()
    
    print(f"\nğŸ“ˆ æ··æ·†çŸ©é˜µå¯¹æ¯”:")
    for model_key, metrics in model_results.items():
        tp, tn, fp, fn = metrics['true_positives'], metrics['true_negatives'], metrics['false_positives'], metrics['false_negatives']
        print(f"   {metrics['model']}: TP={tp}, TN={tn}, FP={fp}, FN={fn}")
    
    # æ‰¾å‡ºæœ‰æ•ˆè¯„ä¼°çš„æ¨¡å‹ï¼ˆæ ·æœ¬æ•°>5ï¼‰
    valid_models = {k: v for k, v in model_results.items() if v['sample_size'] > 5}
    
    if valid_models:
        print(f"\nğŸ’¡ ä¸»è¦å‘ç° (åŸºäºæ ·æœ¬æ•°>5çš„æ¨¡å‹):")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_f1_model = max(valid_models.items(), key=lambda x: x[1]['f1_score'])
        best_precision_model = max(valid_models.items(), key=lambda x: x[1]['precision'])
        best_recall_model = max(valid_models.items(), key=lambda x: x[1]['recall'])
        
        print(f"   ğŸ¥‡ æœ€ä½³F1åˆ†æ•°: {best_f1_model[1]['model']} ({best_f1_model[1]['f1_score']:.4f})")
        print(f"   ğŸ¯ æœ€ä½³ç²¾ç¡®åº¦: {best_precision_model[1]['model']} ({best_precision_model[1]['precision']:.4f})")
        print(f"   ğŸ” æœ€ä½³å¬å›ç‡: {best_recall_model[1]['model']} ({best_recall_model[1]['recall']:.4f})")
    
    print(f"\nâš ï¸ é‡è¦è¯´æ˜:")
    print("   è¯„ä¼°åŸºäºå„æ¨¡å‹å®é™…å¤„ç†çš„è§†é¢‘æ•°é‡")
    print("   GPT-4.1ç”±äºAzureå†…å®¹è¿‡æ»¤æ”¿ç­–é™åˆ¶ï¼Œå¤„ç†æ ·æœ¬è¾ƒå°‘")
    print("   GPT-4oå’ŒGeminiæä¾›äº†æ›´å®Œæ•´çš„Ground Truthè¦†ç›–")
    
    print("="*100)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å®é™…ä¸‰æ¨¡å‹å¯¹æ¯”è¯„ä¼°")
    print("åŸºäºå¯ç”¨çš„å¤„ç†ç»“æœ")
    print("=" * 60)
    
    # åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
    report = create_practical_comparison_report()
    
    if report:
        # æ‰“å°æ€»ç»“
        print_practical_comparison_summary(report)
        
        print(f"\nğŸ‰ å®é™…å¯¹æ¯”è¯„ä¼°å®Œæˆï¼")
        print("   è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° result/comparison/ ç›®å½•")
    else:
        print("âŒ å®é™…å¯¹æ¯”è¯„ä¼°å¤±è´¥")

if __name__ == "__main__":
    main()