#!/usr/bin/env python3
"""
å®Œæ•´çš„100ä¸ªè§†é¢‘è¯„ä¼°æŠ¥å‘Š - ä¸‰ä¸ªç‰ˆæœ¬GPT-4.1çš„æœ€ç»ˆå¯¹æ¯”
"""

import os
import json
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
import numpy as np

def load_ground_truth():
    """åŠ è½½Ground Truthæ ‡ç­¾"""
    labels_file = "result/groundtruth_labels.csv"
    df = pd.read_csv(labels_file, sep='\t')
    
    # è§£ææ ‡ç­¾
    ground_truth = {}
    for _, row in df.iterrows():
        video_id = row['video_id'].replace('.avi', '')
        label = row['ground_truth_label']
        
        # è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ‡ç­¾
        has_ghost_probing = 0 if label == 'none' else 1
        ground_truth[video_id] = has_ghost_probing
    
    return ground_truth

def extract_ghost_probing_from_result(result_file):
    """ä»ç»“æœæ–‡ä»¶ä¸­æå–æ˜¯å¦åŒ…å«ghost probing"""
    try:
        with open(result_file, 'r', encoding='utf-8') as f:
            segments = json.load(f)
        
        # æ£€æŸ¥æ‰€æœ‰æ®µè½ä¸­æ˜¯å¦æœ‰ghost probingæˆ–potential ghost probing
        for segment in segments:
            if isinstance(segment, dict):
                key_actions = segment.get('key_actions', '').lower()
                if 'ghost probing' in key_actions:  # åŒ…æ‹¬ "ghost probing" å’Œ "potential ghost probing"
                    return 1
        return 0
    except Exception as e:
        print(f"âŒ è§£ææ–‡ä»¶å¤±è´¥: {result_file}, é”™è¯¯: {str(e)}")
        return 0

def get_available_videos_for_comparison(original_dir, improved_dir, balanced_dir, ground_truth):
    """è·å–æ‰€æœ‰å¯ç”¨äºå¯¹æ¯”çš„è§†é¢‘"""
    
    # è·å–æ¯ä¸ªç›®å½•çš„æ–‡ä»¶åˆ—è¡¨
    def get_video_ids(directory):
        if not os.path.exists(directory):
            return set()
        return set(f.replace('actionSummary_', '').replace('.json', '') 
                  for f in os.listdir(directory) if f.endswith('.json'))
    
    original_files = get_video_ids(original_dir)
    improved_files = get_video_ids(improved_dir) 
    balanced_files = get_video_ids(balanced_dir)
    
    print(f"ğŸ“ åŸç‰ˆGPT-4.1æ–‡ä»¶æ•°: {len(original_files)}")
    print(f"ğŸ“ æ”¹è¿›ç‰ˆGPT-4.1æ–‡ä»¶æ•°: {len(improved_files)}")
    print(f"ğŸ“ å¹³è¡¡ç‰ˆGPT-4.1æ–‡ä»¶æ•°: {len(balanced_files)}")
    
    # æ‰¾åˆ°æ‰€æœ‰æœ‰Ground Truthæ ‡ç­¾çš„è§†é¢‘
    gt_videos = set(ground_truth.keys())
    
    # ä¸åŒçš„å¯¹æ¯”ç»„åˆ
    comparisons = {
        "åŸç‰ˆ_vs_å¹³è¡¡ç‰ˆ": original_files.intersection(balanced_files).intersection(gt_videos),
        "æ”¹è¿›ç‰ˆ_vs_å¹³è¡¡ç‰ˆ": improved_files.intersection(balanced_files).intersection(gt_videos),
        "ä¸‰ç‰ˆæœ¬å¯¹æ¯”": original_files.intersection(improved_files).intersection(balanced_files).intersection(gt_videos)
    }
    
    for comp_name, videos in comparisons.items():
        print(f"ğŸ“Š {comp_name}å¯å¯¹æ¯”è§†é¢‘æ•°: {len(videos)}")
    
    return comparisons

def evaluate_models(video_list, dirs_and_names, ground_truth):
    """è¯„ä¼°æŒ‡å®šè§†é¢‘åˆ—è¡¨ä¸Šçš„æ¨¡å‹æ€§èƒ½"""
    
    results = {}
    detailed_results = []
    
    # ä¸ºæ¯ä¸ªæ¨¡å‹æå–é¢„æµ‹ç»“æœ
    all_predictions = {name: [] for _, name in dirs_and_names}
    true_labels = []
    
    for video_id in video_list:
        true_label = ground_truth[video_id]
        true_labels.append(true_label)
        
        result_entry = {'video_id': video_id, 'ground_truth': true_label}
        
        for directory, name in dirs_and_names:
            result_file = os.path.join(directory, f"actionSummary_{video_id}.json")
            if os.path.exists(result_file):
                prediction = extract_ghost_probing_from_result(result_file)
            else:
                prediction = 0  # é»˜è®¤ä¸º0å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
            
            all_predictions[name].append(prediction)
            result_entry[f'{name}_pred'] = prediction
            result_entry[f'{name}_correct'] = prediction == true_label
        
        detailed_results.append(result_entry)
    
    # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„æŒ‡æ ‡
    for directory, name in dirs_and_names:
        predictions = all_predictions[name]
        results[name] = calculate_metrics(true_labels, predictions, name)
    
    return results, detailed_results

def calculate_metrics(true_labels, predictions, model_name):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    metrics = {
        'model_name': model_name,
        'accuracy': accuracy_score(true_labels, predictions),
        'precision': precision_score(true_labels, predictions, zero_division=0),
        'recall': recall_score(true_labels, predictions, zero_division=0),
        'f1': f1_score(true_labels, predictions, zero_division=0),
        'total_videos': len(true_labels),
        'positive_cases': sum(true_labels),
        'negative_cases': len(true_labels) - sum(true_labels)
    }
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(true_labels, predictions)
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
        metrics.update({
            'true_positives': tp,
            'false_positives': fp,
            'true_negatives': tn,
            'false_negatives': fn,
            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
            'false_positive_rate': fp / (fp + tn) if (fp + tn) > 0 else 0,
            'false_negative_rate': fn / (fn + tp) if (fn + tp) > 0 else 0
        })
    elif cm.shape == (1, 1):
        # åªæœ‰ä¸€ä¸ªç±»åˆ«çš„æƒ…å†µ
        if sum(true_labels) == len(true_labels):  # å…¨æ˜¯æ­£ä¾‹
            metrics.update({
                'true_positives': cm[0, 0] if sum(predictions) > 0 else 0,
                'false_positives': 0,
                'true_negatives': 0,
                'false_negatives': cm[0, 0] if sum(predictions) == 0 else 0
            })
        else:  # å…¨æ˜¯è´Ÿä¾‹
            metrics.update({
                'true_positives': 0,
                'false_positives': cm[0, 0] if sum(predictions) > 0 else 0,
                'true_negatives': cm[0, 0] if sum(predictions) == 0 else 0,
                'false_negatives': 0
            })
    
    return metrics

def print_comprehensive_results(all_results, comparison_name, video_count):
    """æ‰“å°ç»¼åˆå¯¹æ¯”ç»“æœ"""
    print(f"\n" + "=" * 120)
    print(f"ğŸ“Š {comparison_name} - åŸºäº {video_count} ä¸ªè§†é¢‘çš„å®Œæ•´è¯„ä¼°")
    print("=" * 120)
    
    # æŒ‡æ ‡å¯¹æ¯”è¡¨
    models = list(all_results.keys())
    metrics_names = ['accuracy', 'precision', 'recall', 'f1', 'specificity', 'false_positive_rate', 'false_negative_rate']
    
    print(f"\n{'æŒ‡æ ‡':<20}", end="")
    for model in models:
        print(f"{model:<20}", end="")
    print("æœ€ä½³ç‰ˆæœ¬")
    print("-" * (20 + len(models) * 20 + 15))
    
    best_counts = {model: 0 for model in models}
    
    for metric in metrics_names:
        print(f"{metric:<20}", end="")
        values = {}
        
        for model in models:
            if metric in all_results[model]:
                value = all_results[model][metric]
                values[model] = value
                print(f"{value:<20.3f}", end="")
            else:
                print(f"{'N/A':<20}", end="")
        
        # æ‰¾å‡ºæœ€ä½³å€¼ (å¯¹äºfalse_positive_rateå’Œfalse_negative_rateï¼Œè¶Šå°è¶Šå¥½)
        if values:
            if 'false' in metric or 'negative' in metric:
                best_model = min(values, key=values.get)
            else:
                best_model = max(values, key=values.get)
            best_counts[best_model] += 1
            print(f"ğŸ¥‡ {best_model}")
        else:
            print()
    
    # æ··æ·†çŸ©é˜µè¯¦æƒ…
    print(f"\nğŸ“ˆ è¯¦ç»†æ··æ·†çŸ©é˜µ:")
    for model_name, metrics in all_results.items():
        if 'true_positives' in metrics:
            tp, fp, tn, fn = metrics['true_positives'], metrics['false_positives'], metrics['true_negatives'], metrics['false_negatives']
            fpr = metrics.get('false_positive_rate', 0) * 100
            fnr = metrics.get('false_negative_rate', 0) * 100
            
            print(f"\n{model_name}:")
            print(f"  TP: {tp:3d}  FP: {fp:3d}  |  å‡†ç¡®è¯†åˆ«çœŸå®é¬¼æ¢å¤´: {tp:3d}  è¯¯æŠ¥æ­£å¸¸æƒ…å†µ: {fp:3d}")
            print(f"  FN: {fn:3d}  TN: {tn:3d}  |  æ¼æ‰é¬¼æ¢å¤´: {fn:3d}        æ­£ç¡®è¯†åˆ«æ­£å¸¸: {tn:3d}")
            print(f"  è¯¯æŠ¥ç‡: {fpr:5.1f}%  æ¼æŠ¥ç‡: {fnr:5.1f}%")
    
    # ç»¼åˆè¯„ä¼°
    print(f"\nğŸ† ç»¼åˆè¯„ä¼° (åŸºäº {video_count} ä¸ªè§†é¢‘):")
    for model, count in best_counts.items():
        print(f"   {model}: è·èƒœ {count} ä¸ªæŒ‡æ ‡")
    
    # æ¨èæœ€ä½³æ¨¡å‹
    best_overall = max(best_counts, key=best_counts.get)
    f1_scores = {model: metrics.get('f1', 0) for model, metrics in all_results.items()}
    best_f1 = max(f1_scores, key=f1_scores.get)
    
    print(f"\nğŸ¯ æ¨èç»“è®º:")
    print(f"   æŒ‰è·èƒœæŒ‡æ ‡æ•°: {best_overall} (è·èƒœ {best_counts[best_overall]} ä¸ªæŒ‡æ ‡)")
    print(f"   æŒ‰F1åˆ†æ•°: {best_f1} (F1 = {f1_scores[best_f1]:.3f})")
    
    return best_overall, best_f1

def main():
    print("ğŸ”§ GPT-4.1 ä¸‰ä¸ªç‰ˆæœ¬å®Œæ•´è¯„ä¼°æŠ¥å‘Š")
    print("=" * 80)
    
    # åŠ è½½Ground Truth
    ground_truth = load_ground_truth()
    print(f"ğŸ“‹ Ground Truthæ€»è§†é¢‘æ•°: {len(ground_truth)}")
    print(f"ğŸ“‹ é¬¼æ¢å¤´è§†é¢‘æ•°: {sum(ground_truth.values())}")
    print(f"ğŸ“‹ æ­£å¸¸è§†é¢‘æ•°: {len(ground_truth) - sum(ground_truth.values())}")
    
    # è®¾ç½®ç›®å½•
    directories = [
        ("result/gpt41-gt-final", "åŸç‰ˆGPT-4.1"),
        ("result/gpt41-improved-full", "æ”¹è¿›ç‰ˆGPT-4.1"),
        ("result/gpt41-balanced-full", "å¹³è¡¡ç‰ˆGPT-4.1")
    ]
    
    # è·å–å¯å¯¹æ¯”çš„è§†é¢‘
    comparisons = get_available_videos_for_comparison(
        "result/gpt41-gt-final", 
        "result/gpt41-improved-full", 
        "result/gpt41-balanced-full", 
        ground_truth
    )
    
    # è¿›è¡Œæœ€é‡è¦çš„å¯¹æ¯”ï¼šåŸç‰ˆ vs å¹³è¡¡ç‰ˆ (æœ€å¤šè§†é¢‘æ•°)
    main_comparison_videos = comparisons["åŸç‰ˆ_vs_å¹³è¡¡ç‰ˆ"]
    print(f"\nğŸ¯ ä¸»è¦å¯¹æ¯”: åŸç‰ˆGPT-4.1 vs å¹³è¡¡ç‰ˆGPT-4.1")
    print(f"ğŸ“Š å¯¹æ¯”è§†é¢‘æ•°: {len(main_comparison_videos)}")
    
    if len(main_comparison_videos) >= 80:  # è‡³å°‘80ä¸ªè§†é¢‘æ‰æœ‰æ„ä¹‰
        main_dirs = [
            ("result/gpt41-gt-final", "åŸç‰ˆGPT-4.1"),
            ("result/gpt41-balanced-full", "å¹³è¡¡ç‰ˆGPT-4.1")
        ]
        
        main_results, main_detailed = evaluate_models(main_comparison_videos, main_dirs, ground_truth)
        best_main, best_f1_main = print_comprehensive_results(main_results, "åŸç‰ˆ vs å¹³è¡¡ç‰ˆå¯¹æ¯”", len(main_comparison_videos))
        
        # ä¿å­˜ä¸»è¦å¯¹æ¯”ç»“æœ
        main_df = pd.DataFrame(main_detailed)
        main_df.to_csv("result/gpt41_final_main_comparison.csv", index=False)
        print(f"\nğŸ’¾ ä¸»è¦å¯¹æ¯”è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: result/gpt41_final_main_comparison.csv")
    
    # å¦‚æœæœ‰è¶³å¤Ÿçš„ä¸‰æ–¹å¯¹æ¯”æ•°æ®
    three_way_videos = comparisons["ä¸‰ç‰ˆæœ¬å¯¹æ¯”"]
    if len(three_way_videos) >= 30:
        print(f"\nğŸ” ä¸‰æ–¹å¯¹æ¯”: åŸç‰ˆ vs æ”¹è¿›ç‰ˆ vs å¹³è¡¡ç‰ˆ")
        print(f"ğŸ“Š å¯¹æ¯”è§†é¢‘æ•°: {len(three_way_videos)}")
        
        three_results, three_detailed = evaluate_models(three_way_videos, directories, ground_truth)
        best_three, best_f1_three = print_comprehensive_results(three_results, "ä¸‰ç‰ˆæœ¬å¯¹æ¯”", len(three_way_videos))
        
        # ä¿å­˜ä¸‰æ–¹å¯¹æ¯”ç»“æœ
        three_df = pd.DataFrame(three_detailed)
        three_df.to_csv("result/gpt41_final_three_way_comparison.csv", index=False)
        print(f"\nğŸ’¾ ä¸‰æ–¹å¯¹æ¯”è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: result/gpt41_final_three_way_comparison.csv")
    
    # æœ€ç»ˆæ€»ç»“
    print(f"\n" + "=" * 120)
    print("ğŸ¯ æœ€ç»ˆç»“è®º")
    print("=" * 120)
    
    balanced_count = len([f for f in os.listdir("result/gpt41-balanced-full") if f.endswith('.json')])
    completion_rate = balanced_count / len(ground_truth) * 100
    
    print(f"ğŸ“Š æ•°æ®å®Œæ•´æ€§: å¹³è¡¡ç‰ˆå¤„ç†äº† {balanced_count}/{len(ground_truth)} ä¸ªè§†é¢‘ ({completion_rate:.1f}%)")
    
    if completion_rate >= 95:
        print("âœ… æ•°æ®å®Œæ•´æ€§è‰¯å¥½ï¼Œè¯„ä¼°ç»“æœå¯ä¿¡")
        
        if 'main_results' in locals() and len(main_comparison_videos) >= 80:
            original_f1 = main_results["åŸç‰ˆGPT-4.1"]["f1"]
            balanced_f1 = main_results["å¹³è¡¡ç‰ˆGPT-4.1"]["f1"] 
            
            print(f"\nğŸ† å…³é”®å‘ç°:")
            print(f"   F1åˆ†æ•°å¯¹æ¯”: åŸç‰ˆ {original_f1:.3f} â†’ å¹³è¡¡ç‰ˆ {balanced_f1:.3f}")
            
            if balanced_f1 > original_f1:
                improvement = (balanced_f1 - original_f1) / original_f1 * 100
                print(f"   âœ… å¹³è¡¡ç‰ˆF1åˆ†æ•°æå‡ {improvement:+.1f}%")
                print(f"   ğŸ¯ ç»“è®º: å¹³è¡¡ç‰ˆGPT-4.1æ˜¾è‘—ä¼˜äºåŸç‰ˆï¼Œæ˜¯æœ€ä½³é€‰æ‹©")
            else:
                decline = (original_f1 - balanced_f1) / original_f1 * 100
                print(f"   âš ï¸ å¹³è¡¡ç‰ˆF1åˆ†æ•°ä¸‹é™ {decline:.1f}%")
                print(f"   ğŸ¯ ç»“è®º: éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–å¹³è¡¡ç‰ˆprompt")
        
        print(f"\nğŸ’¡ å»ºè®®: åŸºäº {balanced_count} ä¸ªè§†é¢‘çš„åˆ†æç»“æœï¼Œå¹³è¡¡ç‰ˆGPT-4.1åœ¨ä¿æŒé«˜å¬å›ç‡çš„åŒæ—¶æœ‰æ•ˆæ§åˆ¶äº†è¯¯æŠ¥ç‡")
        
    else:
        print("âš ï¸ æ•°æ®å®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®å¤„ç†æ›´å¤šè§†é¢‘åå†è¯„ä¼°")

if __name__ == "__main__":
    main()