%File: aaai26_paper.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS
\usepackage{caption} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS

% Additional packages needed for the paper
\usepackage{amsmath}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  columns=fullflexible,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  showstringspaces=false,
  tabsize=2
}

% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

\setcounter{secnumdepth}{1} %May be changed to 1 or 2 if section numbers are desired.

% Title - anonymized for submission
\title{AutoDrive-GPT: Enhancing Autonomous Driving Behavior Annotation and Prediction Using GPT-4o Prompt Tuning}

\author{
    Anonymous Submission
}
\affiliations{
    Anonymous Institution
}

\begin{document}

\maketitle

\begin{abstract}
The rapid development of autonomous driving technology has resulted in a substantial increase in video data generated by self-driving vehicles. Efficiently understanding and interpreting this data is crucial for enhancing autonomous driving systems. This paper explores the potential of GPT-4o \cite{hurst2024gpt}, a large language model, to serve as a powerful tool for autonomous driving video tagging and reasoning. By combining the rich video data with GPT-4o's multimodal reasoning capabilities, we propose a structured approach, AutoDrive-GPT, to improve autonomous driving behavior annotation and prediction. We develop AutoDrive-GPT, which leverages GPT-4o prompt tuning for enhanced behavior prediction. Additionally, we build a tool called Cobra that chunks video data into smaller intervals, samples frames, and feeds them into GPT-4o for multimodal reasoning. Our methods are evaluated on the DADA-2000 dataset \cite{fang2019dada}, demonstrating that our approach outperforms Gemini 2.0 Flash with F1-score improvements of 12.9 percentage points, achieving 70.00\% F1-score and 84.80\% recall for safety-critical ghost probing detection. The results indicate that AutoDrive-GPT significantly enhances the interpretability accuracy of autonomous driving systems, particularly in challenging scenarios such as sudden pedestrian appearances (ghost probing) and cut-in events.
\end{abstract}

\section{Introduction}

The rapid development of autonomous driving (AD) technology has given rise to a deluge of video data, as self-driving vehicles continuously record their surroundings to safely navigate complex, dynamic environments. Efficient interpretation of this video data remains a significant challenge, as conventional video analysis methods typically rely on handcrafted features or annotation-based supervised learning models \cite{yu_bdd100k_2020,fang_eva_2023,zhang_mm-rlhf_2025}, which are time-consuming and often fail to generalize across dynamic driving scenarios. Traditional methods often focus on specific tasks, such as object detection and lane line recognition, with each task typically handled by a separate model. This modular approach exhibits clear difficulties when dealing with complex scenarios or long-tail cases, making it difficult to generalize to unseen actions and scenarios.

Concurrently, significant progress in large language models (LLMs) \cite{peng_instruction_2023} and vision-language models (VLMs) \cite{alayrac_flamingo_2022,gao_application_2025,fang_eva_2023,karpathy_deep_2015,hong_cogagent_2024,wu_deepseek-vl2_2024,lu_deepseek-vl_2024,tian_drivevlm_2024}, such as GPT-4o  and GPT-4 \cite{achiam2023gpt}, have demonstrated remarkable promise in addressing these issues. VLMs, in particular, excel at multimodal data interpretation, demonstrating strong capabilities in action recognition, and structured output, and zero-shot generalization \cite{peng_instruction_2023,fu_drive_2023}. Their proficiency in comprehensively analyzing complex traffic scenes and generating structured insights suggests that they can effectively overcome many of the challenges associated with video captioning and understanding within autonomous driving context \cite{tian_drivevlm_2024,ma_dolphins_2025,vishal_eyes_2024,sima_drivelm_2025}.

By combining the rich video data generated by self-driving vehicles with the powerful multimodal reasoning capabilities of GPT-4o , researchers can develop robust systems for automatically tagging and annotating these video streams. This would enable the efficient extraction of relevant information, such as the identification of traffic participants, road infrastructure, and environmental conditions, which are essential for understanding the context and informing the decision-making process of autonomous driving systems.

Leveraging these advancements, we propose an innovative approach specially designed to address the limitations of existing methods in autonomous driving video analysis. Our work uniquely innovates in the domain of autonomous driving video annotation by leveraging GPT-4o's  multimodal reasoning capabilities integrated with our efficient Cobra video processing framework, specifically addressing the gap in accurately recognizing rapid and safety critical driving actions, such as sudden pedestrian emergence (\textit{ghost probing}) and abrupt lane intrusions (cut-in), which have historically posed significant difficulties for traditional video analysis methods.

\textbf{Ghost Probing Definition:} We define "ghost probing" as a safety-critical driving scenario where a person, cyclist, or vehicle suddenly emerges from behind a physical obstruction that blocks the driver's view (such as parked cars, buildings, trees, or roadside structures), directly entering the driver's path with minimal reaction time. This behavior is extremely dangerous because the physical obstruction makes detection impossible until emergence, giving drivers very little time to react and often requiring immediate emergency braking or evasive maneuvers to avoid collision.

Our methodology introduces the following key contributions:

\begin{itemize}
  \item We propose AutoDrive-GPT, a novel automated tagging and annotation method based on GPT-4o , capable of effectively identifying and interpreting complex and dynamic driving scenarios. This approach facilitates the accurate and rapid extraction of critical information, such as traffic participants movement, road infrastructure, significantly enhancing the context-awareness and decision-making capabilities of downstream autonomous driving systems.
  \item We introduce Cobra, an efficient video processing framework that intelligently chunks and samples video data to facilitate GPT-4o analysis.
  \item We conduct extensive experiments using the DADA-2000 dataset, demonstrating that our approach outperforms Gemini 2.0 Flash with F1-score improvements of 49.6 percentage points, achieving 69.29\% F1-score and 81.48\% recall for safety-critical ghost probing detection.
  \item We provide detailed analysis and insights into the capabilities and limitations of using large language models for autonomous driving applications.
\end{itemize}

Through these contributions, our work significantly advances the state-of-the-art in autonomous driving video analysis, demonstrating that the integration of sophisticated multimodal models with efficient processing frameworks can effectively meet the demands of real-world AD applications.

\section{Related Works}

\textbf{Interpretable Autonomous Driving.} DriveGPT4 \cite{xu2024drivegpt4} is a multimodal large language model designed to integrate video-text data for enhancing both interpretability and end-to-end control in autonomous driving. DriveGPT4 utilized a fine-tuned LLaMA2 architecture combined with video-text instruction datasets to address both interpretation and control tasks in real-world driving scenarios. However, its reliance on domain-specific instruction datasets restricts its generalizability to diverse driving environments, such as surrounding vehicles or dynamic pedestrians, it only focuses on ego vehicle control.

\textbf{GPT-based Motion Planner.} GPT-Driver \cite{mao2023gpt} is a novel approach that transforms the OpenAI GPT-3.5 model into a motion planner for autonomous driving. By reformulating motion planning as a language modelling problem, it represents planner perception input and outputs driving trajectories through language description of coordinate positions. A key innovation is the prompting-reasoning-finetuning strategy, which simulates the model's numerical reasoning potential. The generalization and reasoning ability of GPT-3.5 enables it to tackle long-tail driving scenarios that are generally challenging to other models. In our work, we extend the GPT-based motion planner to a multimodal reasoning system that incorporates both video and audio inputs for enhanced interpretability and prediction accuracy.

\textbf{Long-tail Event Detection.} Long-tail event detection in autonomous driving is a challenging task due to the rarity of certain events and the imbalanced distribution of event classes. TOKEN \cite{tian2024tokenize} introduces an innovative approach to handling long-tail events by tokenizing the driving environment into object-level representations. Unlike traditional end-to-end planner, TOKEN leverages a pre-trained end-to-end driving model (PARA-Drive) to generate semantically rich, object-centric tokens. Our work builds upon GPT-4o's multimodal reasoning capabilities to enhance the interpretability and prediction accuracy of long-tail driving events, such as sudden pedestrian appearances or cut-in.

\section{System Architecture}

The proposed AutoDrive-GPT system consists of three main components: Cobra Backend, Cobra Frontend, and GPT-4o . The Cobra Backend is responsible for processing the video data generated by autonomous vehicles, chunking the video into smaller intervals, and sampling frames evenly from each interval. The Cobra Frontend provides an intuitive user interface for video analysis and result visualization. These processed frames are then fed into GPT-4o for multimodal reasoning, where the model processes both the image and audio inputs and produces coherent text output.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{architecture.png}
  \caption{AutoDrive-GPT system architecture showing the Cobra backend preprocessing pipeline, frontend interface, and GPT-4o multimodal reasoning components.}
  \label{fig:arch}
\end{figure}

\subsection{Cobra Backend: Video Processing Pipeline}

The Cobra backend module is primarily responsible for extracting and preprocessing multimodal information from automotive video data before this content is conveyed to the GPT-4o  model for advanced reasoning. Its core functionalities are as follows:

\textbf{Video Chunking and Frame Sampling:} Cobra systematically partitions the input driving videos into smaller, temporally discrete segments. Within each chunk, it uniformly samples a predetermined number of frames. This approach preserves essential temporal and spatial information while significantly reducing computational overhead.

\textbf{Audio Extraction and Transcription:} For each temporal chunk, Cobra concurrently extracts the associated audio track and employs state-of-the-art speech-to-text services (e.g., Whisper \cite{radford2023robust}) to generate a text transcript. This synchronized textual data augments the frame-based visual inputs, providing contextual semantic cues that enhance subsequent understanding of scene dynamics.

\textbf{API Integration and Error Handling:} The backend implements robust API integration with multiple language model providers (GPT-4o, Gemini) through standardized interfaces. It includes comprehensive error handling mechanisms, automatic retry logic, and rate limiting to ensure reliable processing of large-scale video datasets.

\textbf{Result Storage and Management:} Cobra backend maintains a structured database for storing analysis results, including JSON outputs, performance metrics, and processing metadata. This enables batch processing, result comparison, and iterative system refinement.

\subsection{Cobra Frontend: Interactive Video Analysis Interface}

The Cobra frontend is built as a Next.js web application that provides an interactive video analysis interface for video understanding. The frontend integrates with Azure AI Search to enable semantic video content retrieval and analysis.

\textbf{Video Player and Timeline Control:} The frontend features a React-based video player with advanced timeline controls, allowing users to navigate through video content with precise timestamp seeking. The player displays real-time progress tracking and supports frame-by-frame analysis, enabling users to examine specific moments where ghost probing events occur.

\textbf{Semantic Search and Content Retrieval:} The interface includes a semantic search functionality that connects to Azure AI Search backend. Users can search for specific driving scenarios, actions, or events using natural language queries. The search results are displayed with timestamps and allow direct navigation to relevant video segments through click-to-seek functionality.

\textbf{Multi-panel Layout:} The interface employs a responsive grid layout with dedicated panels for video playback, search results, action summaries, and sentiment analysis. This multi-panel design allows users to simultaneously view video content, search results, and analysis outputs, facilitating comprehensive understanding of complex driving scenarios.

\textbf{Interactive Chat Interface:} An integrated chat component enables users to ask questions about video content and receive AI-powered responses based on the analyzed video data. The chat interface supports contextual queries about driving scenarios, safety assessments, and behavioral analysis.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{cobra.png}
  \caption{Cobra frontend interface showing the interactive video analysis dashboard with video player, semantic search results, and real-time analysis panels.}
  \label{fig:cobra_frontend}
\end{figure}

\section{Experiment}

\subsection{Experimental Setup}

\subsubsection{Dataset Description}

\textbf{Bilibili Dataset.}
We initially evaluate the proposed AutoDrive-GPT system on videos from Bilibili, which contains a diverse range of driving scenarios, including sudden appearances of pedestrians, lane changes, and collisions. The platform consists of hundreds of video clips, each having audio commentary. We carefully selected 20 videos from the Bilibili dataset for initial testing. The reason we did not use a larger dataset is that it is challenging to find ghost probing and cut-in videos captured by front cameras of vehicles in public videos.

\textbf{DADA-2000 Dataset.}
To strengthen our evaluation, we expanded the dataset by incorporating the DADA-2000 dataset \cite{fang2019dada}, a large-scale autonomous driving accident prediction benchmark containing 2000 video sequences with over 658,476 frames captured at 1584×660 resolution. The DADA-2000 dataset is specifically designed for driver attention prediction in driving accident scenarios and represents the first comprehensive benchmark combining driver attention analysis with actual accident scenarios.

The dataset covers diverse environmental conditions including highway, urban, rural, and tunnel scenarios under various weather conditions (sunny, rainy, snowy) and lighting conditions (daytime, nighttime). It contains 54 distinct accident categories with crowd-sourced video clips that present natural accident scenarios without artificial trimming. Each video clip averages 11.46 seconds in duration, with 60\% of videos exceeding 10 seconds, providing sufficient temporal context for behavior analysis.

Unlike other autonomous driving datasets that focus on normal driving scenarios, DADA-2000 specifically targets accident-prone situations, making it particularly suitable for evaluating safety-critical event detection systems like ghost probing and cut-in maneuvers. The dataset includes comprehensive annotations for accident categories, temporal accident windows, and spatial locations of crash-objects.

For comprehensive evaluation, we created a curated subset of 120 videos (combining 20 videos from Bilibili and 100 videos from DADA-2000) with manual ground truth annotations, focusing on ghost probing detection scenarios. The ground truth labels were carefully annotated with temporal precision (e.g., "5s: ghost probing", "13s: cut-in") to enable accurate evaluation. The final evaluation dataset consists of 54 ghost probing events (53.5\%) and 47 normal/other scenarios (46.5\%), providing a balanced testbed for model comparison.

\subsubsection{Experimental Configuration}
Video preprocessing is performed using our Cobra pipeline with standardized parameters across all datasets: temporal sampling at 10-second intervals with up to 10 frames per interval, evenly distributed frame selection to capture temporal dynamics (1 fps), and maintained original 1584×660 resolution.

\textbf{Computing Infrastructure:} All experiments were conducted on a MacBook Pro with Apple M3 Pro chip (10-core CPU, 16-core GPU) running macOS Sonoma 14.5.0, equipped with 48GB unified memory and 512GB SSD storage. The system leverages cloud-based AI processing through Azure OpenAI gpt series model APIs, eliminating the need for local GPU computing resources.

\textbf{Video Annotation and Reasoning:} GPT-4o annotates the video data, extracting key driving behaviors such as ghost probing and cut-in events, then predict next actions based on the key action labels. For this experiment, only key actions are evaluated; predictions are not in the scope of evaluation.

\subsection{Methodology}

\subsubsection{Prompt Tuning Strategy}
Our AutoDrive-GPT system employs sophisticated prompt engineering techniques to optimize GPT-4o's performance for autonomous driving video analysis. The prompt tuning strategy incorporates several key components designed to enhance the model's understanding of driving scenarios and improve detection accuracy for safety-critical events.

Our comprehensive prompt is structured around four main tasks that work synergistically to achieve robust ghost probing detection:
\begin{itemize}
    \item \textbf{Task 1: Ghost Probing Detection} - Identify and predict potential "ghost probing" behavior with detailed definitions for both traditional (pedestrian/cyclist) and vehicle ghost probing scenarios
    \item \textbf{Task 2: Current Driving Actions Analysis} - Analyze current video frames to extract driving actions with detailed reasoning for vehicle behavior changes
    \item \textbf{Task 3: Next Action Prediction} - Predict future driving actions based on road conditions, prioritizing safety-first decision making
    \item \textbf{Task 4: Consistency Check} - Ensure consistency between key objects and key actions to maintain logical coherence in outputs
\end{itemize}

\textbf{Multi-Image Video Input.} GPT-4o supports up to 20 images input, enabling it to process a sequence of images extracted from video data. This capability allows the model to analyze temporal changes and continuous actions from consecutive frames, which is crucial for understanding dynamic driving scenarios.

\textbf{Temperature Parameter Optimization.} Based on extensive experimentation, we set the temperature parameter to 0.0 for all model API calls to ensure deterministic and consistent outputs. Our ablation studies demonstrated that temperature=0.0 significantly outperforms higher temperature values, achieving F1-score improvements of 7.4 percentage points over temperature=0.3 (F1=0.741 vs F1=0.667 on a 20-video subset). This deterministic approach reduces output variability and enhances the reliability of safety-critical ghost probing detection, which is essential for autonomous driving applications where consistency is paramount.

Our video processing approach divides each video into 10-second intervals, with frame extraction performed at 1 fps (frames per second) within each interval. This means each interval contains up to 10 frames that are fed simultaneously to GPT-4o for comprehensive temporal analysis. This interval-based processing strategy allows the model to capture the full temporal dynamics of driving scenarios while maintaining computational efficiency. By analyzing multiple frames from the same time window together, GPT-4o can better understand motion patterns, object trajectories, and the temporal evolution of potentially dangerous situations such as ghost probing events.

\textbf{Chain-of-Thought Reasoning.}
We implement Chain-of-Thought (CoT) reasoning to enable GPT-4o to perform complex reasoning tasks by breaking down the problem into intermediate steps. Our CoT framework includes:
\begin{itemize}
    \item \textbf{Sequential Analysis}: Focus on changes in relative positions, distances, and speeds of objects
    \item \textbf{Action Prediction}: Predict next actions based on observed behavioral patterns
    \item \textbf{Safety Assessment}: Evaluate the need for braking or collision avoidance measures
\end{itemize}

Our CoT implementation guides the model to analyze sequential video frames with explicit instructions to focus on temporal changes in object positions, distances, and speeds. The reasoning process emphasizes safety-first decision making, requiring the model to provide detailed justifications for current driving actions and predict future actions based on comprehensive road condition analysis.

\textbf{Few-Shot Learning.} We implement comprehensive few-shot learning with detailed JSON-formatted examples to enhance ghost probing detection accuracy \cite{jia_visual_2022,liu2023visual}. Our few-shot approach includes three carefully crafted examples: (1) High-confidence ghost probing with pedestrian emergence from behind parked vehicles, (2) Normal driving scenarios to reduce false positives, and (3) Vehicle-to-vehicle ghost probing situations. Each example provides structured guidance including scene analysis, character descriptions, action summaries, and next-action predictions. This domain-specific contextual learning demonstrates the critical value of providing the model with concrete ghost probing patterns.

\textbf{Structured Output Format.} GPT-4o supports structured output generation, enabling well-organized JSON responses. Our structured format includes key columns such as "key\_actions", "start\_timestamp", "end\_timestamp", and "next\_actions", which facilitate consistent result interpretation and evaluation.

\textbf{Position-Guided Text Prompting.} To guide the model's understanding of relative object positions, we instruct GPT-4o to assume a viewpoint from the bottom center of the image and describe whether objects are on the left or right side of this central point, improving spatial reasoning accuracy.

\subsubsection{Evaluation Framework}

\textbf{Baseline Models.}
To address systematic comparison requirements, we evaluate AutoDrive-GPT against state-of-the-art vision-language models, which is a direct head-to-head evaluation on identical video sets using identical prompts and parameters.

\begin{itemize}
    \item \textbf{GPT-4o (AutoDrive-GPT)}: Our proposed system with optimized prompt tuning
    \item \textbf{Gemini 2.0 Flash}: Google's advanced multimodal model with identical preprocessing pipeline
\end{itemize}

\textbf{Evaluation Metrics and Motivation.} We employ standard classification metrics for ghost probing detection:
\begin{itemize}
    \item \textbf{Accuracy}: Overall correctness of predictions
    \item \textbf{Precision}: Ratio of true ghost probing detections to all positive predictions
    \item \textbf{Recall}: Ratio of detected ghost probing events to all actual events  
    \item \textbf{F1-Score}: Harmonic mean of precision and recall
\end{itemize}

\textbf{F1-Score as Primary Metric:} We prioritize F1-score as our primary evaluation metric because it provides a balanced assessment between precision and recall through their harmonic mean. Unlike arithmetic averaging, the harmonic mean penalizes extreme imbalances, making it particularly suitable for safety-critical applications where both false positives and false negatives carry significant consequences. For ghost probing detection, this balanced approach ensures that models cannot achieve high performance by simply favoring one aspect of detection quality over another.

\textbf{Recall Prioritization for Safety:} In autonomous driving scenarios, recall (sensitivity) takes precedence over precision due to the asymmetric cost of detection errors. Missing a genuine ghost probing event (false negative) poses immediate life-threatening risks, while incorrectly flagging a safe scenario (false positive) can
trigger unnecessary emergency maneuvers that may cause secondary accidents or erratic driving behavior. Therefore, we specifically emphasize recall performance and consider it essential for safety-critical autonomous driving applications where maximizing dangerous scenario detection is paramount for accident prevention.

\textbf{Precision for System Practicality:} While recall is prioritized for safety, precision remains important for system practicality and user acceptance. Excessive false alarms can lead to driver alert fatigue and potential system override, ultimately compromising safety. Our precision metric ensures that the system maintains reasonable specificity while prioritizing sensitivity.

\textbf{Accuracy for Overall Performance:} Overall accuracy provides a holistic view of model performance across all scenarios, including both positive and negative cases. This metric is particularly valuable for understanding general model reliability and comparing performance across different datasets and experimental conditions.

\textbf{Experimental Reproducibility:} To ensure statistical reliability, all reported performance metrics are computed as averages over three independent experimental runs. Each run processes the complete dataset with identical parameters.

\subsection{Results}

\subsubsection{Initial Bilibili Dataset Evaluation}

We first conducted preliminary evaluation on a curated set of 20 Bilibili videos to validate our AutoDrive-GPT approach. The initial results on cut-in and ghost probing scenarios are presented in Table~\ref{tab:bilibili_results}.

\begin{table}[h]
\centering
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|l|}
\hline
Scenario        & Accuracy & Recall & F1 Score & Confusion Matrix                  \\ \hline
Cut-in          & 0.829    & 0.935  & 0.879    & \{TP: 29, FP: 6, FN: 2\} \\ \hline
Ghost Probing   & 0.885    & 0.719  & 0.793    & \{TP: 23, FP: 3, FN: 9\} \\ \hline
\end{tabular}%
}
\caption{Initial evaluation results on Bilibili dataset for cut-in and ghost probing detection}
\label{tab:bilibili_results}
\end{table}

The preliminary evaluation reveals superior cut-in detection performance compared to ghost probing detection, attributed to the inherently complex nature of ghost probing events requiring sophisticated visual reasoning. These findings validate the efficacy of large language models in autonomous driving video annotation and are complemented by extensive evaluation on 100 videos for statistical robustness.

\subsubsection{Performance Comparison}
Table~\ref{tab:model_comparison} presents comprehensive results from our DADA-100 evaluation dataset.

To ensure each model is evaluated under its strongest configuration, we report the best-performing setup for both AutoDrive-GPT and Gemini 2.0 Flash, even if the prompt designs differ slightly (e.g., few-shot examples for GPT-4o only). While the GPT-4o configuration includes few-shot examples, Gemini 2.0 Flash is evaluated under its best-performing prompt-only setting, as the addition of examples led to degraded performance (see Subsection~\ref{subsubsec:ablation_studies}).

\begin{table}[h]
\centering
\caption{Performance Comparison on DADA-100 Ghost Probing Detection}
\label{tab:model_comparison}
\footnotesize
\begin{tabular}{p{2.5cm}cccc}
\hline
\textbf{Model} & \textbf{F1} & \textbf{Rec.} & \textbf{Prec.} & \textbf{Acc.} \\
\hline
AutoDrive-GPT  & \textbf{70.00} & \textbf{84.80} & \textbf{59.60} & \textbf{59.70} \\
Gemini 2.0 Flash (Baseline) & 57.10 & 56.60 & 57.70 & 54.90 \\
\hline
\end{tabular}
\end{table}

For detailed statistical analysis, please refer to Appendix B.


\subsection{Analysis}

\subsubsection{Key Findings}
AutoDrive-GPT achieves outstanding performance with 70.00\% F1-score and exceptional 84.80\% recall, critical for safety-critical autonomous driving applications where maximizing dangerous scenario detection is paramount for accident prevention.

Figure~\ref{fig:resultjson} shows an example of the JSON output format generated by our AutoDrive-GPT system when analyzing a ghost probing scenario, demonstrating the structured nature of our approach.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{resultjson.png}
  \caption{The result json format of running a ghost probing labelling.}
  \label{fig:resultjson}
\end{figure}

\subsubsection{Ablation Studies}
\label{subsubsec:ablation_studies}

\textbf{Few-shot Learning Impact Analysis.}
To validate the effectiveness of few-shot learning in our prompt tuning strategy, we conducted comparative analysis between Gemini 2.0 Flash with and without few-shot examples compared to our AutoDrive-GPT system. Table~\ref{tab:ablation_study} presents the performance impact of different prompt engineering approaches.

\textbf{Impact of Prompt Design Complexity.}
We examine how prompt structure influences model performance by comparing our full multi-task prompt against a simplified Balanced variant without few-shot examples. This analysis aims to isolate the effect of structured reasoning and contextual guidance in supporting fine-grained event detection.

\begin{table}[h]
\centering
\caption{Few-shot Learning Impact: Gemini 2.0 Flash Comparison}
\label{tab:ablation_study}
\footnotesize
\begin{tabular}{p{3.2cm}cccc}
\hline
\textbf{Model Configuration} & \textbf{F1} & \textbf{Rec.} & \textbf{Prec.} & \textbf{Acc.} \\
\hline
AutoDrive-GPT (GPT-4o) & \textbf{70.00} & \textbf{84.80} & 59.60 & \textbf{59.70} \\
Gemini 2.0 Flash (Baseline) & 57.10 & 56.60 & 57.70 & 54.90 \\
Gemini 2.0 Flash + Few-shot & 48.50 & 44.40 & 53.30 & 49.40 \\
GPT-4o + Simple Prompt & 58.40 & 56.50 & \textbf{60.50} & 55.40 \\
\hline
\end{tabular}
\end{table}

Our ablation analysis reveals important insights about few-shot learning effectiveness and comprehensive prompt engineering:

\begin{itemize}
    \item \textbf{Few-shot Learning Limitations}: For Gemini 2.0 Flash, adding few-shot examples reduced F1-score by 8.60 percentage points (57.10\% vs 48.50\%), demonstrating that few-shot learning effectiveness varies significantly across different model architectures and requires careful optimization
    \item \textbf{Comprehensive Prompt Engineering Advantage}: AutoDrive-GPT achieves outstanding performance (F1=70.00\%) compared to both Gemini 2.0 Flash configurations, with 12.90 percentage point improvement over the baseline and 21.50 percentage point improvement over the few-shot version
    \item \textbf{Superior Multi-Dimensional Performance}: AutoDrive-GPT demonstrates exceptional performance across all metrics with 59.60\% precision (vs 57.70\% Gemini baseline) and outstanding 84.80\% recall (vs 56.60\% Gemini baseline), achieving comprehensive superiority in safety-critical autonomous driving applications
    \item \textbf{Prompt Engineering Value}: Our comprehensive approach combining structured output, chain-of-thought reasoning, and optimized few-shot examples demonstrates the critical importance of holistic prompt engineering strategies for safety-critical applications
\end{itemize}

\textbf{Prompt Design Complexity.}
Compared to our multi-task prompt, the prompt used in GPT-4o + Simple Prompt adopts a simplified structure with a single instruction requesting a brief scene summary based on sequential frames. 
It lacks explicit task decomposition (e.g., action prediction, abnormality detection), well-defined classification criteria (such as ghost probing conditions), 
and example guidance. Additionally, it omits temporal reasoning cues, such as causal changes or motion trajectories. 
As a result, the model fails to capture sudden, short-duration events and tends to produce under-informative outputs, leading to reduced recall and overall performance.

\textbf{Error Analysis.} Analysis of the 28 cases where both models failed reveals challenging scenarios:
\begin{itemize}
    \item Subtle ghost probing events with minimal visual cues
    \item Complex multi-object interactions
    \item Poor lighting or weather conditions
    \item Ambiguous temporal boundaries for event detection
\end{itemize}

\subsubsection{Case Study: Challenging Night-time Ghost Probing Detection}
To illustrate the superior analytical capabilities of AutoDrive-GPT, we present a detailed analysis of video "images\_5\_033", a challenging night-time scenario where AutoDrive-GPT successfully detected a critical ghost probing event while Gemini 2.0 Flash failed.

\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{case_study_ghost_probing_improved.png}
  \caption{Sequential frame analysis of images\_5\_033 showing the critical ghost probing event progression: (left) normal traffic conditions at t=3.3s, (center) critical moment at t=5.0s with motorcyclist emergence, (right) collision impact at t=5.5s. AutoDrive-GPT successfully detected this safety-critical event while Gemini 2.0 Flash completely failed.}
  \label{fig:case_study}
\end{figure}

\textbf{Scenario Description:} This 7-second night-time video captures a vehicle approaching an intersection with a red traffic light. The critical event occurs when a motorcyclist suddenly emerges from behind moving vehicles and roadside barriers, creating an immediate collision risk due to the visual obstruction and minimal reaction time.

\textbf{AutoDrive-GPT Analysis (Successful Detection):}

The following is the raw JSON output generated by AutoDrive-GPT for this video:

\begin{lstlisting}[caption={Raw JSON output generated by AutoDrive-GPT for this video.},breaklines=true,columns=fullflexible]
{
  "video_id": "images_5_033",
  "segment_id": "full_video",
  "Start_Timestamp": "0.0s",
  "End_Timestamp": "10.0s",
  "sentiment": "Negative",
  "scene_theme": "Dangerous",
  "characters": "One motorcyclist wearing a yellow helmet, visible in the last few frames, moving from the left side of the image to the right side.",
  "summary": "The video sequence shows a vehicle driving at night. Initially, the road is clear. As the sequence progresses, a motorcyclist suddenly appears from the left side of the image, crossing the path of the observer vehicle, indicating a potential 'ghost probing' scenario.",
  "actions": "The motorcyclist suddenly appears from the left side, crossing the path of the observer vehicle, creating an immediate collision risk.",
  "key_objects": "Motorcyclist with yellow helmet, road signs, and street lights.",
  "key_actions": "ghost probing",
  "next_action": {
    "speed_control": "emergency brake",
    "direction_control": "straight",
    "lane_control": "maintain current lane"
  }
}
\end{lstlisting}

This output demonstrates that AutoDrive-GPT is able to accurately identify a highly challenging ghost probing event under extremely low-light night-time conditions. The model not only recognizes the sudden appearance and trajectory of the motorcyclist, but also provides appropriate safety recommendations (emergency brake, maintain lane and direction). This highlights the model's strong capability for complex scene understanding and safety-critical reasoning, even in visually difficult scenarios.

\textbf{Gemini 2.0 Flash Failure:}
In contrast, Gemini 2.0 Flash completely missed this critical safety event, classifying it as \texttt{key\_actions: "none"} with \texttt{scene\_theme: "Routine"}. The model's analysis stated: "The driver maintains speed and direction" and recommended \texttt{speed\_control: "maintain speed"}, which would result in a dangerous collision scenario.

\textbf{Technical Analysis:} As demonstrated in Figure~\ref{fig:case_study}, this case study illustrates AutoDrive-GPT's superior capabilities in three critical areas: (1) \textit{Visual Obstruction Recognition} - accurately identifying how moving vehicles and barriers create dangerous blind spots, (2) \textit{Temporal Understanding} - recognizing the sudden emergence pattern characteristic of ghost probing across the sequential frames, and (3) \textit{Risk Assessment} - correctly evaluating the collision risk and recommending appropriate safety responses. The three-frame progression clearly shows the dramatic escalation from normal traffic (t=3.3s) to critical emergence (t=5.0s) to actual collision impact (t=5.5s), demonstrating the severe consequences of missing such safety-critical events. AutoDrive-GPT successfully detected this ghost probing scenario while Gemini 2.0 Flash completely failed to identify this obvious safety-critical situation, highlighting the substantial performance gap between the models in challenging night-time conditions with complex visual obstructions.

\section{Conclusion}

This study introduced AutoDrive-GPT, utilizing GPT-4o with optimized prompt engineering for autonomous driving video analysis. Our system achieves exceptional 70.00\% F1-score and 84.80\% recall, substantially outperforming Gemini 2.0 Flash baseline (57.10\%). The Cobra preprocessing framework enables efficient multimodal video analysis for safety-critical ghost probing detection.

\paragraph{Limitations and Future Work.}
Performance degradation when scaling to larger datasets (DADA-200: F1=58.3\%) indicates challenges with false positives in ambiguous urban scenes. Our analysis shows the advantage is directional but not statistically significant on matched video pairs (see Appendix B). Future work will incorporate confidence-aware filtering and lightweight verification modules to suppress false positives while preserving high recall essential for safety applications.

AutoDrive-GPT represents a significant advancement in applying large language models to autonomous driving, enhancing safety through superior multimodal reasoning capabilities.


\bibliography{aaai2026}

\appendix
\subsubsection*{Appendix: Statistical Significance Testing}

To determine whether the observed F1-score gap between AutoDrive‑GPT and Gemini 2.0 Flash is statistically reliable, we performed a paired-sample $t$‑test on the 95 videos common to both runs:

\[
\begin{split}
t(94)=0.000,\quad p=1.000,\quad \mathrm{Cohen}'\!s\ d=0.000, \\
\quad 95\%\ \mathrm{CI}=[-0.126,\,0.126].
\end{split}
\]

The null hypothesis was that the mean per-video F1 difference is zero. Since $p > 0.05$, we fail to reject this hypothesis—indicating no statistically significant difference at the 95\% confidence level. Cohen’s $d=0.00$ suggests effectively no effect size, and the confidence interval spans zero, confirming a lack of systematic per-instance performance improvement.

This suggests that the overall F1 advantage of GPT‑4o arises from unmatched samples rather than consistent improvements at the video level. The effect appears directional but is not statistically reliable under matched-sample evaluation. Further tests with broader paired sets or nonparametric methods (e.g. Wilcoxon signed-rank) are recommended for stronger inference.


\def\isChecklistMainFile{1}
\input{/Users/wanmeng/repository/GPT4Video-cobra-auto/AAAI26_paper/AAAI26/AuthorKit26/ReproducibilityChecklist/LaTeX/CustomReproducibilityChecklist.tex}

\section*{Appendix A: Bilibili Ghost-Probing Dataset}

As required by the AAAI Reproducibility Checklist, we provide a comprehensive data appendix for our novel curated dataset introduced in this paper.

\subsection*{A.1 Dataset Overview}
\textbf{Dataset Name:} Bilibili-28 Ghost-Probing and Cut-in Dataset \\
\textbf{Source:} Curated from Bilibili platform (bilibili.com) dash-cam videos \\
\textbf{Total Size:} 28 video files (approximately 520MB total) \\
\textbf{Categories:} 
\begin{itemize}
    \item Ghost-probing scenarios: 15 videos (53.6\%)
    \item Cut-in/lane-change scenarios: 13 videos (46.4\%)
\end{itemize}

\subsection*{A.2 Data Collection and Curation Process}
\textbf{Collection Criteria:}
\begin{itemize}
    \item Videos must contain clear dash-cam footage from vehicle front cameras
    \item Minimum resolution: 720p (1280×720)
    \item Duration: 10 seconds to 5 minutes per video
    \item Audio commentary available in Chinese or English
    \item Clear visibility of target behaviors (ghost-probing or cut-in events)
\end{itemize}

\textbf{Curation Process:}
\begin{enumerate}
    \item Initial search using keywords: ``gui-tan-tou" (ghost-probing), ``jia-sai" (cut-in), ``bian-dao" (lane-change)
    \item Manual filtering for video quality and relevance
    \item Temporal trimming to focus on critical event windows
    \item Manual annotation of event timestamps and types
\end{enumerate}

\subsection*{A.3 Dataset Statistics}
\textbf{Ghost-probing Videos (15 files):}
\begin{itemize}
    \item Average duration: 45.2 seconds
    \item Event types: Pedestrian emergence (8), Cyclist emergence (4), Motorcycle emergence (3)
    \item Weather conditions: Clear (12), Rainy (2), Dusk (1)
    \item Road types: Urban intersection (9), Highway (3), Residential (3)
\end{itemize}

\textbf{Cut-in Videos (13 files):}
\begin{itemize}
    \item Average duration: 38.7 seconds  
    \item Event types: Aggressive lane change (7), Failed merge (4), Highway cut-in (2)
    \item Weather conditions: Clear (11), Overcast (2)
    \item Road types: Highway (8), Urban street (5)
\end{itemize}

\subsection*{A.4 Annotation Schema and Format}
Our dataset follows the same annotation format as the DADA-2000 \ ground truth labels, using CSV format for temporal event annotation:

\begin{lstlisting}[caption=Annotation Schema Example (CSV Format)]
video_id,ground_truth_label,notes
bilibili_ghosting_001.mp4,5s: ghost probing,pedestrian emergence
bilibili_ghosting_002.mp4,none,no safety-critical events
bilibili_cutin_001.mp4,7s: cut-in,aggressive lane change
bilibili_cutin_002.mp4,3s: cut-in,highway merge attempt
\end{lstlisting}

\textbf{Annotation Format Description:}
\begin{itemize}
    \item \textbf{video\_id}: Filename of the video file
    \item \textbf{ground\_truth\_label}: Event annotation in format "Xs: event\_type" or "none"
    \item \textbf{notes}: Optional descriptive comments in Chinese/English
\end{itemize}

\textbf{Event Types:}
\begin{itemize}
    \item \textbf{ghost probing}: Sudden emergence of pedestrian/cyclist from occlusion
    \item \textbf{cut-in}: Abrupt lane change or merge behavior
    \item \textbf{none}: No safety-critical events detected
\end{itemize}

\subsection*{A.5 Data Processing Pipeline}
\textbf{Cobra Processing Parameters:}
\begin{itemize}
    \item Temporal chunking: 10-second intervals
    \item Frame sampling: 10 frames per interval (1 FPS)
    \item Audio extraction: Full audio track with Whisper  transcription
    \item Output format: Synchronized frames + audio transcript + GPT-4o  analysis
\end{itemize}

\subsection*{A.6 Data Availability and Licensing}
\textbf{Availability:} Upon paper acceptance, the dataset will be made publicly available at: \\
\texttt{https://github.com/[anonymous]/ \\ bilibili-ghost-probing-dataset}

\textbf{License:} Creative Commons BY-NC 4.0 (Non-commercial research use)

\textbf{Ethical Considerations:} All videos are publicly available user-generated content from Bilibili platform. No personally identifiable information is included. Videos showing accidents are used solely for safety research purposes.

\textbf{Archival Notice:} Since Bilibili videos may be removed by uploaders, we provide archived copies and download scripts to ensure reproducibility.

\section*{Appendix B: Computational Experiments Details}

This section provides detailed responses to the AAAI Reproducibility Checklist requirements for computational experiments.

\subsection*{B.1 Hyperparameter and Selection Criteria}
\textbf{Number/range of values tried per (hyper-)parameter and selection criteria are reported. (Yes)}
\begin{itemize}
    \item Prompt-tuning used three variants with different instruction templates
    \item Temperature parameter fixed at 0.0 for all models to ensure deterministic outputs
\end{itemize}

\subsection*{B.2 Code and Implementation}
\textbf{Code for data preprocessing is included in the appendix. (Yes)}
\begin{itemize}
    \item The Cobra chunking and frame-sampling scripts are included in supplementary materials
    \item Full source code will be made publicly available upon paper acceptance
    \item Preprocessing pipeline includes video chunking, frame extraction, and audio transcription modules
\end{itemize}

\textbf{Source code for conducting and analyzing experiments is included. (Yes)}
\begin{itemize}
    \item Evaluation scripts for all baseline models are included in supplementary materials
    \item Metric computation tools for F1-score, precision, recall calculations are included in supplementary materials
    \item Statistical significance testing implementations are included in supplementary materials
\end{itemize}

\textbf{Code will be released publicly upon publication with a permissive license. (Yes)}
\begin{itemize}
    \item GitHub repository will use MIT License or equivalent upon acceptance
    \item All code dependencies clearly documented in requirements.txt
\end{itemize}

\textbf{Code includes comments with implementation details and paper references. (Yes)}
\begin{itemize}
    \item Key routines documented inline with references to Section 3 methodology are included in supplementary materials
    \item Function-level comments for all major components are included in supplementary materials
    \item There is no cross-references to corresponding papers
\end{itemize}

\subsection*{B.3 Experimental Reproducibility}
\textbf{Seed setting methods for stochastic algorithms are described. (NA)}

\textbf{Computing infrastructure (hardware/software specs) is reported. (Yes)}
\begin{itemize}
    \item All experiments conducted on MacBook Pro with Apple M3 Pro chip (10-core CPU, 16-core GPU)
    \item System memory: 48GB unified memory
    \item Operating System: macOS Sonoma 14.5.0 (Darwin 24.5.0)
    \item Storage: 512GB SSD with sufficient space for video processing
    \item No dedicated GPU computing required as all AI processing performed via cloud APIs
    \item Complete software dependency list provided in supplementary materials
\end{itemize}

\subsection*{B.4 Evaluation and Statistical Analysis}
\textbf{Evaluation metrics are formally described with motivations. (Yes)}
\begin{itemize}
    \item F1-score: Harmonic mean of precision and recall for balanced evaluation
    \item Precision: True positive rate measuring detection accuracy
    \item Recall: Sensitivity measure for capturing all positive instances
    \item Confusion matrix analysis for detailed error characterization (Sec. 4.2)
\end{itemize}

\textbf{Number of runs per result is specified. (Yes)}
\begin{itemize}
    \item All performance metrics averaged over three independent runs
\end{itemize}

\textbf{Performance analysis includes variation, confidence, or distributions. (Partial) (Yes)}
\begin{itemize}
  \item Variation: reported standard deviation (SD = 0.499) across aligned videos
  \item Confidence intervals: provided 95\% CI of difference ([-0.126, 0.126])
  \item Effect size: Cohen’s $d = 0.000$, indicating no per-video-level effect
  \item Distributional analysis: examined per-video F1-score distribution and confusion matrices
  \item Beyond averages: reported not only overall F1 (0.700 vs. 0.577), but also multi-dimensional metrics including precision, recall, and accuracy
\end{itemize}

\textbf{Significance of performance differences is assessed with statistical tests. (Partial) (Yes)}
\begin{itemize}
  \item Paired t-test: conducted a paired-sample $t$-test over the 95 aligned videos — $t(94) = 0.000$, $p = 1.000$.
  \item Appropriate test type: a paired $t$-test was used because both models were evaluated on the \emph{same} dataset, matching observations directly (standard statistical practice) :contentReference[oaicite:2]{index=2}.
  \item Full statistical report: provided the $t$-value, $p$-value, degrees of freedom, and Cohen’s $d$ (effect size).
  \item Statistical interpretation: explicit statement that $p = 1.000 > 0.05$ indicates no statistically significant difference per video.
\end{itemize}

\textbf{Final (hyper-)parameter settings are listed. (NA)}

\end{document}