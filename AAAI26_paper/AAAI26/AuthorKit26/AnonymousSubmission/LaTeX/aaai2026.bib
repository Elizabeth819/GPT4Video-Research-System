@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{xu2024drivegpt4,
  title={Drivegpt4: Interpretable end-to-end autonomous driving via large language model},
  author={Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kwan-Yee K and Li, Zhenguo and Zhao, Hengshuang},
  journal={IEEE Robotics and Automation Letters},
  year={2024},
  publisher={IEEE}
}

@article{mao2023gpt,
  title={Gpt-driver: Learning to drive with gpt},
  author={Mao, Jiageng and Qian, Yuxi and Ye, Junjie and Zhao, Hang and Wang, Yue},
  journal={arXiv preprint arXiv:2310.01415},
  year={2023}
}

@article{tian2024tokenize,
  title={Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving},
  author={Tian, Ran and Li, Boyi and Weng, Xinshuo and Chen, Yuxiao and Schmerling, Edward and Wang, Yue and Ivanovic, Boris and Pavone, Marco},
  journal={arXiv preprint arXiv:2407.00959},
  year={2024}
}


@inproceedings{yu_bdd100k_2020,
	title = {{BDD100K}: {A} {Diverse} {Driving} {Dataset} for {Heterogeneous} {Multitask} {Learning}},
	shorttitle = {{BDD100K}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Yu_BDD100K_A_Diverse_Driving_Dataset_for_Heterogeneous_Multitask_Learning_CVPR_2020_paper.html},
	urldate = {2025-03-06},
	author = {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
	year = {2020},
	pages = {2636--2645},
	file = {Full Text PDF:files/3/Yu et al. - 2020 - BDD100K A Diverse Driving Dataset for Heterogeneous Multitask Learning.pdf:application/pdf},
}

@misc{tian_drivevlm_2024,
	title = {{DriveVLM}: {The} {Convergence} of {Autonomous} {Driving} and {Large} {Vision}-{Language} {Models}},
	shorttitle = {{DriveVLM}},
	url = {http://arxiv.org/abs/2402.12289},
	doi = {10.48550/arXiv.2402.12289},
	abstract = {A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging Vision-Language Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of reasoning modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial reasoning and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. Experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex and unpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on a production vehicle, verifying it is effective in real-world autonomous driving environments.},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Tian, Xiaoyu and Gu, Junru and Li, Bailin and Liu, Yicheng and Wang, Yang and Zhao, Zhiyong and Zhan, Kun and Jia, Peng and Lang, Xianpeng and Zhao, Hang},
	month = jun,
	year = {2024},
	note = {arXiv:2402.12289 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:files/6/Tian et al. - 2024 - DriveVLM The Convergence of Autonomous Driving and Large Vision-Language Models.pdf:application/pdf;Snapshot:files/7/2402.html:text/html},
}

@inproceedings{zheng_large_2024,
	title = {Large {Language} {Models} {Powered} {Context}-aware {Motion} {Prediction} in {Autonomous} {Driving}},
	url = {https://ieeexplore.ieee.org/abstract/document/10802397},
	doi = {10.1109/IROS58592.2024.10802397},
	abstract = {Motion prediction is among the most fundamental tasks in autonomous driving. Traditional methods of motion forecasting primarily encode vector information of maps and historical trajectory data of traffic participants, lacking a comprehensive understanding of overall traffic semantics, which in turn affects the performance of prediction tasks. In this paper, we utilized Large Language Models (LLMs) to enhance the global traffic context understanding for motion prediction tasks. We first conducted systematic prompt engineering, visualizing complex traffic environments and historical trajectory information of traffic participants into image prompts— Transportation Context Map (TC-Map), accompanied by corresponding text prompts. Through this approach, we obtained rich traffic context information from the LLM. By integrating this information into the motion prediction model, we demonstrate that such context can enhance the accuracy of motion predictions. Furthermore, considering the cost associated with LLMs, we propose a cost-effective deployment strategy: enhancing the accuracy of motion prediction tasks at scale with 0.7\% LLM-augmented datasets. Our research offers valuable insights into enhancing the understanding of traffic scenes of LLMs and the motion prediction performance of autonomous driving. The source code is available at https://github.com/AIR-DISCOVER/LLM-Augmented-MTR and https://aistudio.baidu.com/projectdetail/7809548.},
	urldate = {2025-03-06},
	booktitle = {2024 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Zheng, Xiaoji and Wu, Lixiu and Yan, Zhijie and Tang, Yuanrong and Zhao, Hao and Zhong, Chen and Chen, Bokui and Gong, Jiangtao},
	month = oct,
	year = {2024},
	note = {ISSN: 2153-0866},
	keywords = {Accuracy, Autonomous vehicles, Context modeling, Large language models, Predictive models, Prompt engineering, Systematics, Trajectory, Transportation, Vectors},
	pages = {980--985},
	file = {IEEE Xplore Abstract Record:files/14/10802397.html:text/html;Submitted Version:files/13/Zheng et al. - 2024 - Large Language Models Powered Context-aware Motion Prediction in Autonomous Driving.pdf:application/pdf},
}

@inproceedings{ma_dolphins_2025,
	address = {Cham},
	title = {Dolphins: {Multimodal} {Language} {Model} for {Driving}},
	isbn = {978-3-031-72995-9},
	shorttitle = {Dolphins},
	doi = {10.1007/978-3-031-72995-9_23},
	abstract = {The quest for fully autonomous vehicles (AVs) capable of navigating complex real-world scenarios with human-like understanding and responsiveness. In this paper, we introduce Dolphins, a novel vision-language model architected to imbibe human-like abilities as a conversational driving assistant. Dolphins is adept at processing multimodal inputs comprising video (or image) data, text instructions, and historical control signals to generate informed outputs corresponding to the provided instructions. Building upon the open-sourced pretrained Vision-Language Model, OpenFlamingo, we first enhance Dolphins’s reasoning capabilities through an innovative Grounded Chain of Thought (GCoT) process in the general domain. Then we tailored Dolphins to the driving domain by constructing driving-specific instruction data and conducting instruction tuning. Through the utilization of the BDD-X dataset, we designed and consolidated four distinct AV tasks into Dolphins to foster a holistic understanding of intricate driving scenarios. As a result, the distinctive features of Dolphins are characterised into two dimensions: (1) the ability to provide a comprehensive understanding of complex and long-tailed open-world driving scenarios and solve a spectrum of AV tasks, and (2) the emergence of human-like capabilities including gradient-free instant adaptation via in-context learning and error recovery via reflection. The anonymous demo is available at https://vlm-driver.github.io/.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Ma, Yingzi and Cao, Yulong and Sun, Jiachen and Pavone, Marco and Xiao, Chaowei},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	year = {2025},
	pages = {403--420},
}

@misc{yang_llm4drive_2024,
	title = {{LLM4Drive}: {A} {Survey} of {Large} {Language} {Models} for {Autonomous} {Driving}},
	shorttitle = {{LLM4Drive}},
	url = {http://arxiv.org/abs/2311.01043},
	doi = {10.48550/arXiv.2311.01043},
	abstract = {Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their "black box" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about {\textbackslash}textit\{Large Language Models for Autonomous Driving (LLM4AD)\}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Yang, Zhenjie and Jia, Xiaosong and Li, Hongyang and Yan, Junchi},
	month = aug,
	year = {2024},
	note = {arXiv:2311.01043 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/18/Yang et al. - 2024 - LLM4Drive A Survey of Large Language Models for Autonomous Driving.pdf:application/pdf;Snapshot:files/19/2311.html:text/html},
}

@misc{marcu_lingoqa_2024,
	title = {{LingoQA}: {Visual} {Question} {Answering} for {Autonomous} {Driving}},
	shorttitle = {{LingoQA}},
	url = {http://arxiv.org/abs/2312.14115},
	doi = {10.48550/arXiv.2312.14115},
	abstract = {We introduce LingoQA, a novel dataset and benchmark for visual question answering in autonomous driving. The dataset contains 28K unique short video scenarios, and 419K annotations. Evaluating state-of-the-art vision-language models on our benchmark shows that their performance is below human capabilities, with GPT-4V responding truthfully to 59.6\% of the questions compared to 96.6\% for humans. For evaluation, we propose a truthfulness classifier, called Lingo-Judge, that achieves a 0.95 Spearman correlation coefficient to human evaluations, surpassing existing techniques like METEOR, BLEU, CIDEr, and GPT-4. We establish a baseline vision-language model and run extensive ablation studies to understand its performance. We release our dataset and benchmark1 as an evaluation platform for vision-language models in autonomous driving.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Marcu, Ana-Maria and Chen, Long and Hünermann, Jan and Karnsund, Alice and Hanotte, Benoit and Chidananda, Prajwal and Nair, Saurabh and Badrinarayanan, Vijay and Kendall, Alex and Shotton, Jamie and Arani, Elahe and Sinavski, Oleg},
	month = sep,
	year = {2024},
	note = {arXiv:2312.14115 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {PDF:files/20/Marcu et al. - 2024 - LingoQA Visual Question Answering for Autonomous Driving.pdf:application/pdf},
}

@misc{sima_drivelm_2025,
	title = {{DriveLM}: {Driving} with {Graph} {Visual} {Question} {Answering}},
	shorttitle = {{DriveLM}},
	url = {http://arxiv.org/abs/2312.14150},
	doi = {10.48550/arXiv.2312.14150},
	abstract = {We study how vision-language models (VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users. While recent approaches adapt VLMs to driving via single-round visual question answering (VQA), human drivers reason about decisions in multiple steps. Starting from the localization of key objects, humans estimate object interactions before taking actions. The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process. We instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving. The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures. Notably, its benefits are pronounced when it is evaluated zero-shot on unseen objects or sensor configurations. We hope this work can be the starting point to shed new light on how to apply VLMs for autonomous driving. To facilitate future research, all code, data, and models are available to the public.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Sima, Chonghao and Renz, Katrin and Chitta, Kashyap and Chen, Li and Zhang, Hanxue and Xie, Chengen and Beißwenger, Jens and Luo, Ping and Geiger, Andreas and Li, Hongyang},
	month = jan,
	year = {2025},
	note = {arXiv:2312.14150 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:files/21/Sima et al. - 2025 - DriveLM Driving with Graph Visual Question Answering.pdf:application/pdf},
}

@misc{fu_drive_2023,
	title = {Drive {Like} a {Human}: {Rethinking} {Autonomous} {Driving} with {Large} {Language} {Models}},
	shorttitle = {Drive {Like} a {Human}},
	url = {http://arxiv.org/abs/2307.07162},
	doi = {10.48550/arXiv.2307.07162},
	abstract = {In this paper, we explore the potential of using a large language model (LLM) to understand the driving environment in a human-like manner and analyze its ability to reason, interpret, and memorize when facing complex scenarios. We argue that traditional optimization-based and modular autonomous driving (AD) systems face inherent performance limitations when dealing with long-tail corner cases. To address this problem, we propose that an ideal AD system should drive like a human, accumulating experience through continuous driving and using common sense to solve problems. To achieve this goal, we identify three key abilities necessary for an AD system: reasoning, interpretation, and memorization. We demonstrate the feasibility of employing an LLM in driving scenarios by building a closed-loop system to showcase its comprehension and environment-interaction abilities. Our extensive experiments show that the LLM exhibits the impressive ability to reason and solve long-tailed cases, providing valuable insights for the development of human-like autonomous driving. The related code are available at https://github.com/PJLab-ADG/DriveLikeAHuman.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Fu, Daocheng and Li, Xin and Wen, Licheng and Dou, Min and Cai, Pinlong and Shi, Botian and Qiao, Yu},
	month = jul,
	year = {2023},
	note = {arXiv:2307.07162 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Robotics},
	file = {PDF:files/22/Fu et al. - 2023 - Drive Like a Human Rethinking Autonomous Driving with Large Language Models.pdf:application/pdf},
}

@misc{wu_deepseek-vl2_2024,
	title = {{DeepSeek}-{VL2}: {Mixture}-of-{Experts} {Vision}-{Language} {Models} for {Advanced} {Multimodal} {Understanding}},
	shorttitle = {{DeepSeek}-{VL2}},
	url = {http://arxiv.org/abs/2412.10302},
	doi = {10.48550/arXiv.2412.10302},
	abstract = {We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) VisionLanguage Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we incorporate a dynamic tiling vision encoding strategy designed for processing high-resolution images with different aspect ratios. For the language component, we leverage DeepSeekMoE models with the Multi-head Latent Attention mechanism, which compresses Key-Value cache into latent vectors, to enable efficient inference and high throughput. Trained on an improved vision-language dataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding. Our model series is composed of three variants: DeepSeek-VL2-Tiny, DeepSeek-VL2Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated parameters respectively. DeepSeekVL2 achieves competitive or state-of-the-art performance with similar or fewer activated parameters compared to existing open-source dense and MoE-based models. Codes and pre-trained models are publicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Wu, Zhiyu and Chen, Xiaokang and Pan, Zizheng and Liu, Xingchao and Liu, Wen and Dai, Damai and Gao, Huazuo and Ma, Yiyang and Wu, Chengyue and Wang, Bingxuan and Xie, Zhenda and Wu, Yu and Hu, Kai and Wang, Jiawei and Sun, Yaofeng and Li, Yukun and Piao, Yishi and Guan, Kang and Liu, Aixin and Xie, Xin and You, Yuxiang and Dong, Kai and Yu, Xingkai and Zhang, Haowei and Zhao, Liang and Wang, Yisong and Ruan, Chong},
	month = dec,
	year = {2024},
	note = {arXiv:2412.10302 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:files/23/Wu et al. - 2024 - DeepSeek-VL2 Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding.pdf:application/pdf},
}

@misc{lu_deepseek-vl_2024,
	title = {{DeepSeek}-{VL}: {Towards} {Real}-{World} {Vision}-{Language} {Understanding}},
	shorttitle = {{DeepSeek}-{VL}},
	url = {http://arxiv.org/abs/2403.05525},
	doi = {10.48550/arXiv.2403.05525},
	abstract = {We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. Our approach is structured around three key dimensions: • Data Construction: We strive to ensure our data is diverse, scalable and extensively covers real-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based content (expert knowledge, textbooks), aiming for a comprehensive representation of practical contexts. Further, we create a use case taxonomy from real user scenarios and construct an instruction-tuning dataset accordingly. The fine-tuning with this dataset substantially improves the model’s user experience in practical applications.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and Sun, Yaofeng and Deng, Chengqi and Xu, Hanwei and Xie, Zhenda and Ruan, Chong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.05525 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {PDF:files/24/Lu et al. - 2024 - DeepSeek-VL Towards Real-World Vision-Language Understanding.pdf:application/pdf},
}

@misc{esteban_scenario_2025,
	title = {Scenario {Understanding} of {Traffic} {Scenes} {Through} {Large} {Visual} {Language} {Models}},
	url = {http://arxiv.org/abs/2501.17131},
	doi = {10.48550/arXiv.2501.17131},
	abstract = {Deep learning models for autonomous driving, encompassing perception, planning, and control, depend on vast datasets to achieve their high performance. However, their generalization often suffers due to domain-specific data distributions, making an effective scene-based categorization of samples necessary to improve their reliability across diverse domains. Manual captioning, though valuable, is both labor-intensive and time-consuming, creating a bottleneck in the data annotation process. Large Visual Language Models (LVLMs) present a compelling solution by automating image analysis and categorization through contextual queries, often without requiring retraining for new categories. In this study, we evaluate the capabilities of LVLMs, including GPT-4 and LLaVA, to understand and classify urban traffic scenes on both an in-house dataset and the BDD100K. We propose a scalable captioning pipeline that integrates state-of-the-art models, enabling a flexible deployment on new datasets. Our analysis, combining quantitative metrics with qualitative insights, demonstrates the effectiveness of LVLMs to understand urban traffic scenarios and highlights their potential as an efficient tool for datadriven advancements in autonomous driving.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Esteban, Rivera and Jannik, Lübberstedt and Uhlemann, Nico and Lienkamp, Markus},
	month = jan,
	year = {2025},
	note = {arXiv:2501.17131 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:files/25/Esteban et al. - 2025 - Scenario Understanding of Traffic Scenes Through Large Visual Language Models.pdf:application/pdf},
}

@inproceedings{hong_cogagent_2024,
	address = {Seattle, WA, USA},
	title = {{CogAgent}: {A} {Visual} {Language} {Model} for {GUI} {Agents}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5300-6},
	shorttitle = {{CogAgent}},
	url = {https://ieeexplore.ieee.org/document/10655402/},
	doi = {10.1109/CVPR52733.2024.01354},
	abstract = {People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120⇥1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on ﬁve text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks—Mind2Web and AITW, advancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM .},
	language = {en},
	urldate = {2025-03-06},
	booktitle = {2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and Tang, Jie},
	month = jun,
	year = {2024},
	pages = {14281--14290},
	file = {PDF:files/26/Hong et al. - 2024 - CogAgent A Visual Language Model for GUI Agents.pdf:application/pdf},
}

@inproceedings{jain_semantic_2024,
	address = {Jeju Island, Korea, Republic of},
	title = {Semantic {Understanding} of {Traffic} {Scenes} with {Large} {Vision} {Language} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-4881-1},
	url = {https://ieeexplore.ieee.org/document/10588373/},
	doi = {10.1109/IV55156.2024.10588373},
	abstract = {This paper investigates the integration of Large Vision Language Models (LVLMs) with multi-sensor information, including visual and localization data from cameras and LiDAR data to a holistic understanding of traffic videos. Traffic scene understanding is a challenging problem. With complex interaction between the road actors, infrastructure, and traffic rules, it is often difficult to answer questions related to road safety, pedestrian safety, safe maneuvering characteristics, and human factors. Typical processes use a single task-oriented neural network model and combine them through semantic and symbolic reasoning. These processes often suffer from reasoning bias and incompleteness. In recent years, LVLMs have opened new avenues to perceive spatiotemporal information. These models can leverage the large knowledge base from the world and summarize spatiotemporal information effectively. The interactive nature of most of these systems allows humans to directly interact in a visual question-answering mode.},
	language = {en},
	urldate = {2025-03-06},
	booktitle = {2024 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	publisher = {IEEE},
	author = {Jain, Sandesh and Thapa, Surendrabikram and Chen, Kuan-Ting and Abbott, A. Lynn and Sarkar, Abhijit},
	month = jun,
	year = {2024},
	pages = {1580--1587},
	file = {PDF:files/27/Jain et al. - 2024 - Semantic Understanding of Traffic Scenes with Large Vision Language Models.pdf:application/pdf},
}

@misc{vishal_eyes_2024,
	title = {Eyes on the {Road}: {State}-of-the-{Art} {Video} {Question} {Answering} {Models} {Assessment} for {Traffic} {Monitoring} {Tasks}},
	shorttitle = {Eyes on the {Road}},
	url = {http://arxiv.org/abs/2412.01132},
	doi = {10.48550/arXiv.2412.01132},
	abstract = {Recent advances in video question answering (VideoQA) offer promising applications, especially in traffic monitoring, where efficient video interpretation is critical. Within ITS, answering complex, real-time queries like "How many red cars passed in the last 10 minutes?" or "Was there an incident between 3:00 PM and 3:05 PM?" enhances situational awareness and decision-making. Despite progress in vision-language models, VideoQA remains challenging, especially in dynamic environments involving multiple objects and intricate spatiotemporal relationships. This study evaluates state-of-the-art VideoQA models using non-benchmark synthetic and real-world traffic sequences. The framework leverages GPT-4o to assess accuracy, relevance, and consistency across basic detection, temporal reasoning, and decomposition queries. VideoLLaMA-2 excelled with 57\% accuracy, particularly in compositional reasoning and consistent answers. However, all models, including VideoLLaMA-2, faced limitations in multi-object tracking, temporal coherence, and complex scene interpretation, highlighting gaps in current architectures. These findings underscore VideoQA's potential in traffic monitoring but also emphasize the need for improvements in multi-object tracking, temporal reasoning, and compositional capabilities. Enhancing these areas could make VideoQA indispensable for incident detection, traffic flow management, and responsive urban planning. The study's code and framework are open-sourced for further exploration: https://github.com/joe-rabbit/VideoQA\_Pilot\_Study},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Vishal, Joseph Raj and Basina, Divesh and Choudhary, Aarya and Chakravarthi, Bharatesh},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01132 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:files/28/Vishal et al. - 2024 - Eyes on the Road State-of-the-Art Video Question Answering Models Assessment for Traffic Monitoring.pdf:application/pdf},
}

@misc{gao_application_2025,
	title = {Application of {Vision}-{Language} {Model} to {Pedestrians} {Behavior} and {Scene} {Understanding} in {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2501.06680},
	doi = {10.48550/arXiv.2501.06680},
	abstract = {Autonomous driving (AD) has experienced significant improvements in recent years and achieved promising 3D detection, classification, and localization results. However, many challenges remain, e.g. semantic understanding of pedestrians’ behaviors, and downstream handling for pedestrian interactions. Recent studies in applications of Large Language Models (LLM) and Vision-Language Models (VLM) have achieved promising results in scene understanding and high-level maneuver planning in diverse traffic scenarios. However, deploying the billion-parameter LLMs to vehicles requires significant computation and memory resources. In this paper, we analyzed effective knowledge distillation of LLM semantic labels to smaller Vision networks, which can be used for the semantic representation of complex scenes for downstream decision-making for planning and control.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Gao, Haoxiang and Zhao, Yu},
	month = jan,
	year = {2025},
	note = {arXiv:2501.06680 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Robotics, Computer Science - Machine Learning},
	file = {PDF:files/31/Gao and Zhao - 2025 - Application of Vision-Language Model to Pedestrians Behavior and Scene Understanding in Autonomous D.pdf:application/pdf},
}

@misc{zhou_vision_2024,
	title = {Vision {Language} {Models} in {Autonomous} {Driving}: {A} {Survey} and {Outlook}},
	shorttitle = {Vision {Language} {Models} in {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2310.14414},
	doi = {10.48550/arXiv.2310.14414},
	abstract = {The applications of Vision-Language Models (VLMs) in the field of Autonomous Driving (AD) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By incorporating language data, driving systems can gain a better understanding of real-world environments, thereby enhancing driving safety and efficiency. In this work, we present a comprehensive and systematic survey of the advances in vision language models in this domain, encompassing perception and understanding, navigation and planning, decision-making and control, end-to-end autonomous driving, and data generation. We introduce the mainstream VLM tasks in AD and the commonly utilized metrics. Additionally, we review current studies and applications in various areas and summarize the existing language-enhanced autonomous driving datasets thoroughly. Lastly, we discuss the benefits and challenges of VLMs in AD and provide researchers with the current research gaps and future trends.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Zhou, Xingcheng and Liu, Mingyu and Yurtsever, Ekim and Zagar, Bare Luka and Zimmer, Walter and Cao, Hu and Knoll, Alois C.},
	month = jun,
	year = {2024},
	note = {arXiv:2310.14414 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {PDF:files/32/Zhou et al. - 2024 - Vision Language Models in Autonomous Driving A Survey and Outlook.pdf:application/pdf},
}

@misc{gong_multimodal-gpt_2023,
	title = {{MultiModal}-{GPT}: {A} {Vision} and {Language} {Model} for {Dialogue} with {Humans}},
	shorttitle = {{MultiModal}-{GPT}},
	url = {http://arxiv.org/abs/2305.04790},
	doi = {10.48550/arXiv.2305.04790},
	abstract = {We present a vision and language model named MultiModal-GPT to conduct multiround dialogue with humans. MultiModal-GPT is capable of following diverse instructions, such as generating detailed captions, counting specific objects, and addressing general inquiries posed by users. The model is efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) incorporated in both the gated-cross-attention and self-attention components of the language model. Our approach involves constructing instruction templates that incorporate vision and language data for multi-modality instruction tuning, enabling the model to comprehend and adhere to human directives. We observe that the quality of training data is crucial for effective dialogue performance, as a limited dataset with short responses may cause the model to generate brief replies to any instruction. To further enhance MultiModal-GPT’s conversational abilities, we employ language-only instructionfollowing data for joint training alongside visual-language instructions. Utilizing the same instruction template for both types of data results in a significant improvement in dialogue performance. Our experiments demonstrate MultiModal-GPT’s proficiency in maintaining continuous dialogues with humans. The code, dataset, and demo can be found at https://github.com/open-mmlab/Multimodal-GPT.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Gong, Tao and Lyu, Chengqi and Zhang, Shilong and Wang, Yudong and Zheng, Miao and Zhao, Qian and Liu, Kuikun and Zhang, Wenwei and Luo, Ping and Chen, Kai},
	month = jun,
	year = {2023},
	note = {arXiv:2305.04790 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {PDF:files/33/Gong et al. - 2023 - MultiModal-GPT A Vision and Language Model for Dialogue with Humans.pdf:application/pdf},
}

@misc{zhang_mm-rlhf_2025,
	title = {{MM}-{RLHF}: {The} {Next} {Step} {Forward} in {Multimodal} {LLM} {Alignment}},
	shorttitle = {{MM}-{RLHF}},
	url = {http://arxiv.org/abs/2502.10391},
	doi = {10.48550/arXiv.2502.10391},
	abstract = {Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing 120k fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across 10 distinct dimensions and 27 benchmarks, with results demonstrating significant and consistent improvements in performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a 19.5\% increase in conversational abilities and a 60\% improvement in safety.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Zhang, Yi-Fan and Yu, Tao and Tian, Haochen and Fu, Chaoyou and Li, Peiyan and Zeng, Jianshu and Xie, Wulin and Shi, Yang and Zhang, Huanyu and Wu, Junkang and Wang, Xue and Hu, Yibo and Wen, Bin and Yang, Fan and Zhang, Zhang and Gao, Tingting and Zhang, Di and Wang, Liang and Jin, Rong and Tan, Tieniu},
	month = feb,
	year = {2025},
	note = {arXiv:2502.10391 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {PDF:files/37/Zhang et al. - 2025 - MM-RLHF The Next Step Forward in Multimodal LLM Alignment.pdf:application/pdf},
}

@misc{zhang_llama-adapter_2024,
	title = {{LLaMA}-{Adapter}: {Efficient} {Fine}-tuning of {Language} {Models} with {Zero}-init {Attention}},
	shorttitle = {{LLaMA}-{Adapter}},
	url = {http://arxiv.org/abs/2303.16199},
	doi = {10.48550/arXiv.2303.16199},
	abstract = {With the rising tide of large language models (LLMs), there has been a growing interest in developing general-purpose instruction-following models, e.g., ChatGPT. To this end, we present LLaMA-Adapter, a lightweight adaption method for efficient instruction tuning of LLaMA. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning. Specifically, a zero-initialized attention mechanism is proposed. It adopts a learnable zero gating to adaptively inject the instructional cues into LLaMA within self-attention layers, contributing to a stable training process and superior final performance. In this way, LLaMA-Adapter can generate high-quality responses to diverse language instructions, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, by incorporating an image encoder, our approach can be simply extended to a Multi-modal LLM for image-conditioned instruction following, which achieves superior multi-modal reasoning capacity on several popular benchmarks (MME, MMBench, LVLM-eHub). Furthermore, we also verify the proposed zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa, CLIP) on traditional vision and language tasks, demonstrating the effectiveness and generalizability of our approach. Code and models are released at https://github.com/OpenGVLab/LLaMA-Adapter.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
	month = sep,
	year = {2024},
	note = {arXiv:2303.16199 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multimedia},
	file = {PDF:files/38/Zhang et al. - 2024 - LLaMA-Adapter Efficient Fine-tuning of Language Models with Zero-init Attention.pdf:application/pdf},
}

@misc{wang_self-instruct_2023,
	title = {Self-{Instruct}: {Aligning} {Language} {Models} with {Self}-{Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	doi = {10.48550/arXiv.2212.10560},
	abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = may,
	year = {2023},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:files/39/Wang et al. - 2023 - Self-Instruct Aligning Language Models with Self-Generated Instructions.pdf:application/pdf},
}

@article{gilardi_chatgpt_2023,
	title = {{ChatGPT} outperforms crowd workers for text-annotation tasks},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2305016120},
	doi = {10.1073/pnas.2305016120},
	abstract = {Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (
              n
              = 6,183), we show that ChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd workers by about 25 percentage points on average, while ChatGPT’s intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.},
	language = {en},
	number = {30},
	urldate = {2025-03-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
	month = jul,
	year = {2023},
	pages = {e2305016120},
	file = {PDF:files/40/Gilardi et al. - 2023 - ChatGPT outperforms crowd workers for text-annotation tasks.pdf:application/pdf},
}

@misc{jia_visual_2022,
	title = {Visual {Prompt} {Tuning}},
	url = {http://arxiv.org/abs/2203.12119},
	doi = {10.48550/arXiv.2203.12119},
	abstract = {The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, i.e., full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1\% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost. Code is available at github.com/kmnp/vpt.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
	month = jul,
	year = {2022},
	note = {arXiv:2203.12119 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:files/41/Jia et al. - 2022 - Visual Prompt Tuning.pdf:application/pdf},
}

@misc{peng_kosmos-2_2023,
	title = {Kosmos-2: {Grounding} {Multimodal} {Large} {Language} {Models} to the {World}},
	shorttitle = {Kosmos-2},
	url = {http://arxiv.org/abs/2306.14824},
	doi = {10.48550/arXiv.2306.14824},
	abstract = {We introduce KOSMOS-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., “[text span](bounding boxes)”, where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GRIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), KOSMOS-2 integrates the grounding capability into downstream applications. We evaluate KOSMOS-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu},
	month = jul,
	year = {2023},
	note = {arXiv:2306.14824 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {PDF:files/74/Peng et al. - 2023 - Kosmos-2 Grounding Multimodal Large Language Models to the World.pdf:application/pdf},
}

@inproceedings{fang_eva_2023,
	address = {Vancouver, BC, Canada},
	title = {{EVA}: {Exploring} the {Limits} of {Masked} {Visual} {Representation} {Learning} at {Scale}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-0129-8},
	shorttitle = {{EVA}},
	url = {https://ieeexplore.ieee.org/document/10203681/},
	doi = {10.1109/CVPR52729.2023.01855},
	abstract = {We launch EVA, a vision-centric foundation model to Explore the limits of Visual representation at scAle using only publicly accessible data. EVA is a vanilla ViT pretrained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVIS dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models.},
	language = {en},
	urldate = {2025-03-06},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
	month = jun,
	year = {2023},
	pages = {19358--19369},
	file = {PDF:files/76/Fang et al. - 2023 - EVA Exploring the Limits of Masked Visual Representation Learning at Scale.pdf:application/pdf},
}

@misc{zhu_minigpt-4_2023,
	title = {{MiniGPT}-4: {Enhancing} {Vision}-{Language} {Understanding} with {Advanced} {Large} {Language} {Models}},
	shorttitle = {{MiniGPT}-4},
	url = {http://arxiv.org/abs/2304.10592},
	doi = {10.48550/arXiv.2304.10592},
	abstract = {The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous visionlanguage models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model’s generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.},
	language = {en},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
	month = oct,
	year = {2023},
	note = {arXiv:2304.10592 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:files/78/Zhu et al. - 2023 - MiniGPT-4 Enhancing Vision-Language Understanding with Advanced Large Language Models.pdf:application/pdf},
}

@inproceedings{alayrac_flamingo_2022,
	title = {Flamingo: a {Visual} {Language} {Model} for {Few}-{Shot} {Learning}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/960a172bc7fbf0177ccccbb411a7d800-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bińkowski, Mikoł aj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karén},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {23716--23736},
}

@inproceedings{li_blip-2_2023,
	title = {{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}},
	shorttitle = {{BLIP}-2},
	url = {https://proceedings.mlr.press/v202/li23q.html},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model’s emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	language = {en},
	urldate = {2025-03-06},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {19730--19742},
	file = {Full Text PDF:files/98/Li et al. - 2023 - BLIP-2 Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Mode.pdf:application/pdf},
}

@misc{peng_instruction_2023,
	title = {Instruction {Tuning} with {GPT}-4},
	url = {http://arxiv.org/abs/2304.03277},
	doi = {10.48550/arXiv.2304.03277},
	abstract = {Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.},
	urldate = {2025-03-06},
	publisher = {arXiv},
	author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03277 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:files/101/Peng et al. - 2023 - Instruction Tuning with GPT-4.pdf:application/pdf;Snapshot:files/102/2304.html:text/html},
}

@phdthesis{wu_comprehensive_2025,
	title = {A {Comprehensive} {Exploration} of {Video} {Understanding}: {Perspectives} on {Sampling}, {Backbone}, {Representation}, and {Cross}-{Modal} {Learning}},
	author = {Wu, Wenhao},
	year = {2025},
}

@inproceedings{karpathy_deep_2015,
	title = {Deep visual-semantic alignments for generating image descriptions},
	author = {Karpathy, Andrej and Fei-Fei, Li},
	year = {2015},
	pages = {3128--3137},
}

@inproceedings{dosovitskiy_image_2021,
	title = {An image is worth 16x16 words},
	volume = {7},
	booktitle = {{arXiv} preprint {arXiv}:2010.11929},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain},
	year = {2021},
}

@inproceedings{houlsby_parameter-efficient_2019,
	title = {Parameter-efficient transfer learning for {NLP}},
	isbn = {2640-3498},
	publisher = {PMLR},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	year = {2019},
	pages = {2790--2799},
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}

@misc{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2212.04356},
  year={2023},
  note={OpenAI Whisper}
}

@inproceedings{fang2019dada,
  title={DADA-2000: Can Driving Accident be Predicted by Driver Attention? Analyzed by A Benchmark},
  author={Fang, Jianwu and Yan, Dingxin and Qiao, Jiahuan and Xue, Jianru and Wang, Huaici},
  booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
  pages={4303--4309},
  year={2019},
  organization={IEEE},
  doi={10.1109/ITSC.2019.8917218}
}