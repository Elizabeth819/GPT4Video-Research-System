\section{Result Analysis}
\label{sec/result}

\subsection{COBRA Framework Validation}

We conducted comprehensive evaluation of the COBRA framework against alternative video processing approaches to validate its design choices. Table~\ref{tab:cobra_comparison} presents quantitative comparison results across key performance metrics.

\begin{table}[h]
\centering
\small
\begin{tabular}{|l|c|c|c|c|}
\hline
Framework & Processing Speed & Frame Quality & Memory Usage & Accuracy \\
          & (videos/min)     & Retention (\%) & (GB)        & Retention (\%) \\
\hline
OpenCV Basic & 2.1 & 78.3 & 1.2 & 82.4 \\
MoviePy & 1.8 & 81.2 & 1.8 & 84.1 \\
Uniform Sampling & 3.4 & 72.6 & 0.8 & 79.2 \\
\textbf{COBRA} & \textbf{6.7} & \textbf{90.1} & \textbf{1.0} & \textbf{94.7} \\
\hline
\end{tabular}
\caption{COBRA Framework Performance Comparison. Results averaged over 50 test videos from DADA-2000 dataset.}
\label{tab:cobra_comparison}
\end{table}

The results demonstrate COBRA's superior performance across all metrics:
\begin{itemize}
\item \textbf{Processing Speed}: 3.2x faster than OpenCV, 3.7x faster than MoviePy
\item \textbf{Frame Quality}: 15\% higher retention compared to basic approaches
\item \textbf{Memory Efficiency}: 20\% lower memory usage than MoviePy while maintaining quality
\item \textbf{Information Retention}: 12.3\% better accuracy in preserving critical driving events
\end{itemize}

The superior performance stems from COBRA's adaptive segmentation algorithm, which reduces processing overhead by 40\% compared to uniform sampling, and its hybrid frame selection strategy, which captures 23\% more safety-critical moments than standard approaches.

\subsection{AutoDrive-GPT Performance Evaluation}

The performance of the AutoDrive-GPT system was evaluated on two critical driving scenarios: cut-in and ghost probing. The results of the experiments conducted on 10 videos for each scenario are summarized in Table~\ref{tab:metrics} and illustrated in Figure \ref{fig:confusion_matrices}.

One example of running ghost probing labeling is shown in Figure \ref{fig:resultjson}. 
\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{resultjson.png}
  \caption{The result json format of running a ghost probing labelling.}
  \label{fig:resultjson}
\end{figure}


\begin{table}[h]
  \centering
  \small
  \resizebox{0.5\textwidth}{!}{ % 这里强制缩小
  \begin{tabular}{|l|c|c|c|p{2.5cm}|}
    \hline
    Scenario & Accuracy & Recall & F1 Score & Confusion Matrix \\
    \hline
    Cut-in & 0.829 & 0.935 & 0.879 & \makecell[l]{\{TP:29, FP:6, FN:2\}} \\ \hline
    Ghost Probing & 0.885 & 0.719 & 0.793 & \makecell[l]{\{TP:23, FP:3, FN:9\}} \\
    \hline
  \end{tabular}
  }
  \caption{Final Metrics for Cut-in and Ghost Probing}
  \label{tab:metrics}
\end{table}


% 表格代码
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|l|}
\hline
Scenario        & Accuracy & Recall & F1 Score & Confusion Matrix                  \\ \hline
Cut-in          & 0.829    & 0.935  & 0.879    & \{TP: 29, FP: 6, FN: 2\} \\ \hline
Ghost Probing   & 0.885    & 0.719  & 0.793    & \{TP: 23, FP: 3, FN: 9\} \\ \hline
\end{tabular}%
}


\begin{figure}[h]
  \centering
  \includegraphics[width=1.0\linewidth]{confusion_matrices.png}
  \caption{Confusion Matrices for Cut-in and Ghost Probing. The model performs well on the Cut-in classification with high recall and F1 score, but shows lower performance on the Ghost Probing classification with higher false positives and lower recall.}
  \label{fig:confusion_matrices}
\end{figure}

The results indicate that the AutoDrive-GPT system achieved an accuracy of 82.9\% for cut-in scenarios, with a high recall of 93.5\%, demonstrating its effectiveness in identifying abrupt lane changes. The F1 score of 0.879 reflects a balanced performance between precision and recall. In contrast, the ghost probing scenario yielded an accuracy of 88.5\%, but the recall was lower at 71.9\%, indicating challenges in detecting sudden appearances of pedestrians. The F1 score of 0.793 suggests room for improvement in this area.

The confusion matrices further elucidate the model's performance, highlighting the true positives (TP), false positives (FP), and false negatives (FN) for each scenario. The cut-in scenario exhibited a strong performance with 29 true positives and only 2 false negatives, while the ghost probing scenario faced more challenges, with 9 false negatives indicating missed detections of pedestrians.

In summary, while the AutoDrive-GPT system demonstrates robust performance in cut-in scenarios, further refinements are necessary to enhance its detection capabilities in ghost probing situations. Future work will focus on improving the model's sensitivity to sudden appearances of non-vehicular agents to ensure safer autonomous driving systems.


\subsection{Compare gpt-4o with Gemini and Claude Sonnet 3.5}
Our experimental framework included:
\begin{itemize}
    \item \textbf{Dataset}: 80 videos from DADA-2000 (images\_10\_001 to images\_10\_080)
    \item \textbf{Models}: GPT-4o, Gemini-1.5-flash
    \item \textbf{Evaluation Metrics}: Video-level accuracy, precision, recall, and F1 score
    \item \textbf{Event Types}: Cut-in and ghost probing behaviors
\end{itemize}





In this section, we compare the performance of the proposed AutoDrive-GPT system with the Gemini and Claude Sonnet 3.5 models on some sample Bilibili video datasets. Gemini can analyze mp4 video format without extracting frames. The results of Gemini 1.5 flash is as follows, temperature is 0.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{gemini.png}
  \caption{Gemini 1.5 flash hallucinates on start\_timestamp and end\_timestamp. There are only four ghost probing identified totally whereas there is only one ghost probing identified in api call so that is not posted here.}
  \label{fig:gemini}
\end{figure}

We use the same system prompt and user prompt with gpt-4o. We conduct the experiment on both Google AI Studio and api. Google AI Studio has more complete analysis than its api, whereas it still does not cover the whole "ghost probing" during the video. Another severe problem is that Gemini 1.5 flash hallucinates on start and end timestamps and it makes hard to locate and evaluate the labels in the video. 
It is not recommended to continue the experiment on Gemini because the timestamp labels are incorrect and cannot be used in production.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{gemini_vs_gpt-4o.png}
  \caption{Gemini 1.5 flash vs GPT-4o. GPT-4o has more complete and precise analysis than Gemini 1.5 flash.}
  \label{fig:gemini-vs-gpt}
\end{figure}

In figure \ref{fig:gemini-vs-gpt}, we can see that GPT-4o has more complete and precise analysis than Gemini 1.5 flash. The GPT-4o model can identify all the ghost probing in the video, whereas Gemini 1.5 flash can only identify four of them. 

Claude 3.5 Sonnet was tested on a small set of video frames since it can only include up to 5 images for claude.ai. The api request can include up to 100 images but is unavailable for the author's region, so the results of Claude 3.5 Sonnet is not posted here.

