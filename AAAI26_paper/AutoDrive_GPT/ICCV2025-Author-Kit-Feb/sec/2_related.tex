\section{Related Works}
\label{sec:related}

\textbf{Interpretable Autonomous Driving.} DriveGPT4 \cite{xu2024drivegpt4} is a multimodal large language model designed to integrate video-text data for enhancing both interpretability and end-to-end control in autonomous driving. DriveGPT4 utilized a fine-tuned LLaMA2 architecture combined with video-text instruction datasets to address both interpretation and control tasks in real-world driving scenarios. However, its reliance on domain-specific instruction datasets restricts its generalizability to diverse driving environments, such as surrounding vehicles or dynamic pedestrians, it only focuses on ego vehicle control.

\textbf{GPT-based Motion Planner.} GPT-Driver \cite{mao2023gpt} is a novel approach that transforms the OpenAI GPT-3.5 model into a motion planner for autonomous driving. By reformulating motion planning as a language modelling problem, it reprensents planner perception input and outputs driving trajactories through language description of coordinate postions. A key innovation is the prompting-reasoning-finetuning strategy, which simulates the model's numerical reasoning potential. The generalization and reasoning ability of GPT-3.5 enables it to tackle long-tail driving scenarios that are generally challenging to other models. In our work, we extend the GPT-based motion planner to a multimodal reasoning system that incorporates both video and audio inputs for enhanced interpretability and prediction accuracy.

\textbf{Long-tail Event Detection.} Long-tail event detection in autonomous driving is a challenging task due to the rarity of certain events and the imbalanced distribution of event classes. TOKEN \cite{tian2024tokenize} introduces an innovative approach to handling long-tail events by tokenizing the driving environment into object-level representations. Unlike traditional end-to-end planner, TOKEN leverages a pre-trained end-to-end driving model (PARA-Drive) to generate semantically rich, object-centric tokens. Our work builds upon GPT-4o's multimodal reasoning capabilities to enhance the interpretability and prediction accuracy of long-tail driving events, such as sudden pedestrian appearances or cut-in.