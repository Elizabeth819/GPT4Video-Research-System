\begin{abstract}
  The rapid development of autonomous driving technology has resulted in a substantial increase in video data generated by self-driving vehicles. Efficiently understanding and interpreting this data is crucial for enhancing autonomous driving systems. This paper explores the potential of GPT-4o, a large language model, to serve as a powerful tool for autonomous driving video tagging and reasoning. By combining the rich video data with GPT-4o's multimodal reasoning capabilities, we propose a structured approach, AutoDrive-GPT, to improve autonomous driving behavior annotation and prediction. We develop AutoDrive-GPT, which leverages GPT-4o prompt tuning for enhanced behavior prediction. Additionally, we build a tool called Cobra that chunks video data into smaller intervals, samples frames, and feeds them into GPT-4o for multimodal reasoning. Our methods are evaluated on the Bilibili and DADA-2000 dataset, demonstrating that our approach outperforms Gemini 1.5 flash. The results indicate that AutoDrive-GPT significantly enhances the interpretability accuracy of autonomous driving systems, particularly in challenging scenarios such as sudden pedestrian appearances (ghost probing) and cut-in events.
\end{abstract}