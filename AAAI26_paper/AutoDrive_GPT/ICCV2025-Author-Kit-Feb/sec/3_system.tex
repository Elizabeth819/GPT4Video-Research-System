\section{System Architecture}
\label{{sec/3_system}

The proposed AutoDrive-GPT system consists of two main components: COBRA (COmprehensive Behavioral Research Assistant) and GPT-4o. COBRA is a novel video processing framework specifically designed for autonomous driving video analysis, responsible for intelligent preprocessing of multimodal automotive data before feeding into GPT-4o for advanced reasoning. The system architecture is illustrated in Figure~\ref{fig:arch}.

\subsection{COBRA Framework Technical Specification}

COBRA implements a comprehensive 5-stage pipeline optimized for autonomous driving video analysis, with each stage designed to maximize information extraction while minimizing computational cost:

\textbf{Stage 1: Adaptive Video Segmentation}
COBRA employs an adaptive temporal segmentation algorithm that dynamically partitions input videos based on scene complexity. For standard driving scenarios, videos are segmented into 10-second intervals. However, for high-action sequences (detected through optical flow analysis), segment length is reduced to 5 seconds to capture fine-grained temporal dynamics. The segmentation algorithm uses:
\begin{equation}
t_{seg} = \begin{cases} 
5s & \text{if } \sum_{i=1}^{n} ||\mathbf{v}_i|| > \theta_{motion} \\
10s & \text{otherwise}
\end{cases}
\end{equation}
where $\mathbf{v}_i$ represents optical flow vectors and $\theta_{motion} = 2.5$ pixels/frame is the motion threshold.

\textbf{Stage 2: Intelligent Frame Sampling}
Within each segment, COBRA uses a hybrid sampling strategy combining uniform temporal sampling with saliency-based selection. The framework extracts $N_{frames} = 10$ frames per segment using:
\begin{itemize}
\item \textit{Uniform sampling}: 7 frames at regular intervals to maintain temporal consistency
\item \textit{Saliency sampling}: 3 frames selected based on visual complexity scores using Laplacian variance
\end{itemize}
This hybrid approach ensures both temporal coverage and capture of visually significant moments critical for safety analysis.

\textbf{Stage 3: Multimodal Audio Processing}
COBRA implements a sophisticated audio processing pipeline:
\begin{enumerate}
\item \textit{Audio extraction}: Uses FFmpeg to extract audio tracks at 16kHz sampling rate
\item \textit{Noise reduction}: Applies spectral subtraction to remove engine noise (frequencies below 200Hz)
\item \textit{Speech-to-text}: Employs Azure Whisper API with temperature=0 for deterministic transcription
\item \textit{Semantic alignment}: Synchronizes audio transcripts with corresponding video segments using timestamp alignment
\end{enumerate}

\textbf{Stage 4: Structured Data Packaging}
COBRA formats the multimodal data into GPT-4o compatible input structures:
\begin{itemize}
\item Frame images encoded as base64 strings (max 20 images per API call)
\item Audio transcripts formatted as contextual prompts
\item Temporal metadata including timestamp ranges and frame indices
\item Structured JSON schema for consistent output formatting
\end{itemize}

\textbf{Stage 5: Batch Processing and Result Management}
COBRA implements an efficient batch processing system with:
\begin{itemize}
\item \textit{Parallel processing}: Multi-threaded frame extraction and audio processing
\item \textit{Checkpoint mechanism}: Resume capability for interrupted processing
\item \textit{Error handling}: Automatic retry logic with exponential backoff
\item \textit{Result validation}: JSON schema validation and completeness checks
\end{itemize}

\textbf{COBRA vs. Alternative Frameworks}
To validate COBRA's design choices, we conducted comparative analysis against existing video processing frameworks. Table~\ref{tab:cobra_comparison} shows COBRA's superior performance in processing speed and accuracy metrics compared to generic video processing tools like OpenCV's VideoCapture and MoviePy. COBRA achieves 3.2x faster processing speed and 15\% higher frame quality retention compared to standard approaches, primarily due to its adaptive segmentation and intelligent sampling strategies.

The complete COBRA processing workflow can be expressed algorithmically as:
\begin{algorithm}[h]
\caption{COBRA Video Processing Pipeline}
\begin{algorithmic}[1]
\State \textbf{Input:} Video file $V$, configuration parameters $\{t_{seg}, N_{frames}, \theta_{motion}\}$
\State \textbf{Output:} Structured multimodal data package $\mathcal{D}$
\State $segments \leftarrow$ AdaptiveSegmentation($V, t_{seg}, \theta_{motion}$)
\For{each $segment \in segments$}
    \State $frames \leftarrow$ HybridSampling($segment, N_{frames}$)
    \State $audio \leftarrow$ ExtractAudio($segment$)
    \State $transcript \leftarrow$ WhisperTranscribe($audio$)
    \State $\mathcal{D} \leftarrow \mathcal{D} \cup$ PackageData($frames, transcript$)
\EndFor
\State \textbf{return} $\mathcal{D}$
\end{algorithmic}
\end{algorithm}

This systematic approach ensures reproducible results and provides the technical foundation necessary for autonomous driving behavior analysis. The modular design allows for easy adaptation to different video formats and analysis requirements while maintaining consistent performance across diverse driving scenarios.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{architecture.png}
  \caption{AutoDrive-GPT system architecture.}
  \label{fig:arch}
\end{figure}