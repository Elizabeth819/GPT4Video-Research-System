\section{Introduction}
\label{sec:intro}

The rapid development of autonomous driving (AD) technology has given rise to a deluge of video data, as self-driving vehicles continuously record their surroundings to safely navigate complex, dynamic environments. Efficiently interpretation of this video data remains a significant challenge, as conventional video analysis methods typically rely on handcrafted features or annotation-based supervised learning models \cite{yu_bdd100k_2020,fang_eva_2023,zhang_mm-rlhf_2025}, which are time-consuming and often fail to generalize across dynamic driving scenarios. Traditional methods often focus on specific tasks, such as object detection, lane line recognition, with each task typically handled by a separate model. This modular approach exhibits clear difficulties when dealing with complex scenarios or long-tail cases, making it difficult to generalize to unseen actions and scenarios.

Concurrently, significant progress in large language models (LLMs) \cite{peng_instruction_2023} and vision-language models (VLMs) \cite{alayrac_flamingo_2022,gao_application_2025,fang_eva_2023, karpathy_deep_2015, hong_cogagent_2024, wu_deepseek-vl2_2024, lu_deepseek-vl_2024, tian_drivevlm_2024}, such as GPT-4o \cite{hurst2024gpt} and GPT-4 \cite{achiam2023gpt}, have demonstrated remarkable promise in addressing these issues. VLMs, in particular, excel at multimodal data interpretation, demonstrating strong capabilities in action recognition, and structured output, and zero-short generalization \cite{peng_instruction_2023, fu_drive_2023}. Their proficiency in comprehensively analyzing complex traffic scenes and generating structured insights suggests that they can effectively overcome many of the challenges associated with video captioning and understanding within autonomous driving context \cite{tian_drivevlm_2024, ma_dolphins_2025, vishal_eyes_2024,sima_drivelm_2025}.

By combining the rich video data generated by self-driving vehicles with the powerful multimodal reasoning capabilities of GPT-4o, researchers can develop robust systems for automatically tagging and annotating these video streams. This would enable the efficient extraction of relevant information, such as the identification of traffic participants, road infrastructure, and environmental conditions, which are essential for understanding the context and informing the decision-making process of autonomous driving systems. 

Leveraging these advancements, we propose an innovative approach specially designed to address the limitations of existing methods in autonomous driving video analysis. Our methodology introduces the following key contributions: 

\begin{itemize}
  \item We propose AutoDrive-GPT, a novel automated tagging and annotation method based on GPT-4o, capable of effectively identifying and interpreting complex and dynamic driving scenarios. This approach facilitates the accurate and rapid extraction of critical information, such as traffic participants movement, road infrastructure, significantly enhancing the context-awareness and decision-making capabilities of downstream autonomous driving systems.
  \item We introduce Cobra, an efficient video processing framework that intelligently chunks and samples video data to facilitates GPT-4o analysis.
  \item We conduct extensive experiments using the Bibili dataset, demonstrating that our approach outperforms state-of-the-art methods across multiple metrics.
  \item We provide detailed analysis and insights into the capabilities and limitations of using large language models for autonomous driving applications.
\end{itemize}

Through these contributions, our work significantly advances the state-of-the-art in autonomous driving video analysis, demonstrating that the integration of sophisticated multimodal models with efficient processing frameworks can effectively meet the demands of real-world AD applications. 

Our work uniquely innovates in the domain of autonomous driving video annotation by leveraging gpt-4o's multimodal reasoning capabilities integrated with our efficient Cobra video processing framework, specifically addressing the gap in accurately recognizing rapid and safety critical driving actions, such as sudden pedestrian emergance ("ghost probing") and abrupt lane intrusions ("cut-in"), which to our knowledge have historically posed significant difficulties for traditional video analysis methods.

