#!/usr/bin/env python3
"""
Run 8 Failed Videos Retry: é‡æ–°å¤„ç†3ä¸ªå¤±è´¥çš„è§†é¢‘
å¤„ç†å¤±è´¥çš„è§†é¢‘: images_1_019.avi, images_1_025.avi, images_5_003.avi
ä½¿ç”¨ä¸Run 8å®Œå…¨ç›¸åŒçš„é…ç½®
"""

import cv2
import os
import json
import logging
import time
import datetime
from moviepy.editor import VideoFileClip
import pandas as pd
from dotenv import load_dotenv
import base64
import requests
import traceback

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class Run8RetryFailedVideos:
    def __init__(self, output_dir):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        # å¤±è´¥çš„è§†é¢‘åˆ—è¡¨ - éœ€è¦åœ¨initialize_resultsä¹‹å‰å®šä¹‰
        self.failed_videos = [
            "images_1_019.avi",
            "images_1_025.avi", 
            "images_5_003.avi"
        ]
        
        self.setup_logging()
        self.setup_openai_api()
        self.load_ground_truth()
        self.initialize_results()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.timestamp = timestamp
        log_filename = os.path.join(self.output_dir, f"run8_retry_failed_{timestamp}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("Run 8: é‡æ–°å¤„ç†å¤±è´¥è§†é¢‘å¼€å§‹")
        
    def setup_openai_api(self):
        """è®¾ç½®OpenAI API"""
        self.openai_api_key = os.environ.get("OPENAI_API_KEY", "")
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEYæœªè®¾ç½®")
        
        # Azure OpenAIé…ç½®
        self.vision_endpoint = os.environ.get("VISION_ENDPOINT", "")
        self.vision_deployment = os.environ.get("VISION_DEPLOYMENT_NAME", "gpt-4o-global")
        
        if not self.vision_endpoint:
            raise ValueError("VISION_ENDPOINTæœªè®¾ç½®")
            
        self.logger.info(f"Azure OpenAI APIé…ç½®æˆåŠŸ")
        self.logger.info(f"Endpoint: {self.vision_endpoint}")
        self.logger.info(f"Deployment: {self.vision_deployment}")
        self.logger.info(f"Temperature: 0 (ä¸Run 8å®Œå…¨ä¸€è‡´)")
        
    def load_ground_truth(self):
        """åŠ è½½ground truthæ ‡ç­¾"""
        try:
            labels_path = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos/groundtruth_labels.csv"
            if os.path.exists(labels_path):
                self.ground_truth_df = pd.read_csv(labels_path)
                self.logger.info(f"Ground truthæ ‡ç­¾åŠ è½½æˆåŠŸ: {len(self.ground_truth_df)}æ¡è®°å½•")
            else:
                self.logger.warning("Ground truthæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾")
                self.ground_truth_df = None
        except Exception as e:
            self.logger.error(f"åŠ è½½ground truthå¤±è´¥: {str(e)}")
            self.ground_truth_df = None
    
    def get_ground_truth_label(self, video_id):
        """è·å–è§†é¢‘çš„ground truthæ ‡ç­¾"""
        if self.ground_truth_df is None:
            return "unknown"
        
        try:
            video_name = video_id.replace('.avi', '')
            matching_rows = self.ground_truth_df[self.ground_truth_df['video_id'].str.contains(video_name, na=False)]
            
            if len(matching_rows) > 0:
                label = matching_rows.iloc[0]['label']
                return str(label) if pd.notna(label) else "none"
            else:
                return "none"
        except Exception as e:
            self.logger.error(f"è·å–ground truthå¤±è´¥ {video_id}: {str(e)}")
            return "unknown"
        
    def initialize_results(self):
        """åˆå§‹åŒ–ç»“æœç»“æ„"""
        self.results = {
            "experiment_info": {
                "run_id": "Run 8 - Retry Failed Videos",
                "timestamp": self.timestamp,
                "video_count": 3,
                "model": "GPT-4o (Azure)",
                "prompt_version": "Paper_Batch Complex (4-Task) + Few-shot Examples",
                "temperature": 0,
                "max_tokens": 3000,
                "purpose": "é‡æ–°å¤„ç†Run 8ä¸­3ä¸ªå¤±è´¥çš„è§†é¢‘",
                "output_directory": self.output_dir,
                "base_configuration": "ä¸Run 8å®Œå…¨ä¸€è‡´çš„å‚æ•°å’Œprompté…ç½®",
                "failed_videos": self.failed_videos
            },
            "detailed_results": []
        }
        
    def extract_frames_from_video(self, video_path, frame_interval=10, frames_per_interval=10):
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        try:
            clip = VideoFileClip(video_path)
            duration = clip.duration
            
            frames = []
            frames_dir = os.path.join(self.output_dir, "frames_temp")
            os.makedirs(frames_dir, exist_ok=True)
            
            # è®¡ç®—é—´éš”æ•°
            num_intervals = max(1, int(duration / frame_interval))
            
            for interval_idx in range(num_intervals):
                start_time = interval_idx * frame_interval
                end_time = min((interval_idx + 1) * frame_interval, duration)
                
                for frame_idx in range(frames_per_interval):
                    if frames_per_interval == 1:
                        frame_time = start_time + (end_time - start_time) / 2
                    else:
                        frame_time = start_time + (frame_idx / (frames_per_interval - 1)) * (end_time - start_time)
                    
                    if frame_time >= duration:
                        break
                        
                    frame = clip.get_frame(frame_time)
                    frame_filename = f"frame_{interval_idx}_{frame_idx}_{frame_time:.1f}s.jpg"
                    frame_path = os.path.join(frames_dir, frame_filename)
                    
                    cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
                    frames.append(frame_path)
            
            clip.close()
            return frames
        except Exception as e:
            self.logger.error(f"å¸§æå–å¤±è´¥ {video_path}: {str(e)}")
            return []
    
    def get_run8_paper_batch_fewshot_prompt(self, video_id, frame_interval=10, frames_per_interval=10):
        """è·å–ä¸Run 8å®Œå…¨ä¸€è‡´çš„Paper_Batch + Few-shot prompt"""
        return f'''You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

Your job is to take in as an input a transcription of {frame_interval} seconds of audio from a video,
as well as {frames_per_interval} frames split evenly throughout {frame_interval} seconds.
You are to generate and provide a Current Action Summary of the video you are considering ({frames_per_interval}
frames over {frame_interval} seconds), which is generated from your analysis of each frame ({frames_per_interval} in total),
as well as the in-between audio, until we have a full action summary of the video.

**Few-shot Examples for Ghost Probing Detection:**

**Example 1 - TRUE Ghost Probing (Rural Road):**
- Scene: Rural road, child suddenly runs from behind parked car
- Distance: <2 meters when first visible
- Timing: Appears at 3s, requires immediate braking
- Key_actions: "ghost probing"

**Example 2 - TRUE Ghost Probing (Urban Night):**
- Scene: Urban street at night, pedestrian falls directly in vehicle path
- Distance: Within 1-2 meters, sudden appearance
- Timing: Appears at 2s, requires emergency response
- Key_actions: "ghost probing"

**Example 3 - FALSE Positive (Normal Crossing):**
- Scene: Urban intersection with traffic light
- Behavior: Pedestrian crosses at designated crosswalk
- Visibility: Pedestrian visible for 5+ seconds before crossing
- Key_actions: "emergency braking due to pedestrian crossing"

**Example 4 - TRUE Ghost Probing (Highway):**
- Scene: Highway with vehicles and cyclist
- Distance: Cyclist appears suddenly from blind spot <3 meters
- Timing: Requires immediate lane change or braking
- Key_actions: "ghost probing"

**Classification Guidelines:**
Use "ghost probing" ONLY when ALL criteria are met:
1. Object appears suddenly (<3 meters away when first clearly visible)
2. From unexpected location (blind spot, behind obstruction, from side)
3. Requires immediate emergency response (rapid deceleration/swerving)
4. Movement is completely unpredictable given the context
5. High collision risk without immediate action

For borderline cases or normal traffic situations, use descriptive terms like:
- "emergency braking due to pedestrian crossing"
- "cautious driving due to cyclist presence"
- "emergency braking due to child running into street"
- "rapid deceleration for safety"

Your response should be a valid JSON object with the following EXACT structure:
{{
    "video_id": "{video_id}",
    "segment_id": "full_video",
    "Start_Timestamp": "0.0s",
    "End_Timestamp": "{frame_interval}.0s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene",
    "summary": "comprehensive summary of the scene and what happens",
    "actions": "actions taken by the vehicle and driver responses",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "brief description of most important actions (use 'ghost probing' only for true cases matching all criteria)",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

Audio Transcription: [No audio available for this analysis]

Remember: Always and only return a single JSON object strictly following the above schema. Use "ghost probing" only for genuine cases that meet ALL five criteria above.'''
    
    def send_azure_openai_request(self, prompt, images, max_retries=3):
        """å‘é€Azure OpenAIè¯·æ±‚ - ä½¿ç”¨Temperature=0ä¸Run 8ä¿æŒä¸€è‡´ï¼Œå¢åŠ é‡è¯•æœºåˆ¶"""
        encoded_images = []
        for image_path in images:
            try:
                with open(image_path, 'rb') as image_file:
                    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                    encoded_images.append(encoded_string)
            except Exception as e:
                self.logger.error(f"å›¾åƒç¼–ç å¤±è´¥ {image_path}: {str(e)}")
                continue
        
        if not encoded_images:
            return None
            
        content = [{"type": "text", "text": prompt}]
        
        for encoded_image in encoded_images:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}"
                }
            })
        
        data = {
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ],
            "max_tokens": 3000,  # ä¸Run 8ä¿æŒä¸€è‡´
            "temperature": 0     # ä¸Run 8ä¿æŒä¸€è‡´
        }
        
        headers = {
            "Content-Type": "application/json",
            "api-key": self.openai_api_key
        }
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"APIè¯·æ±‚å°è¯• {attempt + 1}/{max_retries}")
                response = requests.post(
                    f"{self.vision_endpoint}/openai/deployments/{self.vision_deployment}/chat/completions?api-version=2024-02-01",
                    headers=headers,
                    json=data,
                    timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°120ç§’
                )
                response.raise_for_status()
                result = response.json()
                return result['choices'][0]['message']['content']
            except Exception as e:
                self.logger.error(f"APIè°ƒç”¨å¤±è´¥ (å°è¯•{attempt + 1}): {str(e)}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 30  # é€’å¢ç­‰å¾…æ—¶é—´
                    self.logger.info(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    return None
    
    def analyze_with_gpt4o(self, video_path, video_id):
        """ä½¿ç”¨GPT-4oåˆ†æè§†é¢‘ï¼ˆä¸Run 8ç›¸åŒé…ç½®ï¼‰"""
        try:
            # æå–å¸§
            frames = self.extract_frames_from_video(video_path)
            if not frames:
                return None
            
            # ç”Ÿæˆprompt
            prompt = self.get_run8_paper_batch_fewshot_prompt(video_id)
            
            # å‘é€APIè¯·æ±‚
            result = self.send_azure_openai_request(prompt, frames)
            
            # æ¸…ç†ä¸´æ—¶å¸§æ–‡ä»¶
            for frame_path in frames:
                if os.path.exists(frame_path):
                    os.remove(frame_path)
            
            return result
        except Exception as e:
            self.logger.error(f"è§†é¢‘åˆ†æå¤±è´¥ {video_id}: {str(e)}")
            return None
    
    def extract_key_actions(self, result_text):
        """æå–key_actions"""
        try:
            if result_text.startswith('```json'):
                result_text = result_text.replace('```json', '').replace('```', '').strip()
            
            result_json = json.loads(result_text)
            return result_json.get('key_actions', '').lower()
        except:
            # å°è¯•æ­£åˆ™è¡¨è¾¾å¼æå–
            import re
            key_actions_match = re.search(r'"key_actions":\s*"([^"]*)"', result_text)
            if key_actions_match:
                return key_actions_match.group(1).lower()
            return result_text.lower()
    
    def evaluate_result(self, video_id, key_actions, ground_truth_label):
        """è¯„ä¼°ç»“æœ - ä½¿ç”¨ä¸Run 8ç›¸åŒçš„è¯„ä¼°é€»è¾‘"""
        has_ghost_probing = "ghost probing" in key_actions
        ground_truth_has_ghost = ground_truth_label != "none"
        
        if has_ghost_probing and ground_truth_has_ghost:
            return "TP"
        elif has_ghost_probing and not ground_truth_has_ghost:
            return "FP"
        elif not has_ghost_probing and ground_truth_has_ghost:
            return "FN"
        else:
            return "TN"
    
    def process_failed_videos(self):
        """å¤„ç†æ‰€æœ‰å¤±è´¥çš„è§†é¢‘"""
        dada_100_base = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos"
        
        for video_id in self.failed_videos:
            video_path = os.path.join(dada_100_base, video_id)
            
            try:
                self.logger.info(f"å¼€å§‹é‡æ–°å¤„ç†è§†é¢‘: {video_id}")
                
                if not os.path.exists(video_path):
                    self.logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                    continue
                
                # è·å–ground truth
                ground_truth_label = self.get_ground_truth_label(video_id)
                
                # åˆ†æè§†é¢‘
                result = self.analyze_with_gpt4o(video_path, video_id)
                
                if result:
                    key_actions = self.extract_key_actions(result)
                    evaluation = self.evaluate_result(video_id, key_actions, ground_truth_label)
                    status = "SUCCESS"
                else:
                    key_actions = ""
                    evaluation = "ERROR"
                    status = "FAILED"
                
                # è®°å½•ç»“æœ
                result_entry = {
                    "video_id": video_id,
                    "ground_truth": ground_truth_label,
                    "key_actions": key_actions,
                    "evaluation": evaluation,
                    "status": status,
                    "raw_result": result
                }
                
                self.results["detailed_results"].append(result_entry)
                
                self.logger.info(f"è§†é¢‘ {video_id}: GT={ground_truth_label}, æ£€æµ‹={key_actions}, è¯„ä¼°={evaluation}, çŠ¶æ€={status}")
                
                # çŸ­æš‚ä¼‘æ¯é¿å…APIé™åˆ¶
                time.sleep(2)
                
            except Exception as e:
                self.logger.error(f"å¤„ç†è§†é¢‘å¤±è´¥ {video_id}: {str(e)}")
                result_entry = {
                    "video_id": video_id,
                    "ground_truth": "unknown",
                    "key_actions": "",
                    "evaluation": "ERROR",
                    "status": "EXCEPTION",
                    "raw_result": str(e)
                }
                self.results["detailed_results"].append(result_entry)
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        result_file = os.path.join(self.output_dir, f"run8_retry_failed_results_{self.timestamp}.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        self.logger.info(f"ç»“æœå·²ä¿å­˜: {result_file}")
        return result_file
    
    def generate_summary(self):
        """ç”Ÿæˆå¤„ç†æ‘˜è¦"""
        successful = sum(1 for r in self.results["detailed_results"] if r["status"] == "SUCCESS")
        failed = sum(1 for r in self.results["detailed_results"] if r["status"] in ["FAILED", "EXCEPTION"])
        
        summary = {
            "total_videos": len(self.failed_videos),
            "successful": successful,
            "failed": failed,
            "success_rate": successful / len(self.failed_videos) if self.failed_videos else 0,
            "results_by_video": {r["video_id"]: r["status"] for r in self.results["detailed_results"]}
        }
        
        return summary

def main():
    # é…ç½®
    output_dir = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/run8_gpt4o_ghost_probing_fewshot_100videos_results/retry_failed"
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = Run8RetryFailedVideos(output_dir)
    
    print(f"ğŸ¯ Run 8: é‡æ–°å¤„ç†å¤±è´¥è§†é¢‘")
    print(f"ğŸ“ å¤±è´¥è§†é¢‘: {', '.join(processor.failed_videos)}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 50)
    
    # å¤„ç†å¤±è´¥è§†é¢‘
    start_time = time.time()
    processor.process_failed_videos()
    end_time = time.time()
    
    # ä¿å­˜ç»“æœ
    result_file = processor.save_results()
    summary = processor.generate_summary()
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nâœ… é‡æ–°å¤„ç†å®Œæˆ!")
    print(f"â±ï¸ æ€»è€—æ—¶: {end_time - start_time:.1f}ç§’")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
    print(f"ğŸ“Š æˆåŠŸç‡: {summary['success_rate']:.1%}")
    print(f"âœ… æˆåŠŸ: {summary['successful']}")
    print(f"âŒ å¤±è´¥: {summary['failed']}")
    
    print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for video_id, status in summary['results_by_video'].items():
        status_icon = "âœ…" if status == "SUCCESS" else "âŒ"
        print(f"  {status_icon} {video_id}: {status}")
    
    if summary['successful'] > 0:
        print(f"\nğŸ‰ æˆåŠŸé‡æ–°å¤„ç†äº†{summary['successful']}ä¸ªè§†é¢‘!")
        print("ğŸ”„ ç°åœ¨å¯ä»¥é‡æ–°è®¡ç®—Run 8çš„å®Œæ•´ç»Ÿè®¡æ•°æ®")

if __name__ == "__main__":
    main()