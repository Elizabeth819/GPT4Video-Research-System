#!/usr/bin/env python3
"""
Ablation Study: 2 Few-shot Samples
åŸºäºRun 8é…ç½®ï¼Œä½¿ç”¨2ä¸ªfew-shotæ ·æœ¬è¿›è¡Œæ¶ˆèå®éªŒ
æµ‹è¯•å¹³è¡¡few-shotå­¦ä¹ çš„æ•ˆæœï¼ˆ1ä¸ªpositive + 1ä¸ªnegativeæ ·æœ¬ï¼‰
"""

import cv2
import os
import json
import logging
import time
import datetime
from moviepy.editor import VideoFileClip
import pandas as pd
from dotenv import load_dotenv
import tqdm
import re
import base64
import requests
import traceback
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥fewshot_examples
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from fewshot_examples import get_fewshot_examples

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class GPT4oAblation2Samples:
    def __init__(self, output_dir, chunk_size=10):
        self.output_dir = output_dir
        self.chunk_size = chunk_size
        os.makedirs(self.output_dir, exist_ok=True)
        self.setup_logging()
        self.setup_openai_api()
        self.load_ground_truth()
        self.initialize_results()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.timestamp = timestamp
        log_filename = os.path.join(self.output_dir, f"ablation_2samples_{timestamp}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("æ¶ˆèå®éªŒ: 2 Few-shot Samples å¼€å§‹")
        
    def setup_openai_api(self):
        """è®¾ç½®OpenAI API"""
        self.openai_api_key = os.environ.get("AZURE_OPENAI_API_KEY", "")
        if not self.openai_api_key:
            raise ValueError("AZURE_OPENAI_API_KEYæœªè®¾ç½®")
        
        # Azure OpenAIé…ç½®
        self.vision_endpoint = os.environ.get("AZURE_OPENAI_API_ENDPOINT", "")
        self.vision_deployment = os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o-global")
        
        if not self.vision_endpoint:
            raise ValueError("AZURE_OPENAI_API_ENDPOINTæœªè®¾ç½®")
            
        self.logger.info(f"Azure OpenAI APIé…ç½®æˆåŠŸ")
        self.logger.info(f"Endpoint: {self.vision_endpoint}")
        self.logger.info(f"Deployment: {self.vision_deployment}")
        self.logger.info(f"Temperature: 0, Few-shot Samples: 2")
        
    def load_ground_truth(self):
        """åŠ è½½ground truthæ ‡ç­¾"""
        gt_path = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos/labels.csv"
        self.ground_truth = pd.read_csv(gt_path, encoding='utf-8-sig')
        self.logger.info(f"åŠ è½½ground truthæ ‡ç­¾: {len(self.ground_truth)}ä¸ªè§†é¢‘")
        self.logger.info(f"æ ‡ç­¾åˆ—: {list(self.ground_truth.columns)}")
        
    def initialize_results(self):
        """åˆå§‹åŒ–ç»“æœç»“æ„"""
        # å°è¯•åŠ è½½ç°æœ‰ç»“æœæ–‡ä»¶ (æŸ¥æ‰¾ä»»ä½•ç°æœ‰çš„ç»“æœæ–‡ä»¶)
        import glob
        pattern = os.path.join(self.output_dir, "ablation_2samples_results_*.json")
        existing_files = glob.glob(pattern)
        
        if existing_files:
            # ä½¿ç”¨æœ€æ–°çš„ç»“æœæ–‡ä»¶
            latest_file = max(existing_files, key=os.path.getmtime)
            try:
                with open(latest_file, 'r', encoding='utf-8') as f:
                    self.results = json.load(f)
                self.logger.info(f"åŠ è½½ç°æœ‰ç»“æœæ–‡ä»¶: {latest_file}")
                self.logger.info(f"å·²å¤„ç†è§†é¢‘æ•°: {len(self.results.get('detailed_results', []))}")
                return
            except Exception as e:
                self.logger.warning(f"åŠ è½½ç°æœ‰ç»“æœå¤±è´¥: {str(e)}")
        
        # åˆ›å»ºæ–°çš„ç»“æœç»“æ„
        self.results = {
            "experiment_info": {
                "run_id": "Ablation Study - 2 Few-shot Samples",
                "timestamp": self.timestamp,
                "video_count": 100,
                "model": "GPT-4o (Azure)",
                "prompt_version": "Paper_Batch Complex (4-Task) + 2 Few-shot Examples",
                "temperature": 0,
                "max_tokens": 3000,
                "purpose": "æ¶ˆèå®éªŒï¼šæµ‹è¯•å¹³è¡¡few-shotå­¦ä¹ çš„æ•ˆæœ",
                "baseline_comparison": "Run 8 (3 few-shot samples, F1=70.0%)",
                "output_directory": self.output_dir,
                "ablation_parameters": {
                    "few_shot_samples": 2,
                    "selected_examples": [
                        "Example 1: Ghost Probing Detection (positiveæ ·æœ¬)",
                        "Example 2: Normal Driving (negativeæ ·æœ¬)"
                    ],
                    "control_variables": [
                        "ç›¸åŒæ¨¡å‹: GPT-4o",
                        "ç›¸åŒTemperature: 0",
                        "ç›¸åŒåŸºç¡€prompt: Paper_Batch Complex",
                        "ç›¸åŒè¯„ä¼°æ•°æ®: DADA-100"
                    ],
                    "test_variable": "Few-shotæ ·æœ¬æ•°é‡: 3 â†’ 2 (å¹³è¡¡çš„positive/negativeæ ·æœ¬)"
                }
            },
            "detailed_results": []
        }
        
    def extract_frames_from_video(self, video_path, frame_interval=10, frames_per_interval=10):
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        try:
            clip = VideoFileClip(video_path)
            duration = clip.duration
            
            frames = []
            frames_dir = os.path.join(self.output_dir, "frames_temp")
            os.makedirs(frames_dir, exist_ok=True)
            
            # è®¡ç®—é—´éš”æ•°
            num_intervals = max(1, int(duration / frame_interval))
            
            for interval_idx in range(num_intervals):
                start_time = interval_idx * frame_interval
                end_time = min((interval_idx + 1) * frame_interval, duration)
                
                for frame_idx in range(frames_per_interval):
                    if frames_per_interval == 1:
                        frame_time = start_time + (end_time - start_time) / 2
                    else:
                        frame_time = start_time + (frame_idx / (frames_per_interval - 1)) * (end_time - start_time)
                    
                    if frame_time >= duration:
                        break
                        
                    frame = clip.get_frame(frame_time)
                    frame_filename = f"frame_{interval_idx}_{frame_idx}_{frame_time:.1f}s.jpg"
                    frame_path = os.path.join(frames_dir, frame_filename)
                    
                    cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
                    frames.append(frame_path)
            
            clip.close()
            return frames
        except Exception as e:
            self.logger.error(f"å¸§æå–å¤±è´¥ {video_path}: {str(e)}")
            return []
    
    def get_paper_batch_prompt_with_2_fewshot(self, video_id, frame_interval=10, frames_per_interval=10):
        """è·å–åŒ…å«2ä¸ªFew-shot Examplesçš„Paper_Batch prompt"""
        
        # è·å–2ä¸ªfew-shotæ ·æœ¬
        fewshot_examples = get_fewshot_examples(num_samples=2)
        
        return f'''You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

Your job is to take in as an input a transcription of {frame_interval} seconds of audio from a video,
as well as {frames_per_interval} frames split evenly throughout {frame_interval} seconds.
You are to generate and provide a Current Action Summary of the video you are considering ({frames_per_interval}
frames over {frame_interval} seconds), which is generated from your analysis of each frame ({frames_per_interval} in total),
as well as the in-between audio, until we have a full action summary of the portion of the video you are considering.

Direction - Please identify the objects in the image based on their position in the image itself. Do not assume your own position within the image. Treat the left side of the image as 'left' and the right side of the image as 'right'. Assume the viewpoint is standing from at the bottom center of the image. Describe whether the objects are on the left or right side of this central point, left is left, and right is right. For example, if there is a car on the left side of the image, state that the car is on the left.

**Task 1: Identify and Predict potential "Ghost Probing(ä¸“ä¸šæœ¯è¯­ï¼šé¬¼æ¢å¤´)" behavior**

"Ghost Probing" includes the following key behaviors:

1) Traditional Ghost Probing: 
   - A person or cyclist suddenly darting out from either left or right side of the car
   - Must emerge from behind a physical obstruction that blocks the driver's view, such as a parked car, a tree, or a wall
   - Directly entering the driver's path with minimal reaction time

2) Vehicle Ghost Probing: 
   - A vehicle suddenly emerging from behind a physical obstruction
   - Examples include: buildings at intersections, parked vehicles, roadside structures, flower beds, a bridge, even a moving car at the front hiding another moving car, etc.
   - Vehicles entering from perpendicular roads that were previously hidden by obstructions

Core Characteristics:
- Presence of a physical obstruction that creates a visual barrier
- Sudden appearance from behind this obstruction with minimal reaction time
- The physical obstruction makes detection impossible until emergence
- Creates an immediate danger or potential collision situation

Note: Only those emerging from behind a physical obstruction can be considered as é¬¼æ¢å¤´ (ghost probing).

**Task 2: Explain Current Driving Actions**
Analyze the current video frames to extract actions. Describe not only the actions themselves but also provide detailed reasoning for why the vehicle is taking these actions, such as changes in speed and direction. Focus solely on the reasoning for the vehicle's actions, excluding any descriptions of pedestrian behavior. Explain why the driver is driving at a certain speed, making turns, or stopping. Your goal is to provide a comprehensive understanding of the vehicle's behavior based on the visual data. Output in the "actions" field of the JSON format template.

**Task 3: Predict Next Driving Action**
Understand the current road conditions, the driving behavior, and to predict the next driving action. Analyze the video and audio to provide a comprehensive summary of the road conditions, including weather, traffic density, road obstacles, and traffic light if visible. Predict the next driving action based on two dimensions, one is driving speed control, such as accelerating, braking, turning, or stopping, the other one is to predict the next lane control, such as change to left lane, change to right lane, keep left in this lane, keep right in this lane, keep straight. Your summary should help understand not only what is happening at the moment but also what is likely to happen next with logical reasoning. The principle is safety first, so the prediction action should prioritize the driver's safety and secondly the pedestrians' safety. Be incredibly detailed. Output in the "next_action" field of the JSON format template.

**Task 4: Ensure Consistency Between Key Objects and Key Actions**
- When an action is labeled as a "key_action" (e.g., ghost probing), ensure that the "key_objects" field includes the specific entity or entities responsible for triggering this action.

Additional Requirements:
- `key_actions` must strictly adhere to the predefined categories:
    - ghost probing
    - overtaking, specify "left-side overtaking" or "right-side overtaking" when relevant.
    - none (if no dangerous behavior is observed)

- All textual fields must be in English.
- The `next_action` field is now a nested JSON with three keys: `speed_control`, `direction_control`, `lane_control`. Each must choose one value from their respective sets.

{fewshot_examples}

Your response should be a valid JSON object with the following EXACT structure:
{{
    "video_id": "{video_id}",
    "segment_id": "full_video",
    "Start_Timestamp": "0.0s",
    "End_Timestamp": "{frame_interval}.0s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene with specific details (age, gender, clothing, transportation)",
    "summary": "comprehensive summary of the scene and what happens with incredible detail",
    "actions": "actions taken by the vehicle and driver responses with detailed reasoning",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "ghost probing/left-side overtaking/right-side overtaking/none",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

Audio Transcription: [No audio available for this analysis]

Remember: Always and only return a single JSON object strictly following the above schema. Be incredibly detailed in your analysis, especially for ghost probing detection. Use the examples above as guidance for the level of detail and accuracy expected.'''
    
    def send_azure_openai_request(self, prompt, images):
        """å‘é€Azure OpenAIè¯·æ±‚ - ä½¿ç”¨Temperature=0"""
        encoded_images = []
        for image_path in images:
            try:
                with open(image_path, 'rb') as image_file:
                    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                    encoded_images.append(encoded_string)
            except Exception as e:
                self.logger.error(f"å›¾åƒç¼–ç å¤±è´¥ {image_path}: {str(e)}")
                continue
        
        if not encoded_images:
            return None
            
        content = [{"type": "text", "text": prompt}]
        
        for encoded_image in encoded_images:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}"
                }
            })
        
        data = {
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ],
            "max_tokens": 3000,
            "temperature": 0  # å…³é”®é…ç½®ï¼šTemperature=0
        }
        
        headers = {
            "Content-Type": "application/json",
            "api-key": self.openai_api_key
        }
        
        url = f"{self.vision_endpoint}openai/deployments/{self.vision_deployment}/chat/completions?api-version=2024-02-15-preview"
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                self.logger.error(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                return None
        except requests.exceptions.Timeout:
            self.logger.error("APIè¯·æ±‚è¶…æ—¶")
            return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"å¤„ç†APIå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None
    
    def parse_json_response(self, response_text):
        """è§£æJSONå“åº”"""
        try:
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
            
            # è§£æJSON
            result = json.loads(cleaned_text)
            return result
        except json.JSONDecodeError as e:
            self.logger.error(f"JSONè§£æå¤±è´¥: {str(e)}")
            self.logger.error(f"åŸå§‹å“åº”: {response_text}")
            return None
    
    def evaluate_result(self, video_id, parsed_result):
        """è¯„ä¼°ç»“æœ"""
        if not parsed_result or 'key_actions' not in parsed_result:
            return 'UNKNOWN'
        
        # è·å–ground truth (video_idå¯èƒ½éœ€è¦åŠ .aviåç¼€)
        video_id_with_ext = video_id + '.avi' if not video_id.endswith('.avi') else video_id
        gt_row = self.ground_truth[self.ground_truth['video_id'] == video_id_with_ext]
        if gt_row.empty:
            self.logger.warning(f"æœªæ‰¾åˆ°ground truth: {video_id_with_ext}")
            return 'UNKNOWN'
        
        gt_label = gt_row.iloc[0]['ground_truth_label']
        predicted_actions = parsed_result['key_actions'].lower()
        
        # åˆ¤æ–­é¢„æµ‹ç»“æœ
        if 'ghost probing' in predicted_actions or 'ghost_probing' in predicted_actions:
            predicted_label = 'ghost_probing'
        else:
            predicted_label = 'none'
        
        # åˆ¤æ–­ground truthï¼ˆå¤„ç†æ—¶é—´ä¿¡æ¯ï¼‰
        gt_has_ghost_probing = 'ghost probing' in str(gt_label).lower()
        
        # è®¡ç®—è¯„ä¼°ç»“æœ
        if gt_has_ghost_probing and predicted_label == 'ghost_probing':
            return 'TP'
        elif not gt_has_ghost_probing and predicted_label == 'none':
            return 'TN'
        elif not gt_has_ghost_probing and predicted_label == 'ghost_probing':
            return 'FP'
        elif gt_has_ghost_probing and predicted_label == 'none':
            return 'FN'
        else:
            return 'UNKNOWN'
    
    def process_video(self, video_path):
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        video_id = os.path.basename(video_path).replace('.avi', '')
        self.logger.info(f"å¤„ç†è§†é¢‘: {video_id}")
        
        try:
            # æå–å¸§
            frames = self.extract_frames_from_video(video_path)
            if not frames:
                self.logger.error(f"æ— æ³•æå–å¸§: {video_id}")
                return None
            
            # ç”Ÿæˆprompt
            prompt = self.get_paper_batch_prompt_with_2_fewshot(video_id)
            
            # å‘é€APIè¯·æ±‚
            response = self.send_azure_openai_request(prompt, frames)
            if not response:
                self.logger.error(f"APIè¯·æ±‚å¤±è´¥: {video_id}")
                return None
            
            # è§£æç»“æœ
            parsed_result = self.parse_json_response(response)
            if not parsed_result:
                self.logger.error(f"ç»“æœè§£æå¤±è´¥: {video_id}")
                return None
            
            # è¯„ä¼°ç»“æœ
            evaluation = self.evaluate_result(video_id, parsed_result)
            
            # æ¸…ç†ä¸´æ—¶å¸§æ–‡ä»¶
            frames_dir = os.path.join(self.output_dir, "frames_temp")
            if os.path.exists(frames_dir):
                import shutil
                shutil.rmtree(frames_dir)
            
            result = {
                "video_id": video_id,
                "ground_truth": self.ground_truth[self.ground_truth['video_id'] == video_id].iloc[0]['ground_truth_label'] if not self.ground_truth[self.ground_truth['video_id'] == video_id].empty else 'unknown',
                "key_actions": parsed_result.get('key_actions', 'unknown'),
                "evaluation": evaluation,
                "raw_result": json.dumps(parsed_result, ensure_ascii=False)
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"è§†é¢‘å¤„ç†å¤±è´¥ {video_id}: {str(e)}")
            self.logger.error(traceback.format_exc())
            return None
    
    def run_experiment(self, limit=100):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        self.logger.info(f"å¼€å§‹æ¶ˆèå®éªŒ: 2 Few-shot Samplesï¼Œå¤„ç† {limit} ä¸ªè§†é¢‘")
        
        # è·å–å·²å¤„ç†çš„è§†é¢‘ID
        processed_video_ids = set()
        if self.results['detailed_results']:
            processed_video_ids = {r['video_id'] for r in self.results['detailed_results']}
            self.logger.info(f"æ£€æµ‹åˆ°å·²å¤„ç† {len(processed_video_ids)} ä¸ªè§†é¢‘ï¼Œä»æ£€æŸ¥ç‚¹ç»§ç»­")
        
        # è·å–è§†é¢‘åˆ—è¡¨
        video_dir = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos"
        all_video_files = [f for f in os.listdir(video_dir) if f.endswith('.avi')]
        
        # è¿‡æ»¤æ‰å·²å¤„ç†çš„è§†é¢‘
        remaining_videos = []
        for video_file in all_video_files:
            video_id = video_file.replace('.avi', '')
            if video_id not in processed_video_ids:
                remaining_videos.append(video_file)
        
        # é™åˆ¶å¤„ç†æ•°é‡
        videos_to_process = remaining_videos[:limit - len(processed_video_ids)]
        self.logger.info(f"éœ€è¦å¤„ç† {len(videos_to_process)} ä¸ªæ–°è§†é¢‘ (å·²å®Œæˆ: {len(processed_video_ids)}/100)")
        
        # å¤„ç†è§†é¢‘
        for video_file in tqdm.tqdm(videos_to_process, desc="å¤„ç†è§†é¢‘"):
            video_path = os.path.join(video_dir, video_file)
            result = self.process_video(video_path)
            
            if result:
                self.results['detailed_results'].append(result)
                self.logger.info(f"âœ… {result['video_id']}: {result['evaluation']}")
            else:
                self.logger.error(f"âŒ {video_file}: å¤„ç†å¤±è´¥")
            
            # æ¯10ä¸ªè§†é¢‘ä¿å­˜ä¸€æ¬¡ç»“æœ
            if len(self.results['detailed_results']) % 10 == 0:
                self.save_results()
        
        # è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
        metrics = self.calculate_metrics()
        self.save_results()
        self.generate_report(metrics)
        
        return metrics
    
    def calculate_metrics(self):
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        results = self.results['detailed_results']
        if not results:
            return None
        
        # ç»Ÿè®¡æ··æ·†çŸ©é˜µ
        tp = sum(1 for r in results if r['evaluation'] == 'TP')
        tn = sum(1 for r in results if r['evaluation'] == 'TN')
        fp = sum(1 for r in results if r['evaluation'] == 'FP')
        fn = sum(1 for r in results if r['evaluation'] == 'FN')
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (tp + tn) / (tp + tn + fp + fn)
        balanced_accuracy = (recall + specificity) / 2
        
        metrics = {
            "confusion_matrix": {"TP": tp, "TN": tn, "FP": fp, "FN": fn},
            "f1_score": f1_score,
            "precision": precision,
            "recall": recall,
            "specificity": specificity,
            "accuracy": accuracy,
            "balanced_accuracy": balanced_accuracy,
            "processed_videos": len(results)
        }
        
        return metrics
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        results_file = os.path.join(self.output_dir, f"ablation_2samples_results_{self.timestamp}.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ç»“æœå·²ä¿å­˜: {results_file}")
    
    def generate_report(self, metrics):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        if not metrics:
            return
        
        report_file = os.path.join(self.output_dir, f"ablation_2samples_report_{self.timestamp}.md")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"""# æ¶ˆèå®éªŒæŠ¥å‘Š: 2 Few-shot Samples

## å®éªŒä¿¡æ¯
- **å®éªŒæ—¶é—´**: {self.timestamp}
- **å®éªŒç›®çš„**: æµ‹è¯•å¹³è¡¡few-shotå­¦ä¹ çš„æ•ˆæœ (1 positive + 1 negativeæ ·æœ¬)
- **åŸºçº¿å¯¹æ¯”**: Run 8 (3 few-shot samples, F1=70.0%)
- **å¤„ç†è§†é¢‘æ•°**: {metrics['processed_videos']}ä¸ª

## å®éªŒé…ç½®
- **æ¨¡å‹**: GPT-4o (Azure)
- **Temperature**: 0
- **Prompt**: Paper_Batch Complex (4-Task)
- **Few-shotæ ·æœ¬æ•°**: 2ä¸ª
  - Example 1: Ghost Probing Detection (positiveæ ·æœ¬)
  - Example 2: Normal Driving (negativeæ ·æœ¬)

## æ€§èƒ½ç»“æœ

### æ··æ·†çŸ©é˜µ
- True Positives (TP): {metrics['confusion_matrix']['TP']}
- True Negatives (TN): {metrics['confusion_matrix']['TN']}
- False Positives (FP): {metrics['confusion_matrix']['FP']}
- False Negatives (FN): {metrics['confusion_matrix']['FN']}

### æ€§èƒ½æŒ‡æ ‡
- **F1 Score**: {metrics['f1_score']:.3f} ({metrics['f1_score']*100:.1f}%)
- **Precision**: {metrics['precision']:.3f} ({metrics['precision']*100:.1f}%)
- **Recall**: {metrics['recall']:.3f} ({metrics['recall']*100:.1f}%)
- **Specificity**: {metrics['specificity']:.3f} ({metrics['specificity']*100:.1f}%)
- **Accuracy**: {metrics['accuracy']:.3f} ({metrics['accuracy']*100:.1f}%)
- **Balanced Accuracy**: {metrics['balanced_accuracy']:.3f} ({metrics['balanced_accuracy']*100:.1f}%)

## ä¸åŸºçº¿å¯¹æ¯” (Run 8: 3 samples)
- **F1å·®å¼‚**: {metrics['f1_score']*100:.1f}% vs 70.0% = {(metrics['f1_score']*100 - 70.0):+.1f}%
- **Recallå·®å¼‚**: {metrics['recall']*100:.1f}% vs 84.8% = {(metrics['recall']*100 - 84.8):+.1f}%
- **Precisionå·®å¼‚**: {metrics['precision']*100:.1f}% vs 59.6% = {(metrics['precision']*100 - 59.6):+.1f}%

## å®éªŒç»“è®º
1. **å¹³è¡¡å­¦ä¹ æ•ˆæœ**: 2ä¸ªæ ·æœ¬(positive+negative)ç›¸æ¯”3ä¸ªæ ·æœ¬çš„æ€§èƒ½å˜åŒ–
2. **æ ·æœ¬è´¨é‡vsæ•°é‡**: éªŒè¯äº†å¹³è¡¡æ ·æœ¬ç»„åˆçš„é‡è¦æ€§
3. **å­¦ä¹ æ•ˆç‡**: åˆ†æäº†æœ€å°æœ‰æ•ˆfew-shotå­¦ä¹ çš„é˜ˆå€¼

## æ–‡ä»¶è·¯å¾„
- è¯¦ç»†ç»“æœ: `ablation_2samples_results_{self.timestamp}.json`
- å®éªŒæ—¥å¿—: `ablation_2samples_{self.timestamp}.log`
""")
        
        self.logger.info(f"å®éªŒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒ: 2 Few-shot Samples')
    parser.add_argument('--limit', type=int, default=100, help='å¤„ç†è§†é¢‘æ•°é‡é™åˆ¶')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/ablation/few-shot-samples/2-samples"
    
    # è¿è¡Œå®éªŒ
    experiment = GPT4oAblation2Samples(output_dir)
    metrics = experiment.run_experiment(limit=args.limit)
    
    if metrics:
        print(f"\nğŸ‰ æ¶ˆèå®éªŒå®Œæˆï¼")
        print(f"ğŸ“Š F1 Score: {metrics['f1_score']*100:.1f}%")
        print(f"ğŸ“ˆ å¤„ç†è§†é¢‘æ•°: {metrics['processed_videos']}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")

if __name__ == "__main__":
    main()