#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›åçš„few-shotæ ·æœ¬
å¯¹æ¯”åŸç‰ˆvsæ”¹è¿›ç‰ˆçš„æ•ˆæœ
"""

import os
import sys
import json
import subprocess
import time
import logging
import datetime

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/ablation/few-shot-samples/test_improved_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def run_baseline_test():
    """è¿è¡ŒRun 6åŸºçº¿æµ‹è¯• (Paper Batch æ— Few-shot)"""
    logger = logging.getLogger(__name__)
    logger.info("ğŸ” å‡†å¤‡è¿è¡ŒåŸºçº¿æµ‹è¯• (æ— Few-shot)")
    
    # åˆ›å»ºä¸´æ—¶çš„æ— few-shotè„šæœ¬
    baseline_script = """
import cv2
import os
import json
import logging
import time
import datetime
from moviepy.editor import VideoFileClip
import pandas as pd
from dotenv import load_dotenv
import tqdm
import base64
import requests
import traceback

load_dotenv()

class GPT4oBaseline:
    def __init__(self, output_dir):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.setup_logging()
        self.setup_openai_api()
        self.load_ground_truth()
        self.initialize_results()
        
    def setup_logging(self):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.timestamp = timestamp
        log_filename = os.path.join(self.output_dir, f"baseline_test_{timestamp}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_openai_api(self):
        self.openai_api_key = os.environ.get("AZURE_OPENAI_API_KEY", "")
        if not self.openai_api_key:
            raise ValueError("AZURE_OPENAI_API_KEYæœªè®¾ç½®")
        
        self.vision_endpoint = os.environ.get("AZURE_OPENAI_API_ENDPOINT", "")
        self.vision_deployment = os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o-global")
        
        if not self.vision_endpoint:
            raise ValueError("AZURE_OPENAI_API_ENDPOINTæœªè®¾ç½®")
            
    def load_ground_truth(self):
        gt_path = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos/labels.csv"
        self.ground_truth = pd.read_csv(gt_path, encoding='utf-8-sig')
        
    def initialize_results(self):
        self.results = {
            "experiment_info": {
                "run_id": "Baseline Test - No Few-shot",
                "timestamp": self.timestamp,
                "purpose": "åŸºçº¿æµ‹è¯•ï¼šPaper Batch promptæ— Few-shot"
            },
            "detailed_results": []
        }
        
    def get_paper_batch_prompt_no_fewshot(self, video_id, frame_interval=10, frames_per_interval=10):
        return f'''You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

Your job is to take in as an input a transcription of {frame_interval} seconds of audio from a video,
as well as {frames_per_interval} frames split evenly throughout {frame_interval} seconds.
You are to generate and provide a Current Action Summary of the video you are considering ({frames_per_interval}
frames over {frame_interval} seconds), which is generated from your analysis of each frame ({frames_per_interval} in total),
as well as the in-between audio, until we have a full action summary of the portion of the video you are considering.

Direction - Please identify the objects in the image based on their position in the image itself. Do not assume your own position within the image. Treat the left side of the image as 'left' and the right side of the image as 'right'. Assume the viewpoint is standing from at the bottom center of the image. Describe whether the objects are on the left or right side of this central point, left is left, and right is right. For example, if there is a car on the left side of the image, state that the car is on the left.

**Task 1: Identify and Predict potential "Ghost Probing(ä¸“ä¸šæœ¯è¯­ï¼šé¬¼æ¢å¤´)" behavior**

"Ghost Probing" includes the following key behaviors:

1) Traditional Ghost Probing: 
   - A person or cyclist suddenly darting out from either left or right side of the car
   - Must emerge from behind a physical obstruction that blocks the driver's view, such as a parked car, a tree, or a wall
   - Directly entering the driver's path with minimal reaction time

2) Vehicle Ghost Probing: 
   - A vehicle suddenly emerging from behind a physical obstruction
   - Examples include: buildings at intersections, parked vehicles, roadside structures, flower beds, a bridge, even a moving car at the front hiding another moving car, etc.
   - Vehicles entering from perpendicular roads that were previously hidden by obstructions

Core Characteristics:
- Presence of a physical obstruction that creates a visual barrier
- Sudden appearance from behind this obstruction with minimal reaction time
- The physical obstruction makes detection impossible until emergence
- Creates an immediate danger or potential collision situation

Note: Only those emerging from behind a physical obstruction can be considered as é¬¼æ¢å¤´ (ghost probing).

**Task 2: Explain Current Driving Actions**
**Task 3: Predict Next Driving Action**
**Task 4: Ensure Consistency Between Key Objects and Key Actions**

Additional Requirements:
- `key_actions` must strictly adhere to the predefined categories:
    - ghost probing
    - overtaking, specify "left-side overtaking" or "right-side overtaking" when relevant.
    - none (if no dangerous behavior is observed)

Your response should be a valid JSON object with the following EXACT structure:
{{
    "video_id": "{video_id}",
    "segment_id": "full_video",
    "Start_Timestamp": "0.0s",
    "End_Timestamp": "{frame_interval}.0s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene with specific details (age, gender, clothing, transportation)",
    "summary": "comprehensive summary of the scene and what happens with incredible detail",
    "actions": "actions taken by the vehicle and driver responses with detailed reasoning",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "ghost probing/left-side overtaking/right-side overtaking/none",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

Audio Transcription: [No audio available for this analysis]

Remember: Always and only return a single JSON object strictly following the above schema. Be incredibly detailed in your analysis, especially for ghost probing detection.'''

# åˆ›å»ºå¿«é€Ÿæµ‹è¯•å‡½æ•°æ¥è¿è¡Œ5ä¸ªè§†é¢‘
def quick_test():
    tester = GPT4oBaseline("/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/ablation/few-shot-samples/baseline_test")
    
    test_videos = [
        "images_1_002.avi",  # çœŸå®TPæ¡ˆä¾‹
        "images_1_020.avi",  # çœŸå®TNæ¡ˆä¾‹  
        "images_1_003.avi",  # çœŸå®TPæ¡ˆä¾‹
        "images_2_001.avi",  # çœŸå®TNæ¡ˆä¾‹
        "images_1_005.avi"   # çœŸå®TPæ¡ˆä¾‹
    ]
    
    print("ğŸ§ª å¼€å§‹åŸºçº¿æµ‹è¯• (5ä¸ªå…³é”®è§†é¢‘)")
    # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…ä¸­éœ€è¦å®Œæ•´çš„è§†é¢‘å¤„ç†é€»è¾‘
    return {"baseline": "completed"}

if __name__ == "__main__":
    quick_test()
"""
    
    # ä¿å­˜ä¸´æ—¶è„šæœ¬
    temp_script_path = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/ablation/few-shot-samples/temp_baseline_test.py"
    with open(temp_script_path, 'w') as f:
        f.write(baseline_script)
    
    logger.info("âœ… åŸºçº¿æµ‹è¯•è„šæœ¬å·²åˆ›å»º")
    return temp_script_path

def run_improved_fewshot_test():
    """è¿è¡Œæ”¹è¿›åçš„few-shotæµ‹è¯•"""
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ”¹è¿›ç‰ˆfew-shotæµ‹è¯•")
    
    # è¿è¡Œ2-samplesæµ‹è¯• (å¹³è¡¡çš„positive+negativeæ ·æœ¬)
    script_path = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/ablation/few-shot-samples/2-samples/run8_ablation_2samples.py"
    
    logger.info("æ‰§è¡Œå‘½ä»¤: python run8_ablation_2samples.py --limit 5")
    
    try:
        cmd = [sys.executable, script_path, "--limit", "5"]
        start_time = time.time()
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
        duration = time.time() - start_time
        
        if result.returncode == 0:
            logger.info(f"âœ… æ”¹è¿›ç‰ˆfew-shotæµ‹è¯•å®Œæˆ (è€—æ—¶: {duration/60:.1f}åˆ†é’Ÿ)")
            return True
        else:
            logger.error(f"âŒ æ”¹è¿›ç‰ˆfew-shotæµ‹è¯•å¤±è´¥")
            logger.error(f"é”™è¯¯è¾“å‡º: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        logger.error(f"â° æ”¹è¿›ç‰ˆfew-shotæµ‹è¯•è¶…æ—¶")
        return False
    except Exception as e:
        logger.error(f"ğŸ’¥ æ”¹è¿›ç‰ˆfew-shotæµ‹è¯•å¼‚å¸¸: {str(e)}")
        return False

def compare_results():
    """å¯¹æ¯”ç»“æœ"""
    logger = logging.getLogger(__name__)
    logger.info("ğŸ“Š å¼€å§‹å¯¹æ¯”åˆ†æ")
    
    # è¿™é‡Œéœ€è¦å®é™…çš„ç»“æœå¯¹æ¯”é€»è¾‘
    logger.info("å¯¹æ¯”åˆ†æå®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    logger = setup_logging()
    logger.info("ğŸ¯ å¼€å§‹æµ‹è¯•æ”¹è¿›åçš„few-shotæ ·æœ¬")
    
    print("ğŸ”¬ Few-shotæ ·æœ¬æ”¹è¿›æµ‹è¯•")
    print("=" * 50)
    print("ç›®æ ‡: éªŒè¯åŸºäºçœŸå®Run 8æ¡ˆä¾‹çš„few-shotæ ·æœ¬æ˜¯å¦èƒ½æå‡æ€§èƒ½")
    print("åŸºçº¿: Paper Batchæ— Few-shot (æœŸæœ›F1 ~63.6%)")
    print("ç›®æ ‡: Paper Batch + æ”¹è¿›Few-shot (æœŸæœ›F1 > 70.0%)")
    print("=" * 50)
    
    # 1. è¿è¡ŒåŸºçº¿æµ‹è¯•
    print("\nğŸ“ ç¬¬1æ­¥: å‡†å¤‡åŸºçº¿æµ‹è¯•")
    baseline_script = run_baseline_test()
    
    # 2. è¿è¡Œæ”¹è¿›ç‰ˆfew-shotæµ‹è¯•  
    print("\nğŸ“ ç¬¬2æ­¥: è¿è¡Œæ”¹è¿›ç‰ˆfew-shotæµ‹è¯•")
    success = run_improved_fewshot_test()
    
    if success:
        print("\nğŸ“ ç¬¬3æ­¥: å¯¹æ¯”åˆ†æ")
        compare_results()
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
        print("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†æ—¥å¿—äº†è§£ç»“æœå¯¹æ¯”")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

if __name__ == "__main__":
    main()