#!/usr/bin/env python3
"""
Few-shotæ ·æœ¬æ•°é‡æ¶ˆèå®éªŒæ€»æ§è„šæœ¬
æŒ‰é¡ºåºè¿è¡Œ1ã€2ã€5ä¸ªæ ·æœ¬çš„æ¶ˆèå®éªŒï¼Œå¹¶ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
"""

import os
import sys
import subprocess
import json
import time
from datetime import datetime

def run_experiment(experiment_name, script_path, limit=100):
    """è¿è¡Œå•ä¸ªæ¶ˆèå®éªŒ"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹è¿è¡Œ: {experiment_name}")
    print(f"ğŸ“ è„šæœ¬è·¯å¾„: {script_path}")
    print(f"ğŸ¯ è§†é¢‘æ•°é‡: {limit}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    try:
        # è¿è¡Œå®éªŒè„šæœ¬
        result = subprocess.run([
            sys.executable, script_path, 
            "--limit", str(limit)
        ], capture_output=True, text=True, cwd=os.path.dirname(script_path))
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            print(f"âœ… {experiment_name} å®ŒæˆæˆåŠŸï¼")
            print(f"â±ï¸  ç”¨æ—¶: {duration/60:.1f} åˆ†é’Ÿ")
            print(f"ğŸ“Š è¾“å‡º: {result.stdout.strip()}")
            return True, duration, result.stdout
        else:
            print(f"âŒ {experiment_name} å¤±è´¥ï¼")
            print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
            return False, duration, result.stderr
            
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        print(f"âŒ {experiment_name} è¿è¡Œå¼‚å¸¸: {str(e)}")
        return False, duration, str(e)

def load_experiment_results(experiment_dir):
    """åŠ è½½å®éªŒç»“æœ"""
    results_files = [f for f in os.listdir(experiment_dir) if f.startswith('ablation_') and f.endswith('_results_') and f.endswith('.json')]
    
    if not results_files:
        return None
    
    # å–æœ€æ–°çš„ç»“æœæ–‡ä»¶
    latest_file = sorted(results_files)[-1]
    results_path = os.path.join(experiment_dir, latest_file)
    
    try:
        with open(results_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"åŠ è½½ç»“æœå¤±è´¥ {results_path}: {str(e)}")
        return None

def calculate_metrics_from_results(results_data):
    """ä»ç»“æœæ•°æ®è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
    if not results_data or 'detailed_results' not in results_data:
        return None
    
    detailed_results = results_data['detailed_results']
    
    # ç»Ÿè®¡æ··æ·†çŸ©é˜µ
    tp = sum(1 for r in detailed_results if r['evaluation'] == 'TP')
    tn = sum(1 for r in detailed_results if r['evaluation'] == 'TN')
    fp = sum(1 for r in detailed_results if r['evaluation'] == 'FP')
    fn = sum(1 for r in detailed_results if r['evaluation'] == 'FN')
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    balanced_accuracy = (recall + specificity) / 2
    
    return {
        "samples": results_data['experiment_info']['ablation_parameters']['few_shot_samples'],
        "processed_videos": len(detailed_results),
        "confusion_matrix": {"TP": tp, "TN": tn, "FP": fp, "FN": fn},
        "f1_score": f1_score,
        "precision": precision,
        "recall": recall,
        "specificity": specificity,
        "accuracy": accuracy,
        "balanced_accuracy": balanced_accuracy
    }

def generate_comprehensive_report(all_metrics, experiment_log):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f"/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/ablation/few-shot-samples/comprehensive_analysis_{timestamp}.md"
    
    # åŸºçº¿æ•°æ® (Run 8: 3 samples)
    baseline_f1 = 70.0
    baseline_recall = 84.8
    baseline_precision = 59.6
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"""# Few-shotæ ·æœ¬æ•°é‡æ¶ˆèå®éªŒç»¼åˆåˆ†ææŠ¥å‘Š

## å®éªŒæ€»è§ˆ
- **å®éªŒæ—¶é—´**: {timestamp}
- **å®éªŒç›®çš„**: ç³»ç»Ÿæ€§è¯„ä¼°few-shotæ ·æœ¬æ•°é‡å¯¹GPT-4o Ghost Probingæ£€æµ‹æ€§èƒ½çš„å½±å“
- **åŸºçº¿å¯¹æ¯”**: Run 8 (3 few-shot samples, F1=70.0%, Recall=84.8%, Precision=59.6%)
- **æµ‹è¯•é…ç½®**: ç›¸åŒæ¨¡å‹(GPT-4o) + ç›¸åŒTemperature(0) + ç›¸åŒåŸºç¡€prompt(Paper_Batch Complex)

## å®éªŒé…ç½®å¯¹æ¯”

| å®éªŒ | Few-shotæ ·æœ¬æ•° | æ ·æœ¬ç»„æˆ | ç›®çš„ |
|------|---------------|----------|------|
| 1æ ·æœ¬å®éªŒ | 1 | Ghost Probing Detection | æµ‹è¯•æœ€å°few-shotå­¦ä¹ æ•ˆæœ |
| 2æ ·æœ¬å®éªŒ | 2 | Ghost Probing + Normal Driving | æµ‹è¯•å¹³è¡¡å­¦ä¹ æ•ˆæœ |
| **åŸºçº¿(Run 8)** | **3** | **Ghost + Normal + Vehicle** | **å½“å‰æœ€ä½³é…ç½®** |
| 5æ ·æœ¬å®éªŒ | 5 | åŸºç¡€3ä¸ª + Cyclist + Highway | æµ‹è¯•è¾¹é™…æ•ˆåº” |

## æ€§èƒ½ç»“æœå¯¹æ¯”

### æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€»

| æ ·æœ¬æ•° | F1åˆ†æ•° | å¬å›ç‡ | ç²¾ç¡®åº¦ | ç‰¹å¼‚æ€§ | å¹³è¡¡å‡†ç¡®ç‡ | å¤„ç†è§†é¢‘æ•° |
|--------|--------|--------|--------|--------|----------|-----------|""")
        
        # æ·»åŠ åŸºçº¿æ•°æ®
        f.write(f"\n| 3 (åŸºçº¿) | {baseline_f1:.1f}% | {baseline_recall:.1f}% | {baseline_precision:.1f}% | 28.3% | 56.6% | 119 |")
        
        # æ·»åŠ å®éªŒæ•°æ®
        for metrics in sorted(all_metrics, key=lambda x: x['samples']):
            f.write(f"\n| {metrics['samples']} | {metrics['f1_score']*100:.1f}% | {metrics['recall']*100:.1f}% | {metrics['precision']*100:.1f}% | {metrics['specificity']*100:.1f}% | {metrics['balanced_accuracy']*100:.1f}% | {metrics['processed_videos']} |")
        
        f.write(f"""

### æ€§èƒ½å˜åŒ–è¶‹åŠ¿åˆ†æ

#### ğŸ“Š F1åˆ†æ•°å˜åŒ–è¶‹åŠ¿
""")
        
        for metrics in sorted(all_metrics, key=lambda x: x['samples']):
            f1_diff = metrics['f1_score']*100 - baseline_f1
            f.write(f"- **{metrics['samples']}æ ·æœ¬**: {metrics['f1_score']*100:.1f}% ({f1_diff:+.1f}% vs åŸºçº¿)\n")
        
        f.write(f"""
#### ğŸ¯ å¬å›ç‡å˜åŒ–è¶‹åŠ¿ (å®‰å…¨ç³»ç»Ÿå…³é”®æŒ‡æ ‡)
""")
        
        for metrics in sorted(all_metrics, key=lambda x: x['samples']):
            recall_diff = metrics['recall']*100 - baseline_recall
            f.write(f"- **{metrics['samples']}æ ·æœ¬**: {metrics['recall']*100:.1f}% ({recall_diff:+.1f}% vs åŸºçº¿)\n")
        
        f.write(f"""
#### ğŸ” ç²¾ç¡®åº¦å˜åŒ–è¶‹åŠ¿
""")
        
        for metrics in sorted(all_metrics, key=lambda x: x['samples']):
            precision_diff = metrics['precision']*100 - baseline_precision
            f.write(f"- **{metrics['samples']}æ ·æœ¬**: {metrics['precision']*100:.1f}% ({precision_diff:+.1f}% vs åŸºçº¿)\n")
        
        f.write(f"""

## å…³é”®å‘ç°

### ğŸ”¬ Few-shotå­¦ä¹ æ•ˆæœåˆ†æ
""")
        
        # åˆ†æå‘ç°
        if len(all_metrics) >= 2:
            metrics_1 = next((m for m in all_metrics if m['samples'] == 1), None)
            metrics_2 = next((m for m in all_metrics if m['samples'] == 2), None)
            metrics_5 = next((m for m in all_metrics if m['samples'] == 5), None)
            
            if metrics_1:
                f.write(f"""
1. **æœ€å°å­¦ä¹ èƒ½åŠ›éªŒè¯** (1æ ·æœ¬ vs 3æ ·æœ¬åŸºçº¿):
   - F1åˆ†æ•°: {metrics_1['f1_score']*100:.1f}% vs 70.0% = {(metrics_1['f1_score']*100 - 70.0):+.1f}%
   - å•ä¸ªé«˜è´¨é‡æ ·æœ¬{'èƒ½å¤Ÿ' if metrics_1['f1_score']*100 > 50 else 'ä¸è¶³ä»¥'}æä¾›åŸºç¡€çš„ghost probingæ£€æµ‹èƒ½åŠ›
   - å¬å›ç‡: {metrics_1['recall']*100:.1f}% (å®‰å…¨ç³»ç»Ÿå¯æ¥å—é˜ˆå€¼åˆ†æ)
""")
            
            if metrics_2:
                f.write(f"""
2. **å¹³è¡¡å­¦ä¹ æ•ˆæœ** (2æ ·æœ¬ vs 3æ ·æœ¬åŸºçº¿):
   - F1åˆ†æ•°: {metrics_2['f1_score']*100:.1f}% vs 70.0% = {(metrics_2['f1_score']*100 - 70.0):+.1f}%
   - Positive+Negativeæ ·æœ¬ç»„åˆçš„å¹³è¡¡å­¦ä¹ æ•ˆæœ
   - ç›¸æ¯”1æ ·æœ¬çš„æ”¹è¿›: {(metrics_2['f1_score'] - metrics_1['f1_score'])*100:+.1f}%
""")
            
            if metrics_5:
                f.write(f"""
3. **è¾¹é™…æ•ˆåº”åˆ†æ** (5æ ·æœ¬ vs 3æ ·æœ¬åŸºçº¿):
   - F1åˆ†æ•°: {metrics_5['f1_score']*100:.1f}% vs 70.0% = {(metrics_5['f1_score']*100 - 70.0):+.1f}%
   - è¾¹é™…æ”¶ç›Š: {(metrics_5['f1_score'] - 0.70)*100:+.1f}% (æ˜¯å¦å€¼å¾—é¢å¤–è®¡ç®—æˆæœ¬)
   - æ ·æœ¬å¤šæ ·æ€§å¯¹æ€§èƒ½çš„å½±å“åˆ†æ
""")
        
        f.write(f"""
### ğŸ¯ æœ€ä¼˜æ ·æœ¬æ•°é‡æ¨è

åŸºäºå®éªŒç»“æœåˆ†æï¼š
""")
        
        # æ‰¾å‡ºæœ€ä½³é…ç½®
        best_metrics = max(all_metrics, key=lambda x: x['f1_score'])
        f.write(f"""
- **æœ€ä½³F1æ€§èƒ½**: {best_metrics['samples']}ä¸ªæ ·æœ¬ (F1={best_metrics['f1_score']*100:.1f}%)
- **è®¡ç®—æ•ˆç‡æƒè¡¡**: è€ƒè™‘æ€§èƒ½æå‡å¹…åº¦å’Œè®¡ç®—æˆæœ¬
- **å®‰å…¨ç³»ç»Ÿè¦æ±‚**: ä¼˜å…ˆè€ƒè™‘å¬å›ç‡ â‰¥ 80% çš„é…ç½®

### ğŸ“ˆ å­¦æœ¯ä»·å€¼

1. **Few-shotå­¦ä¹ æ›²çº¿**: æ­ç¤ºäº†æ ·æœ¬æ•°é‡ä¸æ€§èƒ½çš„å…³ç³»
2. **è¾¹é™…æ•ˆåº”é‡åŒ–**: ä¸ºfew-shotæ ·æœ¬æ•°é‡é€‰æ‹©æä¾›æ•°æ®æ”¯æŒ
3. **å®‰å…¨ç³»ç»Ÿä¼˜åŒ–**: ä¸ºè‡ªåŠ¨é©¾é©¶å®‰å…¨æ£€æµ‹ç³»ç»Ÿçš„few-shoté…ç½®æä¾›æŒ‡å¯¼

## å®éªŒè¿è¡Œæ—¥å¿—

""")
        
        # æ·»åŠ å®éªŒæ—¥å¿—
        for log_entry in experiment_log:
            f.write(f"- **{log_entry['experiment']}**: {log_entry['status']} (ç”¨æ—¶: {log_entry['duration']:.1f}åˆ†é’Ÿ)\n")
        
        f.write(f"""
## ç»“è®ºä¸å»ºè®®

1. **æœ€ä¼˜é…ç½®ç¡®è®¤**: åŸºäºå®éªŒç»“æœï¼Œ{'éªŒè¯äº†å½“å‰3æ ·æœ¬é…ç½®çš„æœ€ä¼˜æ€§' if baseline_f1 >= max(m['f1_score']*100 for m in all_metrics) else 'å‘ç°äº†æ›´ä¼˜çš„æ ·æœ¬æ•°é‡é…ç½®'}
2. **å®ç”¨ä»·å€¼**: ä¸ºAAAI26è®ºæ–‡çš„few-shotå­¦ä¹ æ¶ˆèå®éªŒæä¾›äº†å®Œæ•´çš„æ•°æ®æ”¯æŒ
3. **å·¥ç¨‹åº”ç”¨**: ä¸ºå®é™…éƒ¨ç½²æ—¶çš„few-shotæ ·æœ¬æ•°é‡é€‰æ‹©æä¾›äº†ç§‘å­¦ä¾æ®

## æ–‡ä»¶è·¯å¾„
- ç»¼åˆåˆ†ææŠ¥å‘Š: `comprehensive_analysis_{timestamp}.md`
- 1æ ·æœ¬å®éªŒ: `1-sample/`
- 2æ ·æœ¬å®éªŒ: `2-samples/`  
- 5æ ·æœ¬å®éªŒ: `5-samples/`
""")
    
    print(f"ğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    return report_path

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Few-shotæ ·æœ¬æ•°é‡æ¶ˆèå®éªŒæ€»æ§')
    parser.add_argument('--limit', type=int, default=20, help='æ¯ä¸ªå®éªŒçš„è§†é¢‘æ•°é‡é™åˆ¶ (å»ºè®®å…ˆç”¨20æµ‹è¯•)')
    parser.add_argument('--experiments', nargs='+', default=['1', '2', '5'], 
                      help='è¦è¿è¡Œçš„å®éªŒ (1, 2, 5)')
    
    args = parser.parse_args()
    
    print(f"ğŸ¯ Few-shotæ ·æœ¬æ•°é‡æ¶ˆèå®éªŒå¼€å§‹")
    print(f"ğŸ“Š æ¯ä¸ªå®éªŒå¤„ç† {args.limit} ä¸ªè§†é¢‘")
    print(f"ğŸ§ª å®éªŒåˆ—è¡¨: {args.experiments}")
    
    # å®éªŒé…ç½®
    base_dir = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/ablation/few-shot-samples"
    experiments = {
        '1': {
            'name': '1æ ·æœ¬æ¶ˆèå®éªŒ',
            'script': os.path.join(base_dir, '1-sample', 'run8_ablation_1sample.py'),
            'dir': os.path.join(base_dir, '1-sample')
        },
        '2': {
            'name': '2æ ·æœ¬æ¶ˆèå®éªŒ', 
            'script': os.path.join(base_dir, '2-samples', 'run8_ablation_2samples.py'),
            'dir': os.path.join(base_dir, '2-samples')
        },
        '5': {
            'name': '5æ ·æœ¬æ¶ˆèå®éªŒ',
            'script': os.path.join(base_dir, '5-samples', 'run8_ablation_5samples.py'),
            'dir': os.path.join(base_dir, '5-samples')
        }
    }
    
    # è¿è¡Œå®éªŒ
    experiment_log = []
    all_metrics = []
    
    for exp_id in args.experiments:
        if exp_id not in experiments:
            print(f"âš ï¸  æœªçŸ¥å®éªŒID: {exp_id}")
            continue
        
        exp_config = experiments[exp_id]
        
        # è¿è¡Œå®éªŒ
        success, duration, output = run_experiment(
            exp_config['name'], 
            exp_config['script'], 
            args.limit
        )
        
        # è®°å½•å®éªŒæ—¥å¿—
        experiment_log.append({
            'experiment': exp_config['name'],
            'status': 'æˆåŠŸ' if success else 'å¤±è´¥',
            'duration': duration / 60,  # è½¬æ¢ä¸ºåˆ†é’Ÿ
            'output': output
        })
        
        # å¦‚æœæˆåŠŸï¼ŒåŠ è½½ç»“æœ
        if success:
            time.sleep(2)  # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆ
            results_data = load_experiment_results(exp_config['dir'])
            if results_data:
                metrics = calculate_metrics_from_results(results_data)
                if metrics:
                    all_metrics.append(metrics)
                    print(f"ğŸ“Š {exp_config['name']} ç»“æœ: F1={metrics['f1_score']*100:.1f}%")
        
        print(f"\nâ¸ï¸  ç­‰å¾…5ç§’åç»§ç»­ä¸‹ä¸€ä¸ªå®éªŒ...")
        time.sleep(5)
    
    # ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
    if all_metrics:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
        print(f"{'='*60}")
        
        report_path = generate_comprehensive_report(all_metrics, experiment_log)
        
        print(f"\nğŸ‰ æ‰€æœ‰æ¶ˆèå®éªŒå®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸå®Œæˆ {len(all_metrics)}/{len(args.experiments)} ä¸ªå®éªŒ")
        print(f"ğŸ“ ç»¼åˆæŠ¥å‘Š: {report_path}")
        
        # æ˜¾ç¤ºç®€è¦ç»“æœ
        print(f"\nğŸ“ˆ ç»“æœæ‘˜è¦:")
        for metrics in sorted(all_metrics, key=lambda x: x['samples']):
            print(f"  {metrics['samples']}æ ·æœ¬: F1={metrics['f1_score']*100:.1f}%, Recall={metrics['recall']*100:.1f}%")
        
    else:
        print(f"\nâŒ æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœï¼Œæ— æ³•ç”Ÿæˆç»¼åˆæŠ¥å‘Š")

if __name__ == "__main__":
    main()