#!/usr/bin/env python3
"""
Run 18 Performance Evaluation
è¯„ä¼°Gemini-2.0-Flash + Simple Promptåœ¨DADA-100ä¸Šçš„æ€§èƒ½
"""

import os
import json
import pandas as pd
from pathlib import Path
import datetime

class Run18PerformanceEvaluator:
    def __init__(self):
        self.output_dir = Path(__file__).parent
        self.project_root = Path("/Users/wanmeng/repository/GPT4Video-cobra-auto")
        self.groundtruth_file = self.project_root / "result" / "DADA-100-videos" / "groundtruth_labels.csv"
        
    def load_groundtruth(self):
        """åŠ è½½ground truthæ ‡ç­¾"""
        try:
            df = pd.read_csv(self.groundtruth_file, sep='\t')
            groundtruth = {}
            
            for _, row in df.iterrows():
                video_id = row['video_id'].replace('.avi', '')
                label = row['ground_truth_label']
                
                # å¤„ç†æ ‡ç­¾æ ¼å¼
                if pd.isna(label) or label == 'none':
                    groundtruth[video_id] = False
                else:
                    # åŒ…å«ghost probingçš„éƒ½æ ‡è®°ä¸ºTrue
                    groundtruth[video_id] = 'ghost probing' in str(label).lower()
            
            print(f"âœ… Loaded {len(groundtruth)} ground truth labels")
            return groundtruth
            
        except Exception as e:
            print(f"âŒ Error loading ground truth: {e}")
            return {}
    
    def load_run18_results(self):
        """åŠ è½½Run 18çš„åˆ†æç»“æœ"""
        results = {}
        processed_count = 0
        
        # éå†æ‰€æœ‰ç»“æœæ–‡ä»¶
        for result_file in self.output_dir.glob("actionSummary_*.json"):
            try:
                video_id = result_file.stem.replace("actionSummary_", "")
                
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æ£€æŸ¥key_actionså­—æ®µ
                key_actions = data.get('key_actions', '').lower()
                has_ghost_probing = 'ghost probing' in key_actions
                
                results[video_id] = {
                    'predicted': has_ghost_probing,
                    'key_actions': data.get('key_actions', ''),
                    'summary': data.get('summary', ''),
                    'sentiment': data.get('sentiment', ''),
                    'scene_theme': data.get('scene_theme', '')
                }
                processed_count += 1
                
            except Exception as e:
                print(f"âš ï¸ Error processing {result_file}: {e}")
                continue
        
        print(f"âœ… Loaded {processed_count} Run 18 predictions")
        return results
    
    def calculate_metrics(self, groundtruth, predictions):
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        # æ‰¾åˆ°å…±åŒçš„è§†é¢‘ID
        common_videos = set(groundtruth.keys()) & set(predictions.keys())
        
        if not common_videos:
            print("âŒ No common videos found between ground truth and predictions")
            return None
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        tp = fp = tn = fn = 0
        
        detailed_results = []
        
        for video_id in sorted(common_videos):
            gt_label = groundtruth[video_id]
            pred_label = predictions[video_id]['predicted']
            
            if gt_label and pred_label:
                tp += 1
                result_type = "TP"
            elif not gt_label and not pred_label:
                tn += 1
                result_type = "TN"
            elif not gt_label and pred_label:
                fp += 1
                result_type = "FP"
            else:  # gt_label and not pred_label
                fn += 1
                result_type = "FN"
            
            detailed_results.append({
                'video_id': video_id,
                'ground_truth': gt_label,
                'predicted': pred_label,
                'result_type': result_type,
                'key_actions': predictions[video_id]['key_actions'],
                'summary': predictions[video_id]['summary'][:100] + "..."
            })
        
        # è®¡ç®—æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        
        metrics = {
            'total_videos': len(common_videos),
            'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'specificity': specificity,
            'accuracy': accuracy,
            'detailed_results': detailed_results
        }
        
        return metrics
    
    def print_performance_summary(self, metrics):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸš€ Run 18 Performance Analysis Summary")
        print("="*60)
        print(f"ğŸ“‹ Model: Gemini-2.0-Flash-exp + Simple Prompt")
        print(f"ğŸ“Š Dataset: DADA-100 ({metrics['total_videos']} videos)")
        print(f"ğŸ“… Analysis Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("\n" + "-"*40)
        print("ğŸ“ˆ Performance Metrics:")
        print("-"*40)
        print(f"ğŸ¯ F1-Score:    {metrics['f1_score']:.3f}")
        print(f"ğŸ” Precision:   {metrics['precision']:.3f}")
        print(f"ğŸ“¡ Recall:      {metrics['recall']:.3f}")
        print(f"ğŸ›¡ï¸  Specificity: {metrics['specificity']:.3f}")
        print(f"âœ… Accuracy:    {metrics['accuracy']:.3f}")
        print("\n" + "-"*40)
        print("ğŸ§® Confusion Matrix:")
        print("-"*40)
        print(f"True Positives (TP):  {metrics['tp']}")
        print(f"False Positives (FP): {metrics['fp']}")
        print(f"True Negatives (TN):  {metrics['tn']}")
        print(f"False Negatives (FN): {metrics['fn']}")
        
        # é”™è¯¯æ¡ˆä¾‹åˆ†æ
        print("\n" + "-"*40)
        print("âŒ Error Analysis:")
        print("-"*40)
        
        fp_cases = [r for r in metrics['detailed_results'] if r['result_type'] == 'FP']
        fn_cases = [r for r in metrics['detailed_results'] if r['result_type'] == 'FN']
        
        print(f"False Positives ({len(fp_cases)} cases):")
        for case in fp_cases[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"  â€¢ {case['video_id']}: {case['key_actions']} - {case['summary']}")
        
        print(f"\nFalse Negatives ({len(fn_cases)} cases):")
        for case in fn_cases[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"  â€¢ {case['video_id']}: {case['key_actions']} - {case['summary']}")
    
    def save_detailed_results(self, metrics):
        """ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ"""
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_file = self.output_dir / f"run18_detailed_evaluation_{timestamp}.json"
        evaluation_data = {
            'run_info': {
                'run_id': 'Run 18',
                'model': 'gemini-2.0-flash-exp',
                'prompt_type': 'Simple Paper Batch (No Few-shot)',
                'evaluation_timestamp': timestamp
            },
            'performance_metrics': {
                'f1_score': metrics['f1_score'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'specificity': metrics['specificity'],
                'accuracy': metrics['accuracy'],
                'total_videos': metrics['total_videos']
            },
            'confusion_matrix': {
                'tp': metrics['tp'],
                'fp': metrics['fp'],
                'tn': metrics['tn'],
                'fn': metrics['fn']
            },
            'detailed_results': metrics['detailed_results']
        }
        
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Detailed results saved to: {detailed_file}")
        
        # ä¿å­˜CSVæ ¼å¼çš„è¯¦ç»†ç»“æœ
        csv_file = self.output_dir / f"run18_detailed_results_{timestamp}.csv"
        df = pd.DataFrame(metrics['detailed_results'])
        df.to_csv(csv_file, index=False)
        print(f"ğŸ“‹ CSV results saved to: {csv_file}")
        
        return evaluation_data
    
    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("ğŸš€ Starting Run 18 Performance Evaluation...")
        
        # åŠ è½½æ•°æ®
        groundtruth = self.load_groundtruth()
        predictions = self.load_run18_results()
        
        if not groundtruth or not predictions:
            print("âŒ Failed to load data")
            return None
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics(groundtruth, predictions)
        
        if not metrics:
            print("âŒ Failed to calculate metrics")
            return None
        
        # æ˜¾ç¤ºç»“æœ
        self.print_performance_summary(metrics)
        
        # ä¿å­˜ç»“æœ
        evaluation_data = self.save_detailed_results(metrics)
        
        return evaluation_data

def main():
    """ä¸»å‡½æ•°"""
    evaluator = Run18PerformanceEvaluator()
    result = evaluator.run_evaluation()
    
    if result:
        print("\nâœ… Run 18 evaluation completed successfully!")
        print(f"ğŸ¯ F1-Score: {result['performance_metrics']['f1_score']:.3f}")
    else:
        print("\nâŒ Run 18 evaluation failed!")

if __name__ == "__main__":
    main()