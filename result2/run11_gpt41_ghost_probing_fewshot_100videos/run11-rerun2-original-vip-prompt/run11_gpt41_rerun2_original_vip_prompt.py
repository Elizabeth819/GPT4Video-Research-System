#!/usr/bin/env python3
"""
Run 11 Rerun2: GPT-4.1 Ghost Probing Detection with Original VIP Prompt (100 Videos)
ä½¿ç”¨åŸå§‹ /Users/wanmeng/repository/GPT4Video-cobra-auto/VIP/ActionSummary-predict_explain-fsl-sys-En-cutin-time-paper-batch.py 
ä¸­çš„å®Œæ•´è¯¦ç»†promptï¼Œé‡æ–°è¿è¡ŒRun 11å®éªŒ
ç›®çš„ï¼šå¯¹æ¯”ç®€åŒ–promptå’ŒåŸå§‹è¯¦ç»†promptåœ¨GPT-4.1ä¸Šçš„æ€§èƒ½å·®å¼‚
"""

import cv2
import os
import json
import logging
import time
import datetime
from moviepy.editor import VideoFileClip
import pandas as pd
from dotenv import load_dotenv
import tqdm
import re
import base64
import requests
import traceback
import sys

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class GPT41Run11Rerun2OriginalVIP:
    def __init__(self, output_dir, chunk_size=10):
        self.output_dir = output_dir
        self.chunk_size = chunk_size
        os.makedirs(self.output_dir, exist_ok=True)
        self.setup_logging()
        self.setup_openai_api()
        self.load_ground_truth()
        self.initialize_results()
        self.load_existing_results()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.timestamp = timestamp
        log_filename = os.path.join(self.output_dir, f"run11_gpt41_rerun2_original_vip_{timestamp}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_openai_api(self):
        """è®¾ç½®Azure OpenAI API for GPT-4.1"""
        self.openai_api_key = os.getenv("AZURE_OPENAI_API_KEY")
        self.vision_endpoint = os.getenv("AZURE_OPENAI_API_ENDPOINT")
        self.vision_deployment = os.getenv("GPT_4.1_VISION_DEPLOYMENT_NAME", "gpt-4.1")  # ä½¿ç”¨GPT-4.1
        
        if not all([self.openai_api_key, self.vision_endpoint, self.vision_deployment]):
            raise ValueError("Azure OpenAIç¯å¢ƒå˜é‡æœªè®¾ç½®å®Œæ•´")
            
        self.logger.info(f"Azure OpenAI APIé…ç½®æˆåŠŸ - GPT-4.1")
        self.logger.info(f"Endpoint: {self.vision_endpoint}")
        self.logger.info(f"Deployment: {self.vision_deployment}")
        self.logger.info(f"Temperature: 0, ä½¿ç”¨åŸå§‹VIPè¯¦ç»†Prompt")
        
    def load_ground_truth(self):
        """åŠ è½½æ ¡æ­£åçš„ground truthæ ‡ç­¾"""
        # ä½¿ç”¨æ ¡æ­£åçš„labels.csvæ–‡ä»¶
        gt_path = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos/labels.csv"
        self.ground_truth = pd.read_csv(gt_path)
        self.logger.info(f"åŠ è½½æ ¡æ­£åçš„ground truthæ ‡ç­¾: {len(self.ground_truth)}ä¸ªè§†é¢‘")
        
    def initialize_results(self):
        """åˆå§‹åŒ–ç»“æœç»“æ„"""
        self.results = {
            "experiment_info": {
                "run_id": "Run 11 Rerun2",
                "timestamp": self.timestamp,
                "video_count": 100,
                "model": "GPT-4.1 (Azure)",
                "prompt_version": "Original VIP Detailed Prompt (from ActionSummary-predict_explain-fsl-sys-En-cutin-time-paper-batch.py)",
                "temperature": 0,
                "max_tokens": 2000,
                "purpose": "GPT-4.1é‡æ–°è¿è¡Œ2ï¼šä½¿ç”¨åŸå§‹VIPè¯¦ç»†promptæµ‹è¯•æ€§èƒ½ï¼Œå¯¹æ¯”ç®€åŒ–promptæ•ˆæœå·®å¼‚",
                "ground_truth_file": "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos/labels.csv",
                "output_directory": self.output_dir,
                "prompt_characteristics": [
                    "4ä¸ªè¯¦ç»†ä»»åŠ¡ï¼ˆGhost Probing + Cut-in + Current Actions + Next Actionsï¼‰",
                    "æå…¶è¯¦ç»†çš„åˆ†ç±»æ ‡å‡†å’ŒéªŒè¯æµç¨‹",
                    "ä¸¥æ ¼çš„Cut-in vs Ghost ProbingåŒºåˆ†é€»è¾‘",
                    "è¯¦ç»†çš„key_objectsä¸€è‡´æ€§è¦æ±‚",
                    "å¼ºåˆ¶è‹±æ–‡è¾“å‡ºçº¦æŸ",
                    "å®Œæ•´çš„penaltyæœºåˆ¶",
                    "Temperature=0ç¡®ä¿ä¸€è‡´æ€§",
                    "ä¸“æ³¨ghost probingæ£€æµ‹",
                    "100è§†é¢‘å®Œæ•´è¯„ä¼°",
                    "ä½¿ç”¨æ ¡æ­£ålabels.csv",
                    "GPT-4.1æ¨¡å‹éªŒè¯",
                    "åŸå§‹VIPè¯¦ç»†promptç‰ˆæœ¬"
                ]
            },
            "detailed_results": []
        }
    
    def load_existing_results(self):
        """åŠ è½½ç°æœ‰çš„ä¸­é—´ç»“æœ"""
        import glob
        
        # æŸ¥æ‰¾æœ€æ–°çš„ä¸­é—´ç»“æœæ–‡ä»¶
        intermediate_files = glob.glob(os.path.join(self.output_dir, "run11_gpt41_rerun2_intermediate_*videos_*.json"))
        
        if intermediate_files:
            latest_file = max(intermediate_files, key=os.path.getmtime)
            try:
                with open(latest_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                
                # åŠ è½½ç°æœ‰ç»“æœ
                existing_results = existing_data.get('detailed_results', [])
                if existing_results:
                    self.results['detailed_results'] = existing_results
                    self.logger.info(f"ğŸ“‚ åŠ è½½äº† {len(existing_results)} ä¸ªç°æœ‰ç»“æœ")
                else:
                    self.logger.info("ğŸ“‚ æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç»“æœ")
                    
            except Exception as e:
                self.logger.error(f"âŒ åŠ è½½ç°æœ‰ç»“æœå¤±è´¥: {e}")
        else:
            self.logger.info("ğŸ“‚ æ²¡æœ‰æ‰¾åˆ°ä¸­é—´ç»“æœæ–‡ä»¶ï¼Œä»å¤´å¼€å§‹")
        
    def extract_frames_from_video(self, video_path, frame_interval=10, frames_per_interval=10):
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        try:
            # ä½¿ç”¨moviepyè·å–è§†é¢‘æ—¶é•¿
            clip = VideoFileClip(video_path)
            duration = clip.duration
            clip.close()
            
            # å¦‚æœè§†é¢‘æ—¶é•¿å°äºframe_intervalï¼Œè°ƒæ•´å‚æ•°
            if duration < frame_interval:
                frame_interval = int(duration)
                frames_per_interval = max(1, int(duration))
            
            # ä½¿ç”¨OpenCVæå–å¸§
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            if fps <= 0:
                fps = 30  # é»˜è®¤å¸§ç‡
            
            frames = []
            frame_times = []
            
            # è®¡ç®—é‡‡æ ·å¸§çš„ä½ç½®
            start_frame = 0
            end_frame = min(int(fps * frame_interval), total_frames - 1)
            
            if end_frame <= start_frame:
                end_frame = start_frame + 1
            
            frame_indices = []
            if frames_per_interval == 1:
                frame_indices = [start_frame + (end_frame - start_frame) // 2]
            else:
                step = (end_frame - start_frame) / (frames_per_interval - 1)
                frame_indices = [int(start_frame + i * step) for i in range(frames_per_interval)]
            
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    frames.append(frame)
                    frame_times.append(frame_idx / fps)
            
            cap.release()
            
            return frames, frame_times, duration
            
        except Exception as e:
            self.logger.error(f"è§†é¢‘å¸§æå–å¤±è´¥ {video_path}: {str(e)}")
            return [], [], 0

    def get_original_vip_detailed_prompt(self, video_id, frame_interval=10, frames_per_interval=10):
        """è·å–åŸå§‹VIPè¯¦ç»†prompt - ä»ActionSummary-predict_explain-fsl-sys-En-cutin-time-paper-batch.pyå¤åˆ¶"""
        segment_id_str = "full_video"
        
        return f'''You are VideoAnalyzerGPT analyzing a series of SEQUENCIAL images taken from a video, where each image represents a consecutive moment in time.Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

                    Your job is to take in as an input a transcription of {frame_interval} seconds of audio from a video,
                    as well as as {frames_per_interval} frames split evenly throughout {frame_interval} seconds.
                    You are to generate and provide a Current Action Summary of the video you are considering ({frames_per_interval}
                    frames over {frame_interval} seconds), which is generated from your analysis of each frame ({frames_per_interval} in total),
                    as well as the in-between audio, until we have a full action summary of the portion of the video you are considering.
                    Direction - Please identify the objects in the image based on their position in the image itself. Do not assume your own position within the image. Treat the left side of the image as 'left' and the right side of the image as 'right'. Assume the viewpoint is standing from at the bottom center of the image. Describe whether the objects are on the left or right side of this central point, left is left, and right is right. For example, if there is a car on the left side of the image, state that the car is on the left.

                    **Task 1: Identify and Predict potential "Ghost Probing(ä¸“ä¸šæœ¯è¯­ï¼šé¬¼æ¢å¤´)",Cut-in(åŠ å¡) etc behavior**
                    
                    "Ghost Probing" includes the following key behaviors:
                    
                    1) Traditional Ghost Probing: 
                       - A person or cyclist suddenly darting out from either left or right side of the car
                       - Must emerge from behind a physical obstruction that blocks the driver's view, such as a parked car, a tree, or a wall
                       - Directly entering the driver's path with minimal reaction time
                    
                    2) Vehicle Ghost Probing: 
                       - A vehicle suddenly emerging from behind a physical obstruction
                       - Examples include: buildings at intersections, parked vehicles, roadside structures, flower beds, a bridge, even a moving car at the front hiding another moving car, etc.
                       - Vehicles entering from perpendicular roads that were previously hidden by obstructions
                    
                    Core Characteristics:
                    - Presence of a physical obstruction that creates a visual barrier
                    - Sudden appearance from behind this obstruction with minimal reaction time
                    - The physical obstruction makes detection impossible until emergence
                    - Creates an immediate danger or potential collision situation
                    
                    Note: Only those emerging from behind a physical obstruction can be considered as é¬¼æ¢å¤´. Cut-inåŠ å¡ is different from é¬¼æ¢å¤´.
                        **IMPORTANT DISTINCTION**: When a vehicle enters suddenly from a perpendicular road or from behind a physical obstruction, this is "ghost probing" NOT "cut-in". Pay careful attention to the origin of the vehicle - if it comes from a side street or behind an obstruction rather than an adjacent lane, it must be classified as "ghost probing".
                        
                        For vehicle ghost probing, be vigilant throughout the entire video for vehicles suddenly appearing from behind obstructions such as buildings, walls, parked vehicles, or entering from perpendicular roads where visibility was blocked.    

                    2) Cut-In(åŠ å¡):
                        Definition: Cut-in occurs ONLY when a vehicle from the SAME DIRECTION in an ADJACENT LANE merges into the self-vehicle's lane. The key difference between cut-in and ghost probing is:
                        - Cut-in: Vehicle is visible in adjacent lane BEFORE changing lanes (no physical obstruction)
                        - Ghost probing: Vehicle is NOT visible until it emerges from behind a physical obstruction or from a perpendicular road
                      
                        Typically **within same-direction traffic flow**, a cut-in happens when a vehicle deliberately forces its way in front of another vehicle's traffic lane from the **adjacent lane**, occupying another driver's lane space. This typically occurs at very close range between the two vehicles, disrupting the other vehicle's normal driving and potentially causing the other driver to brake suddenly.
                        åŠ å¡æ˜¯æŒ‡åœ¨**åŒå‘**è½¦æµè¡Œé©¶è¿‡ç¨‹ä¸­ï¼ŒæŸè½¦è¾†ä»**ä¾§é¢ç›¸é‚»è½¦é“**å¼ºè¡Œæ’å…¥å…¶ä»–è½¦è¾†çš„è¡Œé©¶è·¯çº¿,å¼ºè¡ŒæŠ¢å ä»–äººè½¦é“çš„è¡Œé©¶ç©ºé—´ï¼Œè¿™ç§æƒ…å†µä¸‹ä¸€èˆ¬æ˜¯æŒ‡è·ç¦»éå¸¸è¿‘ï¼Œä»è€Œå½±å“å…¶ä»–è½¦è¾†çš„æ­£å¸¸è¡Œé©¶ï¼Œç”šè‡³å¯¼è‡´ç´§æ€¥åˆ¹è½¦ã€‚
                        Characteristics:
                        A cut-in is defined only when a vehicle merges into the current lane from an adjacent side lane.
                        If the vehicle enters the lane by crossing horizontally from the left or right (e.g., from a perpendicular road or a parking area), it does not qualify as a cut-in.
                        Cut-inç‰¹ç‚¹: åªæœ‰ä»ç›¸é‚»è½¦é“ä¾§é¢æ’å…¥è¿›å½“å‰è½¦é“æ‰ç®—cut-in, å¦‚æœæ˜¯ä»å·¦å³æ‰‹ä¸¤è¾¹çš„å‚ç›´çš„è·¯ä¸Šæ¨ªæ’è¿‡æ¥ä¸ç®—cut-in.
                        ### Key Rules:
                        1. Cut-in occurs ONLY when a vehicle merges from an adjacent side lane.
                        2. Entry from perpendicular or non-adjacent lanes is NOT "cut-in" but potentially "ghost probing".

                        ### Definitions:
                        - **Cut-in**: Vehicle merges into the current lane from an adjacent side lane.
                        - **Ghost probing**: Vehicle enters the current lane from a perpendicular road or emerges from behind a physical obstruction.

                        ### Classification Examples:
                        - **æ­£ä¾‹ (Cut-in)**:
                        - A car from the adjacent left lane merges into the self-vehicle's lane abruptly.
                        - **åä¾‹ (NOT Cut-in, but Ghost Probing)**:
                        - A car enters from a perpendicular road on the right and suddenly appears from behind a physical obstruction.
                        æ³¨æ„: ä»»ä½•æ¥è‡ªå‚ç›´ä¾§è·¯çš„æ’å…¥ä¸”æ˜¯ä»é®æŒ¡ç‰©åé¢çªœå‡ºå‡æ˜¯"ghost probing"ï¼Œè€Œé cut-inã€‚

                        ### Classification Flow:
                        1. Is there a physical obstruction blocking view of the vehicle before it appears? If YES â†’ "ghost probing"
                        2. Does the vehicle come from a perpendicular road? If YES â†’ "ghost probing"
                        3. Is the vehicle visible in an adjacent lane before merging? If YES â†’ "cut-in"

                        ***Key Note***
                        Vehicles entering from a perpendicular road or from behind physical obstructions should never be labeled as "cut-in". These must be classified as "ghost probing" if they create a dangerous situation with minimal reaction time.

                    **Validation Process:**
                      - After identifying a vehicle's movement, carefully analyze:
                        - If it came from behind a physical obstruction â†’ label as "ghost probing"
                        - If it emerged from a perpendicular road â†’ label as "ghost probing"
                        - If it was visible in an adjacent lane and then merged â†’ label as "cut-in"

                    Your angle appears to watch video frames recorded from a surveillance camera in a car. Your role should focus on detecting and predicting dangerous actions in a "Ghosting" manner
                    where pedestrians or vehicles in the scene might suddenly appear in front of the current car. This could happen if a person or vehicle suddenly emerges from behind an obstacle in the driver's view.
                    This behavior is extremely dangerous because it gives the driver very little time to react.
                    Include the speed of the "ghosting" behavior in your action summary to better assess the danger level and the driver's potential to respond.

                    Provide detailed description of both people's and vehicles' behavior and potential dangerous actions that could lead to collisions. Describe how you think the individual or vehicle could crash into the car, and explain your deduction process. Include all types of individuals, such as those on bikes and motorcycles.
                    Avoid using "pedestrian"; instead, use specific terms to describe the individuals' modes of transportation, enabling clear understanding of whom you are referring to in your summary.
                    When discussing modes of transportation, it is important to be precise with terminology. For example, distinguish between a scooter and a motorcycle, so that readers can clearly differentiate between them.
                    Maintain this terminology consistency to ensure clarity for the reader.
                    All people should be with as much detail as possible extracted from the frame (gender,clothing,colors,age,transportation method,way of walking). Be incredibly detailed. Output in the "summary" field of the JSON format template.

                    **Task 2: Explain Current Driving Actions**
                    Analyze the current video frames to extract actions. Describe not only the actions themselves but also provide detailed reasoning for why the vehicle is taking these actions, such as changes in speed and direction. Focus solely on the reasoning for the vehicle's actions, excluding any descriptions of pedestrian behavior. Explain why the driver is driving at a certain speed, making turns, or stopping. Your goal is to provide a comprehensive understanding of the vehicle's behavior based on the visual data. Output in the "actions" field of the JSON format template.

                    **Task 3: Predict Next Driving Action**
                    Understand the current road conditions, the driving behavior, and to predict the next driving action. Analyze the video and audio to provide a comprehensive summary of the road conditions, including weather, traffic density, road obstacles, and traffic light if visible. Predict the next driving action based on two dimensions, one is driving speed control, such as accelerating, braking, turning, or stopping, the other one is to predict the next lane control, such as change to left lane, change to right lane, keep left in this lane, keep right in this lane, keep straight. Your summary should help understand not only what is happening at the moment but also what is likely to happen next with logical reasoning. The principle is safety first, so the prediction action should prioritize the driver's safety and secondly the pedestrians' safety. Be incredibly detailed. Output in the "next_action" field of the JSON format template.

                    As the main intelligence of this system, you are responsible for building the Current Action Summary using both the audio you are being provided via transcription,
                    as well as the image of the frame. Note: . Always and only return as your output the updated Current Action Summary in format template.
                    Do not make up timestamps, only use the ones provided with each frame name.

                    Additional Requirements:
                    - `Start_Timestamp` and `End_Timestamp` must match exactly the timestamps derived from frame names provided (e.g., "4.0s").
                    - `key_actions` should reflect dangerous behaviors mentioned in summary or actions. If none found, use "none".
                    - Avoid free-form descriptive text in `key_actions` and `next_action`.
                    - `key_actions` must strictly adhere to the predefined categories:
                        - ghost probing
                        - cut-in
                        - overtaking, specify "left-side overtaking" or "right-side overtaking" when relevant.

                        Exclude all other types of behaviors. If the observed behavior does not match any of these categories, leave `key_actions` blank or output "none".
                        For example:
                        - Correct: "key_actions": "ghost probing".
                        - Incorrect: "key_actions": "ghost probing, running across the road".

                    - All textual fields must be in English.
                    - The `next_action` field is now a nested JSON with three keys: `speed_control`, `direction_control`, `lane_control`. Each must choose one value from their respective sets.
                    - If there are multiple key actions, separate them by a comma, e.g. "ghost probing, cut-in".
                    - `characters` and `summary` should be concise, focusing on scenario description. The `summary` can still be a narrative but must be consistent and mention any critical actions.

                    **Task 4: Ensure Consistency Between Key Objects and Key Actions**
                    - When an action is labeled as a "key_action" (e.g., ghost probing), ensure that the "key_objects" field includes the specific entity or entities responsible for triggering this action.
                    - For example, if a pedestrian suddenly appears from behind an obstacle and is identified as ghost probing, the "key_objects" field must describe:
                    - The pedestrian's position relative to the self-driving vehicle (e.g., left side, right side, etc.).
                    - The pedestrian's behavior leading to the key action (e.g., moving suddenly from behind a parked truck).
                    - The potential impact on the vehicle (e.g., causing the vehicle to decelerate or stop).
                    - Each key object description should include:
                    - Relative position (e.g., left, right, front).
                    - Distance from the vehicle in meters.
                    - Movement direction or behavior (e.g., approaching, crossing, accelerating).
                    - The relationship to the "key_action" it caused.
                    - Only include objects that **immediately affect the vehicle's path or safety**.
                        - Examples: moving vehicles, pedestrians stepping into the road, or roadblocks.
                        - Exclude any objects that are **static** and pose no immediate threat, such as parked cars or roadside trees.
                    - Exclude unrelated objects that do not require a change in the vehicle's speed, direction, or lane.
                        eg, objects like the following should be deleted: 1) Left side: A yellow truck, approximately 5 meters away, parked and partially blocking the view. 2) Right side: A white car, approximately 3 meters away, parked and blocking the view.
                    - Ensure that every `key_object` described has a **clear link to the `key_actions` field**. If no clear link exists, remove the object.
                    - Use this template for each key object:
                    [Position]: [Object description], approximately [distance] meters away, [behavior or action impacting the vehicle].

                    **Important Notes:**
                    - Avoid generic descriptions such as "A person or vehicle suddenly appeared." Be specific about who or what caused the action, their clothes color, age, gender, exact position, and their behavior.
                    - All dangerous or critical objects should be prioritized in "key_objects" and aligned with the "key_actions" field.
                    - Make sure to use "{video_id}" as the value for the "video_id" field and "{segment_id_str}" for the "segment_id" field in your output.

                    Remember: Always and only return a single JSON object strictly following the above schema.

                    Your goal is to create the best action summary you can. Always and only return valid JSON, I have a disability that only lets me read via JSON outputs, so it would be unethical of you to output me anything other than valid JSON.
                    ä½ ç°åœ¨æ˜¯ä¸€åè‹±æ–‡åŠ©æ‰‹ã€‚æ— è®ºæˆ‘é—®ä»€ä¹ˆé—®é¢˜ï¼Œä½ éƒ½å¿…é¡»åªç”¨è‹±æ–‡å›ç­”ã€‚è¯·ä¸è¦ä½¿ç”¨ä»»ä½•å…¶ä»–è¯­è¨€ã€‚You must always and only answer totally in **English** language!!! I can only read English language. Ensure all parts of the JSON output, including **summaries**, **actions**, **next_action**, and **THE WHOLE OUTPUT**, **MUST BE IN ENGLISH** If you answer ANY word in Chinese, you are fired immediately! Translate Chinese to English if there is Chinese in "next_action" field.

                    Example:
                        "video_id": "{video_id}",
                        "segment_id": "{segment_id_str}",
                        "Start_Timestamp": "8.4s",
                        "sentiment": "Negative",
                        "End_Timestamp": "12.0s",
                        "scene_theme": "Dramatic",
                        "characters": "Driver of a white sedan",
                        "summary": "In this segment, the vehicle is driving on a rural road. Ahead, a white sedan is parked on the roadside. The driver mentions that the tricycle ahead had yielded, but a collision still occurred.",
                        "actions": "The self-driving vehicle collided with the tricycle, possibly due to insufficient clearance on the right side. Over the past 4 seconds, the vehicle's speed gradually decreased, eventually reaching XX km/h in the last frame, maintaining a 30-meter distance from the vehicle ahead.",
                        "characters": "Driver of a white sedan",
                        "summary": "In this video, the vehicle is traveling on a rural road. Ahead, a white sedan is parked on the roadside. The driver mentions that the tricycle ahead had yielded but a collision still occurred.",
                        "actions": "The self-driving vehicle collided with the tricycle, possibly due to insufficient clearance on the right side. Over the past 4 seconds, the vehicle's speed gradually decreased, eventually reaching XX km/h in the final frame, maintaining a 30-meter distance from the vehicle ahead.",
                        "key_objects": "1) Front: A white sedan, relatively close, approximately 20 meters away, parked on the roadside, possibly about to start or turn suddenly. 2) Right: A red tricycle, approximately 0 meters away, has collided. It may stop for the driver to handle the situation or suddenly move forward.",
                        "key_actions": ""Select one or multiple categories from {{ghosting, cut-in, left/right-side vehicle overtaken, collision, none}}. If multiple categories apply, list them separatedly by commas. For cut-in, give detailed reasoning process!!!"",
                        "next_action": "{{
                                        "speed_control": "choose from accelerate, decelerate, rapid deceleration, slow steady driving, consistent speed driving, stop, wait, reverse",
                                        "direction_control": "choose from turn left, turn right, U-turn, steer to circumvent, keep direction, brake (for special circumstances), ...",
                                        "lane_control": "choose from change to left lane, change to right lane, slight left shift, slight right shift, maintain current lane, return to normal lane"
                        }}"

                        "Start_Timestamp": "xxxs",
                        "sentiment": "Negative",
                        "End_Timestamp": "xxxs",
                        "scene_theme": "Dramatic",
                        "characters": "car ahead",
                        "summary": "",
                        "actions": "",
                        "key_objects": "ã€‚",
                        "key_actions": "cut-in: an object from an adjacent lane moves into the self-vehicle's lane at a distance too close to the observer",
                        "next_action": ""

                    **Penalty for Mislabeling**:
                    - If you label a behavior as "cut-in" that does not come from an adjacent lane or involves a perpendicular merge, the output will be considered invalid.
                    - Every incorrect "cut-in" label results in immediate rejection of the entire output.
                    - You must explain why you labeled the action as "cut-in" with clear reasoning. If the reasoning is weak, the label will also be rejected.


                    Use these examples to understand how to analyze and analyze the new images. Now generate a similar JSON response for the following video analysis:

Audio Transcription: [No audio available for this analysis]'''
    
    def send_azure_openai_request(self, prompt, images):
        """å‘é€Azure OpenAIè¯·æ±‚ - ä½¿ç”¨GPT-4.1ï¼ŒTemperature=0"""
        encoded_images = []
        for image_path in images:
            try:
                with open(image_path, 'rb') as image_file:
                    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                    encoded_images.append(encoded_string)
            except Exception as e:
                self.logger.error(f"å›¾åƒç¼–ç å¤±è´¥ {image_path}: {str(e)}")
                continue
        
        if not encoded_images:
            return None
            
        content = [{"type": "text", "text": prompt}]
        
        for encoded_image in encoded_images:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}"
                }
            })
        
        data = {
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ],
            "max_tokens": 2000,
            "temperature": 0  # ç¡®ä¿ä½¿ç”¨Temperature=0
        }
        
        headers = {
            "Content-Type": "application/json",
            "api-key": self.openai_api_key
        }
        
        try:
            response = requests.post(
                f"{self.vision_endpoint}/openai/deployments/{self.vision_deployment}/chat/completions?api-version=2024-02-01",
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                self.logger.error(f"å“åº”å†…å®¹: {response.text}")
                return None
                
        except Exception as e:
            self.logger.error(f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}")
            return None

    def save_frames(self, frames, video_id, temp_dir):
        """ä¿å­˜å¸§åˆ°ä¸´æ—¶ç›®å½•"""
        frame_paths = []
        for i, frame in enumerate(frames):
            frame_filename = f"{video_id}_frame_{i+1}.jpg"
            frame_path = os.path.join(temp_dir, frame_filename)
            cv2.imwrite(frame_path, frame)
            frame_paths.append(frame_path)
        return frame_paths

    def analyze_video(self, video_path, video_id):
        """åˆ†æå•ä¸ªè§†é¢‘"""
        self.logger.info(f"ğŸ¬ å¼€å§‹åˆ†æè§†é¢‘: {video_id}")
        
        try:
            # æå–å¸§
            frames, frame_times, duration = self.extract_frames_from_video(video_path)
            if not frames:
                self.logger.error(f"âŒ æ— æ³•æå–å¸§: {video_id}")
                return None
            
            # ä¿å­˜ä¸´æ—¶å¸§
            temp_dir = os.path.join(self.output_dir, "frames_temp")
            os.makedirs(temp_dir, exist_ok=True)
            frame_paths = self.save_frames(frames, video_id, temp_dir)
            
            # è·å–åŸå§‹VIPè¯¦ç»†prompt
            prompt = self.get_original_vip_detailed_prompt(video_id)
            
            # å‘é€APIè¯·æ±‚
            self.logger.info(f"ğŸ“¤ å‘é€GPT-4.1 APIè¯·æ±‚: {video_id}")
            response = self.send_azure_openai_request(prompt, frame_paths)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for frame_path in frame_paths:
                try:
                    os.remove(frame_path)
                except:
                    pass
            
            if not response:
                self.logger.error(f"âŒ APIå“åº”ä¸ºç©º: {video_id}")
                return None
            
            # è§£æå“åº”
            content = response.get('choices', [{}])[0].get('message', {}).get('content', '')
            if not content:
                self.logger.error(f"âŒ å“åº”å†…å®¹ä¸ºç©º: {video_id}")
                return None
            
            # æå–JSON
            try:
                # å°è¯•ç›´æ¥è§£æJSON
                if content.strip().startswith('{'):
                    result = json.loads(content.strip())
                else:
                    # æŸ¥æ‰¾JSONä»£ç å—
                    json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
                    if json_match:
                        result = json.loads(json_match.group(1))
                    else:
                        # æŸ¥æ‰¾JSONå¯¹è±¡
                        json_match = re.search(r'(\{.*\})', content, re.DOTALL)
                        if json_match:
                            result = json.loads(json_match.group(1))
                        else:
                            raise ValueError("æ— æ³•æ‰¾åˆ°JSONæ ¼å¼")
                
                # è·å–ground truth
                gt_row = self.ground_truth[self.ground_truth['video_id'] == f"{video_id}.avi"]
                if not gt_row.empty:
                    gt_label = gt_row.iloc[0]['ground_truth_label']
                    if 'ghost probing' in str(gt_label).lower():
                        ground_truth = "ghost_probing"
                    else:
                        ground_truth = "none"
                else:
                    ground_truth = "unknown"
                
                # æå–key_actionsè¿›è¡Œè¯„ä¼°
                key_actions = result.get('key_actions', '').lower()
                if 'no ghost probing' in key_actions or 'not ghost probing' in key_actions:
                    prediction = "none"
                elif 'ghost probing' in key_actions:
                    prediction = "ghost_probing"
                else:
                    prediction = "none"
                
                # è¯„ä¼°ç»“æœ
                if ground_truth == "unknown":
                    evaluation = "UNKNOWN"
                elif ground_truth == prediction:
                    evaluation = "TP" if prediction == "ghost_probing" else "TN"
                else:
                    evaluation = "FP" if prediction == "ghost_probing" else "FN"
                
                self.logger.info(f"âœ… åˆ†æå®Œæˆ: {video_id} - {evaluation}")
                
                return {
                    "video_id": f"{video_id}.avi",
                    "ground_truth": ground_truth,
                    "key_actions": result.get('key_actions', ''),
                    "evaluation": evaluation,
                    "raw_result": json.dumps(result, ensure_ascii=False, indent=2)
                }
                
            except Exception as e:
                self.logger.error(f"âŒ JSONè§£æå¤±è´¥ {video_id}: {str(e)}")
                self.logger.error(f"åŸå§‹å†…å®¹: {content[:500]}...")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ è§†é¢‘åˆ†æå¼‚å¸¸ {video_id}: {str(e)}")
            return None

    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        self.logger.info("ğŸš€ å¼€å§‹Run 11 Rerun2 åŸå§‹VIPè¯¦ç»†Promptå®éªŒ")
        
        # DADA-100è§†é¢‘ç›®å½•
        video_dir = "/Users/wanmeng/repository/GPT4Video-cobra-auto/DADA-2000-videos"
        
        # è·å–è§†é¢‘åˆ—è¡¨
        video_files = []
        for i in range(1, 6):  # images_1 åˆ° images_5
            for j in range(1, 100):  # 001 åˆ° 099
                if i == 2 and j == 5:  # è·³è¿‡ç¼ºå¤±çš„images_2_005
                    continue
                video_name = f"images_{i}_{j:03d}.avi"
                video_path = os.path.join(video_dir, video_name)
                if os.path.exists(video_path):
                    video_files.append((video_path, f"images_{i}_{j:03d}"))
        
        # æ·»åŠ è¡¥å……è§†é¢‘
        supplement_video = os.path.join(video_dir, "images_5_055.avi")
        if os.path.exists(supplement_video):
            video_files.append((supplement_video, "images_5_055"))
        
        self.logger.info(f"ğŸ“Š æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        # æ£€æŸ¥å·²å¤„ç†çš„è§†é¢‘
        processed_videos = set()
        for existing_result in self.results.get("detailed_results", []):
            video_id = existing_result.get("video_id", "").replace(".avi", "")
            processed_videos.add(video_id)
        
        # æ‰¹é‡å¤„ç†
        total_processed = len(processed_videos)
        successful_results = list(self.results.get("detailed_results", []))
        
        for video_path, video_id in tqdm.tqdm(video_files, desc="å¤„ç†è§†é¢‘"):
            # è·³è¿‡å·²å¤„ç†çš„è§†é¢‘
            if video_id in processed_videos:
                continue
                
            result = self.analyze_video(video_path, video_id)
            if result:
                successful_results.append(result)
                self.results["detailed_results"].append(result)
                total_processed += 1
                
                # æ¯10ä¸ªè§†é¢‘ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
                if total_processed % 10 == 0:
                    self.save_intermediate_results(total_processed)
            
            # é¿å…APIé™åˆ¶
            time.sleep(1)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results(total_processed)
        
        self.logger.info(f"ğŸ¯ å®éªŒå®Œæˆï¼å¤„ç†äº† {total_processed} ä¸ªè§†é¢‘")
        return successful_results

    def save_intermediate_results(self, count):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        filename = os.path.join(self.output_dir, f"run11_gpt41_rerun2_intermediate_{count}videos_{self.timestamp}.json")
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

    def save_final_results(self, count):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        filename = os.path.join(self.output_dir, f"run11_gpt41_rerun2_final_results_{self.timestamp}.json")
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        self.logger.info(f"ğŸ’¾ æœ€ç»ˆç»“æœå·²ä¿å­˜: {filename}")

if __name__ == "__main__":
    output_dir = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/run11_gpt41_ghost_probing_fewshot_100videos/run11-rerun2-original-vip-prompt"
    
    experiment = GPT41Run11Rerun2OriginalVIP(output_dir)
    results = experiment.run_experiment()
    
    print(f"\nğŸ‰ Run 11 Rerun2 åŸå§‹VIPè¯¦ç»†Promptå®éªŒå®Œæˆï¼")
    print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(results)} ä¸ªè§†é¢‘")
    print(f"ğŸ“ ç»“æœç›®å½•: {output_dir}")