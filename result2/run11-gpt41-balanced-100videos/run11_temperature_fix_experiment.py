#!/usr/bin/env python3
"""
Run 11 Temperatureä¿®æ­£å®éªŒ - å°è¯•å¤ç°GPT-4.1å†å²æœ€ä½³ç»“æœ
åŸºäºå‘ç°å†å²é…ç½®ä½¿ç”¨temperature=0.3è€Œé0çš„å…³é”®å·®å¼‚
"""

import os
import sys
import json
import time
import base64
import logging
import requests
import pandas as pd
import cv2
import numpy as np
from datetime import datetime
from dotenv import load_dotenv
from moviepy.editor import VideoFileClip

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append("/Users/wanmeng/repository/GPT4Video-cobra-auto")
import video_utilities as vu

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class GPT41TemperatureFixExperiment:
    def __init__(self):
        """åˆå§‹åŒ–å®éªŒé…ç½®"""
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/run11-gpt41-balanced-100videos"
        
        # APIé…ç½® - ä¿®æ­£å…³é”®çš„temperatureå‚æ•°
        self.openai_api_key = os.environ["OPENAI_API_KEY"]
        self.vision_endpoint = os.environ["VISION_ENDPOINT"]
        if not self.vision_endpoint.endswith("/"):
            self.vision_endpoint += "/"
        self.vision_deployment = os.environ.get("GPT_4.1_VISION_DEPLOYMENT_NAME", "gpt-4.1")
        
        # å…³é”®ä¿®æ­£ï¼šä½¿ç”¨å†å²æœ€ä½³é…ç½®çš„temperature=0.3
        self.temperature = 0.3  # å†å²é…ç½®ä½¿ç”¨0.3ï¼ŒRun 11ä½¿ç”¨0
        
        # å…¶ä»–å†å²åŒ¹é…é…ç½®
        self.max_tokens = 2000
        self.api_version = "2024-02-15-preview"
        
        # è§†é¢‘å¤„ç†é…ç½®
        self.frame_interval = 10
        self.frames_per_interval = 10
        self.fps = 1
        
        # æ•°æ®é›†é…ç½®
        self.video_dir = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos"
        self.ground_truth_path = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos/groundtruth_labels.csv"
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(self.output_dir, f'temperature_fix_experiment_{self.timestamp}.log')),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½ground truthæ•°æ®
        self.ground_truth = pd.read_csv(self.ground_truth_path, sep='\t', engine='python')
        # å¤„ç†æ ‡ç­¾ï¼Œå°†"ghost probing"è½¬æ¢ä¸º1ï¼Œ"none"è½¬æ¢ä¸º0
        self.ground_truth['ghost_probing'] = self.ground_truth['ground_truth_label'].apply(
            lambda x: 1 if 'ghost probing' in str(x) else 0
        )
        self.logger.info(f"åŠ è½½ground truthæ•°æ®: {len(self.ground_truth)}ä¸ªè§†é¢‘")
        
        # ç»“æœå­˜å‚¨
        self.results = {
            "experiment_info": {
                "run_id": "Run 11 Temperature Fix",
                "timestamp": self.timestamp,
                "key_change": "temperature: 0 -> 0.3 (å†å²é…ç½®æ¢å¤)",
                "model": "GPT-4.1 (Azure)",
                "prompt_version": "Balanced (Historical Best Recreation)",
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "purpose": "ä¿®æ­£temperatureå‚æ•°ä»¥å¤ç°å†å²æœ€ä½³ç»“æœ",
                "target_metrics": {
                    "f1_score": 0.712,
                    "recall": 0.963,
                    "precision": 0.565
                }
            },
            "detailed_results": []
        }
        
        self.logger.info("ğŸ§ª Temperatureä¿®æ­£å®éªŒåˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ”¥ å…³é”®ä¿®æ­£: temperature = {self.temperature} (å†å²é…ç½®)")
        self.logger.info(f"ğŸ“Š ç›®æ ‡: F1=0.712, å¬å›ç‡=0.963, ç²¾ç¡®åº¦=0.565")
    
    def get_balanced_prompt(self, video_id, segment_id="full_video", start_time=0, end_time=10):
        """è·å–å†å²æœ€ä½³çš„Balanced prompt - å®Œå…¨ä¸€è‡´ç‰ˆæœ¬"""
        return f"""You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

Your job is to take in as an input a transcription of {self.frame_interval} seconds of audio from a video,
as well as {self.frames_per_interval} frames split evenly throughout {self.frame_interval} seconds.
You are to generate and provide a Current Action Summary of the video you are considering ({self.frames_per_interval}
frames over {self.frame_interval} seconds), which is generated from your analysis of each frame ({self.frames_per_interval} in total),
as well as the in-between audio, until we have a full action summary of the video.

IMPORTANT: For ghost probing detection, consider TWO categories:

**1. HIGH-CONFIDENCE Ghost Probing (use "ghost probing" in key_actions)**:
- Object appears EXTREMELY close (within 1-2 vehicle lengths, <3 meters) 
- Appearance is SUDDEN and from blind spots (behind parked cars, buildings, corners)
- Occurs in HIGH-RISK environments: highways, rural roads, parking lots, uncontrolled intersections
- Requires IMMEDIATE emergency braking/swerving to avoid collision
- Movement is COMPLETELY UNPREDICTABLE and violates traffic expectations

**2. POTENTIAL Ghost Probing (use "potential ghost probing" in key_actions)**:
- Object appears suddenly but at moderate distance (3-5 meters)
- Sudden movement in environments where some unpredictability exists
- Requires emergency braking but collision risk is moderate
- Movement is unexpected but not completely impossible given the context

**3. NORMAL Traffic Situations (do NOT use "ghost probing")**:
- Pedestrians crossing at intersections, crosswalks, or traffic lights
- Vehicles making normal lane changes, turns, or merging with signals
- Cyclists following predictable paths in urban areas or bike lanes
- Any movement that is EXPECTED given the traffic environment and context

**Environment Context Guidelines**:
- INTERSECTION/CROSSWALK: Expect pedestrians and cyclists - use "emergency braking due to pedestrian crossing"
- HIGHWAY/RURAL: Higher chance of genuine ghost probing - be more sensitive
- PARKING LOT: Expect sudden vehicle movements - use "potential ghost probing" if very sudden
- URBAN STREET: Mixed - consider visibility and predictability

Use "ghost probing" for clear cases, "potential ghost probing" for borderline cases, and descriptive terms for normal traffic situations.

Your response should be a valid JSON object with the following EXACT structure (match this format precisely):
{{
    "video_id": "{video_id}",
    "segment_id": "{segment_id}",
    "Start_Timestamp": "{start_time:.1f}s",
    "End_Timestamp": "{end_time:.1f}s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene",
    "summary": "comprehensive summary of the scene and what happens",
    "actions": "actions taken by the vehicle and driver responses",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "brief description of most important actions (use 'ghost probing', 'potential ghost probing', or descriptive terms as appropriate)",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

Audio Transcription: [No audio analysis in this experiment]"""
    
    def send_azure_openai_request(self, prompt, frames):
        """å‘é€Azure OpenAIè¯·æ±‚ - ä½¿ç”¨ä¿®æ­£çš„temperatureå‚æ•°"""
        encoded_images = []
        for frame_path in frames:
            with open(frame_path, 'rb') as image_file:
                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                encoded_images.append(encoded_string)
        
        content = [{"type": "text", "text": prompt}]
        
        for encoded_image in encoded_images:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}"
                }
            })
        
        # å…³é”®ä¿®æ­£ï¼šä½¿ç”¨temperature=0.3è€Œé0
        data = {
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ],
            "max_tokens": self.max_tokens,
            "temperature": self.temperature  # ğŸ”¥ å…³é”®ä¿®æ­£ç‚¹
        }
        
        headers = {
            "Content-Type": "application/json",
            "api-key": self.openai_api_key
        }
        
        url = f"{self.vision_endpoint}openai/deployments/{self.vision_deployment}/chat/completions?api-version={self.api_version}"
        
        # å†å²é…ç½®çš„é‡è¯•æœºåˆ¶
        max_retries = 2
        wait_exponential_multiplier = 2000
        wait_exponential_max = 60000
        
        for attempt in range(max_retries):
            try:
                response = requests.post(url, headers=headers, json=data, timeout=90)
                
                if response.status_code == 200:
                    response_data = response.json()
                    return response_data['choices'][0]['message']['content']
                else:
                    self.logger.error(f"APIè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {response.status_code} - {response.text}")
                    
            except requests.exceptions.Timeout as e:
                self.logger.error(f"APIè¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            except Exception as e:
                self.logger.error(f"APIè¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
            
            if attempt < max_retries - 1:
                wait_time = min(wait_exponential_multiplier * (2 ** attempt), wait_exponential_max) / 1000
                time.sleep(wait_time)
        
        return None
    
    def extract_frames_from_video(self, video_path):
        """ä»è§†é¢‘ä¸­æå–å¸§ - è‡ªå®ç°ç‰ˆæœ¬"""
        frames_dir = "frames_temp"
        if not os.path.exists(frames_dir):
            os.makedirs(frames_dir)
        
        try:
            # ä½¿ç”¨moviepyæå–å¸§
            video_clip = VideoFileClip(video_path)
            duration = video_clip.duration
            
            frame_files = []
            for i in range(self.frames_per_interval):
                frame_time = i * (self.frame_interval / self.frames_per_interval)
                if frame_time >= duration:
                    break
                
                frame_path = os.path.join(frames_dir, f"frame_at_{frame_time:.1f}s.jpg")
                
                # æå–å¸§å¹¶ä¿å­˜
                frame = video_clip.get_frame(frame_time)
                cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
                frame_files.append(frame_path)
            
            video_clip.close()
            return frame_files
            
        except Exception as e:
            self.logger.error(f"å¸§æå–å¤±è´¥: {str(e)}")
            return []
    
    def process_single_video(self, video_id):
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        start_time = time.time()
        
        # è·å–ground truthæ ‡ç­¾
        gt_row = self.ground_truth[self.ground_truth['video_id'] == video_id]
        if gt_row.empty:
            self.logger.error(f"è§†é¢‘ {video_id} åœ¨ground truthä¸­æœªæ‰¾åˆ°")
            return None
        
        actual_label = int(gt_row.iloc[0]['ghost_probing'])
        
        # æ„å»ºè§†é¢‘æ–‡ä»¶è·¯å¾„ - å¤„ç†é‡å¤æ‰©å±•åé—®é¢˜
        if video_id.endswith('.avi'):
            video_path = os.path.join(self.video_dir, video_id)
        else:
            video_path = os.path.join(self.video_dir, f"{video_id}.avi")
        if not os.path.exists(video_path):
            self.logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return None
        
        # æå–å¸§
        frames = self.extract_frames_from_video(video_path)
        if not frames:
            self.logger.error(f"è§†é¢‘ {video_id} å¸§æå–å¤±è´¥")
            return None
        
        # ç”Ÿæˆprompt
        prompt = self.get_balanced_prompt(video_id)
        
        # å‘é€APIè¯·æ±‚
        response = self.send_azure_openai_request(prompt, frames)
        
        # æ¸…ç†ä¸´æ—¶å¸§æ–‡ä»¶
        for frame_path in frames:
            if os.path.exists(frame_path):
                os.remove(frame_path)
        
        if response is None:
            self.logger.error(f"è§†é¢‘ {video_id} APIè¯·æ±‚å¤±è´¥")
            return {
                "video_id": video_id,
                "status": "api_error", 
                "actual_label": actual_label,
                "predicted_label": None,
                "processing_time": time.time() - start_time
            }
        
        # è§£æå“åº”
        try:
            parsed_result = json.loads(response)
            key_actions = parsed_result.get('key_actions', '').lower()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºghost probing
            predicted_label = 1 if 'ghost probing' in key_actions else 0
            
            processing_time = time.time() - start_time
            self.logger.info(f"è§†é¢‘ {video_id} å®Œæˆ - é¢„æµ‹: {predicted_label}, å®é™…: {actual_label}, ç”¨æ—¶: {processing_time:.1f}s")
            
            return {
                "video_id": video_id,
                "status": "success",
                "actual_label": actual_label, 
                "predicted_label": predicted_label,
                "processing_time": processing_time,
                "raw_response": response,
                "parsed_result": parsed_result,
                "key_actions": key_actions
            }
            
        except json.JSONDecodeError as e:
            self.logger.error(f"è§†é¢‘ {video_id} JSONè§£æå¤±è´¥: {str(e)}")
            return {
                "video_id": video_id,
                "status": "parse_error",
                "actual_label": actual_label,
                "predicted_label": None,
                "processing_time": time.time() - start_time,
                "raw_response": response
            }
    
    def calculate_metrics(self):
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        tp = fp = tn = fn = errors = 0
        
        for result in self.results["detailed_results"]:
            if result["status"] != "success":
                errors += 1
                continue
                
            predicted = result["predicted_label"]
            actual = result["actual_label"]
            
            if predicted == 1 and actual == 1:
                tp += 1
            elif predicted == 1 and actual == 0:
                fp += 1
            elif predicted == 0 and actual == 1:
                fn += 1
            else:
                tn += 1
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        
        return {
            "f1_score": f1,
            "precision": precision,
            "recall": recall,
            "accuracy": accuracy,
            "tp": tp, "fp": fp, "tn": tn, "fn": fn, "errors": errors
        }
    
    def run_experiment(self, limit=20):
        """è¿è¡ŒTemperatureä¿®æ­£å®éªŒ"""
        self.logger.info(f"ğŸš€ å¼€å§‹Temperatureä¿®æ­£å®éªŒ (limit={limit})")
        self.logger.info(f"ğŸ”¥ ä½¿ç”¨temperature={self.temperature} (å†å²é…ç½®)")
        
        # è·å–è§†é¢‘åˆ—è¡¨
        video_ids = self.ground_truth['video_id'].unique()[:limit]
        
        for i, video_id in enumerate(video_ids, 1):
            self.logger.info(f"å¤„ç†è§†é¢‘ {i}/{len(video_ids)}: {video_id}")
            
            result = self.process_single_video(video_id)
            if result:
                self.results["detailed_results"].append(result)
            
            # æ¯5ä¸ªè§†é¢‘ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if i % 5 == 0:
                self.save_intermediate_results()
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        metrics = self.calculate_metrics()
        self.results["performance_metrics"] = metrics
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results()
        
        # è¾“å‡ºç»“æœæ‘˜è¦
        self.logger.info("ğŸ Temperatureä¿®æ­£å®éªŒå®Œæˆ")
        self.logger.info(f"æ€§èƒ½æŒ‡æ ‡: F1={metrics['f1_score']:.3f}, å¬å›ç‡={metrics['recall']:.3f}, ç²¾ç¡®åº¦={metrics['precision']:.3f}")
        
        # ä¸å†å²ç›®æ ‡å¯¹æ¯”
        f1_diff = metrics['f1_score'] - 0.712
        recall_diff = metrics['recall'] - 0.963
        precision_diff = metrics['precision'] - 0.565
        
        self.logger.info("ğŸ“Š ä¸å†å²ç›®æ ‡å¯¹æ¯”:")
        self.logger.info(f"  F1åˆ†æ•°: {metrics['f1_score']:.3f} vs 0.712 ({f1_diff:+.3f})")
        self.logger.info(f"  å¬å›ç‡: {metrics['recall']:.3f} vs 0.963 ({recall_diff:+.3f})")
        self.logger.info(f"  ç²¾ç¡®åº¦: {metrics['precision']:.3f} vs 0.565 ({precision_diff:+.3f})")
        
        return metrics
    
    def save_intermediate_results(self):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        filename = os.path.join(self.output_dir, f"temperature_fix_intermediate_{self.timestamp}.json")
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
    
    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        # ä¿å­˜è¯¦ç»†ç»“æœ
        json_filename = os.path.join(self.output_dir, f"temperature_fix_final_results_{self.timestamp}.json")
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ€§èƒ½æŠ¥å‘Š
        metrics = self.results["performance_metrics"]
        report_filename = os.path.join(self.output_dir, f"temperature_fix_report_{self.timestamp}.md")
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(f"""# Temperatureä¿®æ­£å®éªŒæŠ¥å‘Š (Run 11 Fix)

## å®éªŒä¿¡æ¯
- **è¿è¡Œæ—¶é—´**: {self.timestamp}
- **å…³é”®ä¿®æ­£**: temperature: 0 â†’ 0.3 (æ¢å¤å†å²é…ç½®)
- **æ¨¡å‹**: GPT-4.1 (Azure)
- **å¤„ç†è§†é¢‘æ•°**: {len(self.results['detailed_results'])}ä¸ª

## æ€§èƒ½ç»“æœ

### å½“å‰å®éªŒç»“æœ
- **F1åˆ†æ•°**: {metrics['f1_score']:.3f}
- **å¬å›ç‡**: {metrics['recall']:.3f}
- **ç²¾ç¡®åº¦**: {metrics['precision']:.3f}
- **å‡†ç¡®ç‡**: {metrics['accuracy']:.3f}

### ä¸å†å²ç›®æ ‡å¯¹æ¯”
- **F1åˆ†æ•°**: {metrics['f1_score']:.3f} vs 0.712 ({metrics['f1_score']-0.712:+.3f})
- **å¬å›ç‡**: {metrics['recall']:.3f} vs 0.963 ({metrics['recall']-0.963:+.3f})
- **ç²¾ç¡®åº¦**: {metrics['precision']:.3f} vs 0.565 ({metrics['precision']-0.565:+.3f})

### æ··æ·†çŸ©é˜µ
- **TP**: {metrics['tp']}, **FP**: {metrics['fp']}
- **TN**: {metrics['tn']}, **FN**: {metrics['fn']}
- **é”™è¯¯**: {metrics['errors']}

## ç»“è®º
{'âœ… Temperatureä¿®æ­£æ˜¾è‘—æ”¹å–„æ€§èƒ½' if metrics['f1_score'] > 0.4 else 'âŒ Temperatureä¿®æ­£æœªèƒ½æœ‰æ•ˆæ”¹å–„æ€§èƒ½'}

## å»ºè®®
{'ç»§ç»­æ‰©å±•åˆ°æ›´å¤šè§†é¢‘éªŒè¯' if metrics['f1_score'] > 0.5 else 'è€ƒè™‘å…¶ä»–å¤ç°æ–¹æ¡ˆæˆ–é‡‡ç”¨æ··åˆç­–ç•¥'}
""")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='GPT-4.1 Temperatureä¿®æ­£å®éªŒ')
    parser.add_argument('--limit', type=int, default=20, help='å¤„ç†è§†é¢‘æ•°é‡é™åˆ¶')
    parser.add_argument('--temperature', type=float, default=0.3, help='æ¸©åº¦å‚æ•°')
    
    args = parser.parse_args()
    
    # è¿è¡Œå®éªŒ
    experiment = GPT41TemperatureFixExperiment()
    if args.temperature != 0.3:
        experiment.temperature = args.temperature
        experiment.logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰æ¸©åº¦å‚æ•°: {args.temperature}")
    
    metrics = experiment.run_experiment(limit=args.limit)
    
    print("\n" + "="*60)
    print("ğŸ§ª TEMPERATUREä¿®æ­£å®éªŒå®Œæˆ")
    print("="*60)
    print(f"ğŸ“Š F1åˆ†æ•°: {metrics['f1_score']:.3f} (ç›®æ ‡: 0.712)")
    print(f"ğŸ“Š å¬å›ç‡: {metrics['recall']:.3f} (ç›®æ ‡: 0.963)")
    print(f"ğŸ“Š ç²¾ç¡®åº¦: {metrics['precision']:.3f} (ç›®æ ‡: 0.565)")
    print("="*60)
    
    if metrics['f1_score'] > 0.5:
        print("âœ… å®éªŒæ˜¾ç¤ºæ­£é¢æ•ˆæœï¼Œå»ºè®®æ‰©å±•éªŒè¯")
    else:
        print("âŒ ä¿®æ­£æ•ˆæœæœ‰é™ï¼Œè€ƒè™‘å…¶ä»–å¤ç°æ–¹æ¡ˆ")

if __name__ == "__main__":
    main()