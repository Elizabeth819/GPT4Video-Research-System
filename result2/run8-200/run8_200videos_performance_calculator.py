#!/usr/bin/env python3
"""
Run 8-200æ€§èƒ½æŒ‡æ ‡è®¡ç®—å™¨
è®¡ç®—200è§†é¢‘æ•°æ®é›†ä¸Šçš„è¯¦ç»†æ€§èƒ½æŒ‡æ ‡å¹¶ä¸Run 8 (100è§†é¢‘)å¯¹æ¯”
"""

import json
import pandas as pd
from collections import Counter
import numpy as np
from datetime import datetime

def calculate_performance_metrics(results_file):
    """è®¡ç®—è¯¦ç»†æ€§èƒ½æŒ‡æ ‡"""
    
    # åŠ è½½ç»“æœæ•°æ®
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    detailed_results = data['detailed_results']
    
    # ç»Ÿè®¡æ··æ·†çŸ©é˜µ
    tp = sum(1 for r in detailed_results if r['evaluation'] == 'TP')
    tn = sum(1 for r in detailed_results if r['evaluation'] == 'TN') 
    fp = sum(1 for r in detailed_results if r['evaluation'] == 'FP')
    fn = sum(1 for r in detailed_results if r['evaluation'] == 'FN')
    unknown = sum(1 for r in detailed_results if r['evaluation'] == 'UNKNOWN')
    
    total_processed = tp + tn + fp + fn + unknown
    valid_evaluations = tp + tn + fp + fn  # æ’é™¤unknown
    
    print("=" * 60)
    print("ğŸ“Š Run 8-200è§†é¢‘å®éªŒæ€§èƒ½åˆ†ææŠ¥å‘Š")
    print("=" * 60)
    print(f"ğŸ¯ å®éªŒä¿¡æ¯:")
    print(f"   - å®éªŒID: {data['experiment_info']['run_id']}")
    print(f"   - æ—¶é—´æˆ³: {data['experiment_info']['timestamp']}")
    print(f"   - æ¨¡å‹: {data['experiment_info']['model']}")
    print(f"   - Promptç‰ˆæœ¬: {data['experiment_info']['prompt_version']}")
    print(f"   - Temperature: {data['experiment_info']['temperature']}")
    print()
    
    print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
    print(f"   - ç›®æ ‡è§†é¢‘æ•°: {data['experiment_info']['video_count']}")
    print(f"   - æˆåŠŸå¤„ç†: {total_processed} ä¸ªè§†é¢‘")
    print(f"   - æœ‰æ•ˆè¯„ä¼°: {valid_evaluations} ä¸ªè§†é¢‘") 
    print(f"   - å¤„ç†æˆåŠŸç‡: {total_processed/data['experiment_info']['video_count']*100:.1f}%")
    print()
    
    print(f"ğŸ² æ··æ·†çŸ©é˜µç»Ÿè®¡:")
    print(f"   - True Positives (TP): {tp}")
    print(f"   - True Negatives (TN): {tn}")
    print(f"   - False Positives (FP): {fp}")
    print(f"   - False Negatives (FN): {fn}")
    print(f"   - Unknown/å¤±è´¥: {unknown}")
    print()
    
    if valid_evaluations > 0:
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (tp + tn) / valid_evaluations
        
        # å¹³è¡¡å‡†ç¡®ç‡
        sensitivity = recall  # æ•æ„Ÿæ€§ = å¬å›ç‡
        balanced_accuracy = (sensitivity + specificity) / 2
        
        print(f"ğŸ“Š æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:")
        print(f"   - F1 Score: {f1_score:.3f} ({f1_score*100:.1f}%)")
        print(f"   - Precision: {precision:.3f} ({precision*100:.1f}%)")
        print(f"   - Recall: {recall:.3f} ({recall*100:.1f}%)")
        print(f"   - Specificity: {specificity:.3f} ({specificity*100:.1f}%)")
        print(f"   - Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)")
        print(f"   - Balanced Accuracy: {balanced_accuracy:.3f} ({balanced_accuracy*100:.1f}%)")
        print()
        
        # æ•°æ®é›†åˆ†å¸ƒåˆ†æ
        ghost_probing_count = tp + fn  # å®é™…ghost probingæ•°é‡
        normal_count = tn + fp  # å®é™…normalæ•°é‡
        
        print(f"ğŸ“‹ æ•°æ®é›†åˆ†å¸ƒ:")
        print(f"   - Ghost Probingæ¡ˆä¾‹: {ghost_probing_count} ({ghost_probing_count/valid_evaluations*100:.1f}%)")
        print(f"   - Normalæ¡ˆä¾‹: {normal_count} ({normal_count/valid_evaluations*100:.1f}%)")
        print()
        
        # é¢„æµ‹åˆ†å¸ƒ
        predicted_positive = tp + fp
        predicted_negative = tn + fn
        
        print(f"ğŸ”® é¢„æµ‹åˆ†å¸ƒ:")
        print(f"   - é¢„æµ‹ä¸ºGhost Probing: {predicted_positive} ({predicted_positive/valid_evaluations*100:.1f}%)")
        print(f"   - é¢„æµ‹ä¸ºNormal: {predicted_negative} ({predicted_negative/valid_evaluations*100:.1f}%)")
        print()
        
        # é”™è¯¯åˆ†æ
        print(f"âŒ é”™è¯¯åˆ†æ:")
        print(f"   - è¯¯æŠ¥ç‡ (FPR): {fp/(tn+fp)*100:.1f}% - {fp}ä¸ªnormalè¢«è¯¯è¯†åˆ«ä¸ºghost probing")
        print(f"   - æ¼æŠ¥ç‡ (FNR): {fn/(tp+fn)*100:.1f}% - {fn}ä¸ªghost probingè¢«è¯¯è¯†åˆ«ä¸ºnormal")
        print()
        
        # ä¸Run 8 (100è§†é¢‘)å¯¹æ¯”æ•°æ®
        print("=" * 60)
        print("ğŸ“ˆ ä¸Run 8 (100è§†é¢‘)æ€§èƒ½å¯¹æ¯”")
        print("=" * 60)
        print("Run 8 (100è§†é¢‘) å‚è€ƒæŒ‡æ ‡:")
        print("   - F1 Score: 65.0%")
        print("   - Precision: 54.2%")
        print("   - Recall: 81.2%")
        print("   - Specificity: 67.4%") 
        print("   - Accuracy: 72.0%")
        print()
        
        # æ€§èƒ½å˜åŒ–åˆ†æ
        f1_diff = f1_score * 100 - 65.0
        precision_diff = precision * 100 - 54.2
        recall_diff = recall * 100 - 81.2
        specificity_diff = specificity * 100 - 67.4
        accuracy_diff = accuracy * 100 - 72.0
        
        print(f"ğŸ“Š Run 8-200 vs Run 8æ€§èƒ½å˜åŒ–:")
        print(f"   - F1 Score: {f1_score*100:.1f}% ({f1_diff:+.1f})")
        print(f"   - Precision: {precision*100:.1f}% ({precision_diff:+.1f})")
        print(f"   - Recall: {recall*100:.1f}% ({recall_diff:+.1f})")
        print(f"   - Specificity: {specificity*100:.1f}% ({specificity_diff:+.1f})")
        print(f"   - Accuracy: {accuracy*100:.1f}% ({accuracy_diff:+.1f})")
        print()
        
        # 95%ç½®ä¿¡åŒºé—´ä¼°è®¡
        n = valid_evaluations
        f1_se = np.sqrt(f1_score * (1 - f1_score) / n)
        f1_ci_lower = max(0, f1_score - 1.96 * f1_se)
        f1_ci_upper = min(1, f1_score + 1.96 * f1_se)
        
        precision_se = np.sqrt(precision * (1 - precision) / (tp + fp)) if (tp + fp) > 0 else 0
        precision_ci_lower = max(0, precision - 1.96 * precision_se)
        precision_ci_upper = min(1, precision + 1.96 * precision_se)
        
        recall_se = np.sqrt(recall * (1 - recall) / (tp + fn)) if (tp + fn) > 0 else 0
        recall_ci_lower = max(0, recall - 1.96 * recall_se)
        recall_ci_upper = min(1, recall + 1.96 * recall_se)
        
        print(f"ğŸ“ 95%ç½®ä¿¡åŒºé—´:")
        print(f"   - F1 Score: [{f1_ci_lower*100:.1f}%, {f1_ci_upper*100:.1f}%]")
        print(f"   - Precision: [{precision_ci_lower*100:.1f}%, {precision_ci_upper*100:.1f}%]")
        print(f"   - Recall: [{recall_ci_lower*100:.1f}%, {recall_ci_upper*100:.1f}%]")
        print()
        
        # æ•°æ®è§„æ¨¡æ•ˆåº”åˆ†æ
        print(f"ğŸ“ æ•°æ®è§„æ¨¡æ•ˆåº”åˆ†æ:")
        print(f"   - Run 8æ•°æ®è§„æ¨¡: 100 videos")
        print(f"   - Run 8-200æ•°æ®è§„æ¨¡: {valid_evaluations} videos")
        print(f"   - è§„æ¨¡å¢é•¿: {valid_evaluations/100:.1f}x")
        print(f"   - æœ‰æ•ˆå¤„ç†ç‡: {valid_evaluations/200*100:.1f}%")
        
        # ç»“è®ºå’Œå»ºè®®
        print("=" * 60)
        print("ğŸ¯ æ€»ç»“ä¸å»ºè®®")
        print("=" * 60)
        
        if f1_score >= 0.65:
            print("âœ… ä¼˜ç§€è¡¨ç°: F1-scoreä¿æŒåœ¨65%ä»¥ä¸Šï¼Œæ€§èƒ½ç¨³å®š")
        elif f1_score >= 0.60:
            print("âš ï¸  è‰¯å¥½è¡¨ç°: F1-scoreåœ¨60-65%åŒºé—´ï¼Œæœ‰æå‡ç©ºé—´")
        else:
            print("âŒ éœ€è¦æ”¹è¿›: F1-scoreä½äº60%ï¼Œå»ºè®®ä¼˜åŒ–")
            
        if precision >= 0.55:
            print("âœ… ç²¾ç¡®ç‡è‰¯å¥½: è¯¯æŠ¥æ§åˆ¶åœ¨åˆç†èŒƒå›´")
        else:
            print("âš ï¸  ç²¾ç¡®ç‡åä½: å­˜åœ¨è¾ƒå¤šè¯¯æŠ¥ï¼Œå»ºè®®ä¼˜åŒ–åˆ¤æ–­é€»è¾‘")
            
        if recall >= 0.80:
            print("âœ… å¬å›ç‡ä¼˜ç§€: å®‰å…¨å…³é”®åœºæ™¯æ£€æµ‹èƒ½åŠ›å¼º")
        elif recall >= 0.70:
            print("âš ï¸  å¬å›ç‡ä¸­ç­‰: éƒ¨åˆ†ghost probingå¯èƒ½è¢«é—æ¼")
        else:
            print("âŒ å¬å›ç‡åä½: å­˜åœ¨å®‰å…¨é£é™©ï¼Œå»ºè®®æé«˜æ£€æµ‹æ•æ„Ÿåº¦")
        
        print()
        print("ğŸ“ å…³é”®å‘ç°:")
        print(f"   1. åœ¨{valid_evaluations}ä¸ªæœ‰æ•ˆè§†é¢‘ä¸Šä¿æŒäº†{'ç¨³å®š' if abs(f1_diff) < 5 else 'æ˜¾è‘—å˜åŒ–'}çš„æ€§èƒ½")
        print(f"   2. ç›¸æ¯”100è§†é¢‘æ•°æ®é›†ï¼ŒF1-score{'æå‡' if f1_diff > 0 else 'ä¸‹é™'}äº†{abs(f1_diff):.1f}%")
        print(f"   3. {'ç²¾ç¡®ç‡' if precision_diff > recall_diff else 'å¬å›ç‡'}ç›¸å¯¹è¡¨ç°æ›´å¥½")
        print(f"   4. é€‚åˆ{'ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²' if f1_score >= 0.60 and recall >= 0.75 else 'è¿›ä¸€æ­¥ä¼˜åŒ–åéƒ¨ç½²'}")
        
        return {
            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,
            'precision': precision, 'recall': recall, 'f1_score': f1_score,
            'specificity': specificity, 'accuracy': accuracy, 
            'balanced_accuracy': balanced_accuracy,
            'total_processed': total_processed, 'valid_evaluations': valid_evaluations
        }
    
    else:
        print("âŒ æ— æœ‰æ•ˆè¯„ä¼°æ•°æ®ï¼Œæ— æ³•è®¡ç®—æ€§èƒ½æŒ‡æ ‡")
        return None

if __name__ == "__main__":
    results_file = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/run8-200/run8_200videos_final_results_20250730_134411.json"
    metrics = calculate_performance_metrics(results_file)
    
    if metrics:
        # ä¿å­˜æ€§èƒ½æŒ‡æ ‡åˆ°JSONæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        metrics_file = f"/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/run8-200/run8_200videos_performance_metrics_{timestamp}.json"
        
        metrics_data = {
            "experiment_info": {
                "experiment_id": "Run 8-200",
                "timestamp": timestamp,
                "video_dataset": "DADA-200-videos",
                "total_videos": 200,
                "processed_videos": metrics['total_processed'],
                "valid_evaluations": metrics['valid_evaluations']
            },
            "performance_metrics": {
                "f1_score": metrics['f1_score'],
                "precision": metrics['precision'], 
                "recall": metrics['recall'],
                "specificity": metrics['specificity'],
                "accuracy": metrics['accuracy'],
                "balanced_accuracy": metrics['balanced_accuracy']
            },
            "confusion_matrix": {
                "true_positives": metrics['tp'],
                "true_negatives": metrics['tn'],
                "false_positives": metrics['fp'],
                "false_negatives": metrics['fn']
            },
            "comparison_with_run8": {
                "run8_f1": 0.65,
                "run8_precision": 0.542,
                "run8_recall": 0.812,
                "run8_specificity": 0.674,
                "run8_accuracy": 0.72,
                "f1_improvement": metrics['f1_score'] - 0.65,
                "precision_improvement": metrics['precision'] - 0.542,
                "recall_improvement": metrics['recall'] - 0.812,
                "specificity_improvement": metrics['specificity'] - 0.674,
                "accuracy_improvement": metrics['accuracy'] - 0.72
            }
        }
        
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}")