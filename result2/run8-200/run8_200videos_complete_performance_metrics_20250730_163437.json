{
  "experiment_info": {
    "experiment_id": "Run 8-200 Complete",
    "timestamp": "20250730_163437",
    "video_dataset": "DADA-200-videos",
    "total_videos": 200,
    "processed_videos": 200,
    "valid_evaluations": 199
  },
  "performance_metrics": {
    "f1_score": 0.5826086956521739,
    "precision": 0.4589041095890411,
    "recall": 0.7976190476190477,
    "specificity": 0.3130434782608696,
    "accuracy": 0.5175879396984925,
    "balanced_accuracy": 0.5553312629399586
  },
  "confusion_matrix": {
    "true_positives": 67,
    "true_negatives": 36,
    "false_positives": 79,
    "false_negatives": 17,
    "unknown": 1
  },
  "dataset_distribution": {
    "ghost_probing_cases": 84,
    "normal_cases": 115,
    "ghost_probing_percentage": 42.211055276381906,
    "normal_percentage": 57.78894472361809
  },
  "comparison": {
    "vs_run8_100videos": {
      "f1_improvement": -6.739130434782609,
      "precision_improvement": -8.30958904109589,
      "recall_improvement": -1.4380952380952294,
      "specificity_improvement": -36.095652173913045,
      "accuracy_improvement": -20.24120603015075
    },
    "vs_run8_190videos": {
      "f1_improvement": -0.6391304347826079,
      "precision_improvement": -0.9095890410958845
    }
  },
  "confidence_intervals_95": {
    "f1_score": [
      0.5140930565926372,
      0.6511243347117106
    ],
    "precision": [
      0.3780731499820894,
      0.5397350691959928
    ],
    "recall": [
      0.7116981106629867,
      0.8835399845751086
    ]
  }
}