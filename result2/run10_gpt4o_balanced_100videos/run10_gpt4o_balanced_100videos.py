#!/usr/bin/env python3
"""
Run 10: GPT-4o Balanced Version (100 Videos)
åŸºäºGPT-4.1 Balancedçš„promptè®¾è®¡ï¼Œä½¿ç”¨GPT-4oæ¨¡å‹è¿›è¡Œ100è§†é¢‘å®Œæ•´æµ‹è¯•
ä¸GPT-4.1 Balancedä¿æŒå®Œå…¨ä¸€è‡´çš„promptå’Œå‚æ•°é…ç½®
"""

import cv2
import os
import json
import logging
import time
import datetime
from moviepy.editor import VideoFileClip
import pandas as pd
from dotenv import load_dotenv
import tqdm
import re
import base64
import requests
import traceback
import sys
from collections import Counter

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class GPT4oRun10BalancedExperiment:
    def __init__(self, output_dir, chunk_size=10):
        self.output_dir = output_dir
        self.chunk_size = chunk_size
        os.makedirs(self.output_dir, exist_ok=True)
        self.setup_logging()
        self.setup_openai_api()
        self.load_ground_truth()
        self.initialize_results()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.timestamp = timestamp
        log_filename = os.path.join(self.output_dir, f"run10_gpt4o_balanced_{timestamp}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("Run 10: GPT-4o Balanced Version (100 Videos) å¼€å§‹")
        
    def setup_openai_api(self):
        """è®¾ç½®OpenAI API"""
        self.openai_api_key = os.environ.get("OPENAI_API_KEY", "")
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEYæœªè®¾ç½®")
        
        # Azure OpenAIé…ç½®
        self.vision_endpoint = os.environ.get("VISION_ENDPOINT", "")
        self.vision_deployment = os.environ.get("VISION_DEPLOYMENT_NAME", "gpt-4o-global")
        
        if not self.vision_endpoint:
            raise ValueError("VISION_ENDPOINTæœªè®¾ç½®")
            
        self.logger.info(f"Azure OpenAI APIé…ç½®æˆåŠŸ")
        self.logger.info(f"Endpoint: {self.vision_endpoint}")
        self.logger.info(f"Deployment: {self.vision_deployment}")
        self.logger.info(f"Temperature: 0.3 (ä¸GPT-4.1 Balancedä¿æŒä¸€è‡´)")
        
    def load_ground_truth(self):
        """åŠ è½½ground truthæ ‡ç­¾"""
        gt_path = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos/groundtruth_labels.csv"
        self.ground_truth = pd.read_csv(gt_path, sep='\t')
        self.logger.info(f"åŠ è½½ground truthæ ‡ç­¾: {len(self.ground_truth)}ä¸ªè§†é¢‘")
        
    def initialize_results(self):
        """åˆå§‹åŒ–ç»“æœç»“æ„"""
        self.results = {
            "experiment_info": {
                "run_id": "Run 10",
                "timestamp": self.timestamp,
                "video_count": 100,
                "model": "GPT-4o (Azure)",
                "prompt_version": "GPT-4.1 Balanced (ç§»æ¤åˆ°GPT-4o)",
                "temperature": 0.3,
                "max_tokens": 2000,
                "purpose": "å¯¹æ¯”GPT-4oåœ¨Balanced promptä¸‹çš„100è§†é¢‘æ€§èƒ½",
                "output_directory": self.output_dir,
                "prompt_characteristics": [
                    "ä¸‰å±‚ghost probingåˆ†ç±»ç³»ç»Ÿ",
                    "ç¯å¢ƒä¸Šä¸‹æ–‡æ•´åˆ",
                    "å¹³è¡¡ç²¾ç¡®åº¦ä¸å¬å›ç‡",
                    "ç®€åŒ–éªŒè¯æµç¨‹",
                    "ä¸GPT-4.1 Balancedå®Œå…¨ä¸€è‡´çš„prompt",
                    "Temperature=0.3ä¿æŒå†å²ä¸€è‡´æ€§"
                ]
            },
            "detailed_results": []
        }
        
    def extract_frames_from_video(self, video_path, frame_interval=10, frames_per_interval=10):
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        try:
            clip = VideoFileClip(video_path)
            duration = clip.duration
            
            frames = []
            frames_dir = os.path.join(self.output_dir, "frames_temp")
            os.makedirs(frames_dir, exist_ok=True)
            
            # è®¡ç®—é—´éš”æ•°
            num_intervals = max(1, int(duration / frame_interval))
            
            for interval_idx in range(num_intervals):
                start_time = interval_idx * frame_interval
                end_time = min((interval_idx + 1) * frame_interval, duration)
                
                for frame_idx in range(frames_per_interval):
                    if frames_per_interval == 1:
                        frame_time = start_time + (end_time - start_time) / 2
                    else:
                        frame_time = start_time + (frame_idx / (frames_per_interval - 1)) * (end_time - start_time)
                    
                    if frame_time >= duration:
                        break
                        
                    frame = clip.get_frame(frame_time)
                    frame_filename = f"frame_{interval_idx}_{frame_idx}_{frame_time:.1f}s.jpg"
                    frame_path = os.path.join(frames_dir, frame_filename)
                    
                    cv2.imwrite(frame_path, cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
                    frames.append(frame_path)
            
            clip.close()
            return frames
        except Exception as e:
            self.logger.error(f"å¸§æå–å¤±è´¥ {video_path}: {str(e)}")
            return []
    
    def get_gpt4o_balanced_prompt(self, video_id, frame_interval=10, frames_per_interval=10):
        """è·å–ä¸GPT-4.1 Balancedå®Œå…¨ä¸€è‡´çš„promptï¼ˆç§»æ¤åˆ°GPT-4oï¼‰"""
        return f'''You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

Your job is to take in as an input a transcription of {frame_interval} seconds of audio from a video,
as well as {frames_per_interval} frames split evenly throughout {frame_interval} seconds.
You are to generate and provide a Current Action Summary of the video you are considering ({frames_per_interval}
frames over {frame_interval} seconds), which is generated from your analysis of each frame ({frames_per_interval} in total),
as well as the in-between audio, until we have a full action summary of the video.

IMPORTANT: For ghost probing detection, consider THREE categories:

**1. HIGH-CONFIDENCE Ghost Probing (use "ghost probing" in key_actions)**:
- Object appears EXTREMELY close (within 1-2 vehicle lengths, <3 meters) 
- Appearance is SUDDEN and from blind spots (behind parked cars, buildings, corners)
- Occurs in HIGH-RISK environments: highways, rural roads, parking lots, uncontrolled intersections
- Requires IMMEDIATE emergency braking/swerving to avoid collision
- Movement is COMPLETELY UNPREDICTABLE and violates traffic expectations

**2. POTENTIAL Ghost Probing (use "potential ghost probing" in key_actions)**:
- Object appears suddenly but at moderate distance (3-5 meters)
- Sudden movement in environments where some unpredictability exists
- Requires emergency braking but collision risk is moderate
- Movement is unexpected but not completely impossible given the context

**3. NORMAL Traffic Situations (do NOT use "ghost probing")**:
- Pedestrians crossing at intersections, crosswalks, or traffic lights
- Vehicles making normal lane changes, turns, or merging with signals
- Cyclists following predictable paths in urban areas or bike lanes
- Any movement that is EXPECTED given the traffic environment and context

**Environment Context Guidelines**:
- INTERSECTION/CROSSWALK: Expect pedestrians and cyclists - use "emergency braking due to pedestrian crossing"
- HIGHWAY/RURAL: Higher chance of genuine ghost probing - be more sensitive
- PARKING LOT: Expect sudden vehicle movements - use "potential ghost probing" if very sudden
- URBAN STREET: Mixed - consider visibility and predictability

Use "ghost probing" for clear cases, "potential ghost probing" for borderline cases, and descriptive terms for normal traffic situations.

Your response should be a valid JSON object with the following EXACT structure:
{{
    "video_id": "{video_id}",
    "segment_id": "full_video",
    "Start_Timestamp": "0.0s",
    "End_Timestamp": "{frame_interval}.0s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene",
    "summary": "comprehensive summary of the scene and what happens",
    "actions": "actions taken by the vehicle and driver responses",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "brief description of most important actions (use 'ghost probing', 'potential ghost probing', or descriptive terms as appropriate)",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

Audio Transcription: [No audio available for this analysis]

Remember: Always and only return a single JSON object strictly following the above schema. Use the three-tier classification system exactly as specified to achieve optimal balance between precision and recall.'''
    
    def send_azure_openai_request(self, prompt, images):
        """å‘é€Azure OpenAIè¯·æ±‚ - ä½¿ç”¨Temperature=0.3ä¿æŒä¸GPT-4.1 Balancedä¸€è‡´"""
        encoded_images = []
        for image_path in images:
            try:
                with open(image_path, 'rb') as image_file:
                    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                    encoded_images.append(encoded_string)
            except Exception as e:
                self.logger.error(f"å›¾åƒç¼–ç å¤±è´¥ {image_path}: {str(e)}")
                continue
        
        if not encoded_images:
            return None
            
        content = [{"type": "text", "text": prompt}]
        
        for encoded_image in encoded_images:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}"
                }
            })
        
        data = {
            "messages": [
                {
                    "role": "user",
                    "content": content
                }
            ],
            "max_tokens": 2000,  # ä¸GPT-4.1 Balancedä¿æŒä¸€è‡´
            "temperature": 0.3   # ä¸GPT-4.1 Balancedä¿æŒä¸€è‡´
        }
        
        headers = {
            "Content-Type": "application/json",
            "api-key": self.openai_api_key
        }
        
        try:
            response = requests.post(
                f"{self.vision_endpoint}/openai/deployments/{self.vision_deployment}/chat/completions?api-version=2024-02-01",
                headers=headers,
                json=data,
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content']
        except Exception as e:
            self.logger.error(f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
            return None
    
    def analyze_with_gpt4o(self, video_path, video_id):
        """ä½¿ç”¨GPT-4oåˆ†æè§†é¢‘ï¼ˆBalanced promptï¼‰"""
        try:
            # æå–å¸§
            frames = self.extract_frames_from_video(video_path)
            if not frames:
                return None
            
            # ç”Ÿæˆprompt
            prompt = self.get_gpt4o_balanced_prompt(video_id)
            
            # å‘é€APIè¯·æ±‚
            result = self.send_azure_openai_request(prompt, frames)
            
            # æ¸…ç†ä¸´æ—¶å¸§æ–‡ä»¶
            for frame_path in frames:
                if os.path.exists(frame_path):
                    os.remove(frame_path)
            
            return result
        except Exception as e:
            self.logger.error(f"è§†é¢‘åˆ†æå¤±è´¥ {video_id}: {str(e)}")
            return None
    
    def extract_key_actions(self, result_text):
        """æå–key_actions"""
        try:
            if result_text.startswith('```json'):
                result_text = result_text.replace('```json', '').replace('```', '').strip()
            
            result_json = json.loads(result_text)
            return result_json.get('key_actions', '').lower()
        except:
            # å°è¯•æ­£åˆ™è¡¨è¾¾å¼æå–
            key_actions_match = re.search(r'"key_actions":\s*"([^"]*)"', result_text)
            if key_actions_match:
                return key_actions_match.group(1).lower()
            return result_text.lower()
    
    def evaluate_result(self, video_id, key_actions, ground_truth_label):
        """è¯„ä¼°ç»“æœ"""
        has_ghost_probing = ("ghost probing" in key_actions) or ("potential ghost probing" in key_actions)
        ground_truth_has_ghost = ground_truth_label != "none"
        
        if has_ghost_probing and ground_truth_has_ghost:
            return "TP"
        elif has_ghost_probing and not ground_truth_has_ghost:
            return "FP"
        elif not has_ghost_probing and ground_truth_has_ghost:
            return "FN"
        else:
            return "TN"
    
    def run_experiment(self):
        """è¿è¡ŒRun 10 GPT-4o Balancedå®éªŒ"""
        # ä»ground truthæ–‡ä»¶ä¸­è·å–å®Œæ•´çš„100ä¸ªè§†é¢‘åˆ—è¡¨
        test_videos = self.ground_truth['video_id'].tolist()
        
        self.logger.info(f"å¼€å§‹Run 10å®éªŒï¼Œå¤„ç† {len(test_videos)} ä¸ªè§†é¢‘")
        self.logger.info(f"ä½¿ç”¨GPT-4.1 Balancedç›¸åŒçš„promptè®¾è®¡")
        
        start_time = time.time()
        
        for i, video_id in enumerate(tqdm.tqdm(test_videos, desc="å¤„ç†è§†é¢‘")):
            try:
                self.logger.info(f"å¤„ç†è§†é¢‘ {i+1}/100: {video_id}")
                
                # è§†é¢‘è·¯å¾„
                video_path = f"/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos/{video_id}"
                if not os.path.exists(video_path):
                    self.logger.warning(f"è§†é¢‘ä¸å­˜åœ¨: {video_path}")
                    continue
                
                # è·å–ground truth
                gt_row = self.ground_truth[self.ground_truth['video_id'] == video_id]
                if gt_row.empty:
                    self.logger.warning(f"æœªæ‰¾åˆ°ground truth: {video_id}")
                    continue
                
                ground_truth_label = gt_row.iloc[0]['ground_truth_label']
                
                # åˆ†æè§†é¢‘
                result = self.analyze_with_gpt4o(video_path, video_id)
                
                if result:
                    key_actions = self.extract_key_actions(result)
                    evaluation = self.evaluate_result(video_id, key_actions, ground_truth_label)
                else:
                    key_actions = ""
                    evaluation = "ERROR"
                
                # è®°å½•ç»“æœ
                result_entry = {
                    "video_id": video_id,
                    "ground_truth": ground_truth_label,
                    "key_actions": key_actions,
                    "evaluation": evaluation,
                    "raw_result": result
                }
                
                self.results["detailed_results"].append(result_entry)
                
                self.logger.info(f"è§†é¢‘ {video_id}: GT={ground_truth_label}, æ£€æµ‹={key_actions}, è¯„ä¼°={evaluation}")
                
                # æ¯5ä¸ªè§†é¢‘ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
                if (i + 1) % 5 == 0:
                    self.save_intermediate_results(i + 1)
                
            except Exception as e:
                self.logger.error(f"å¤„ç†è§†é¢‘å¤±è´¥ {video_id}: {str(e)}")
                # è®°å½•é”™è¯¯ç»“æœ
                error_entry = {
                    "video_id": video_id,
                    "ground_truth": ground_truth_label if 'ground_truth_label' in locals() else "unknown",
                    "key_actions": "",
                    "evaluation": "ERROR",
                    "raw_result": f"å¤„ç†é”™è¯¯: {str(e)}"
                }
                self.results["detailed_results"].append(error_entry)
                continue
        
        end_time = time.time()
        total_time = end_time - start_time
        
        self.logger.info(f"Run 10å®éªŒå®Œæˆï¼Œæ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_final_results()
        self.generate_performance_metrics()
        
    def save_intermediate_results(self, processed_count):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        intermediate_file = os.path.join(self.output_dir, f"run10_intermediate_{processed_count}videos_{self.timestamp}.json")
        with open(intermediate_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        self.logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜: {intermediate_file}")
    
    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        final_file = os.path.join(self.output_dir, f"run10_final_results_{self.timestamp}.json")
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        self.logger.info(f"æœ€ç»ˆç»“æœå·²ä¿å­˜: {final_file}")
    
    def generate_performance_metrics(self):
        """ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡å¹¶ä¸GPT-4.1 Balancedå¯¹æ¯”"""
        from collections import Counter
        
        evaluations = [r['evaluation'] for r in self.results["detailed_results"]]
        eval_counts = Counter(evaluations)
        
        tp = eval_counts.get('TP', 0)
        fp = eval_counts.get('FP', 0)
        tn = eval_counts.get('TN', 0)
        fn = eval_counts.get('FN', 0)
        errors = eval_counts.get('ERROR', 0)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        
        metrics = {
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "accuracy": accuracy,
            "tp": tp, "fp": fp, "tn": tn, "fn": fn, "errors": errors,
            "total_videos": len(self.results["detailed_results"]),
            "comparison_with_gpt41_balanced": {
                "gpt41_balanced_f1": 0.712,
                "gpt41_balanced_recall": 0.963,
                "gpt41_balanced_precision": 0.565,
                "gpt4o_balanced_f1": f1,
                "gpt4o_balanced_recall": recall,
                "gpt4o_balanced_precision": precision,
                "f1_difference": f1 - 0.712,
                "recall_difference": recall - 0.963,
                "precision_difference": precision - 0.565
            }
        }
        
        self.logger.info("=== Run 10 Performance Metrics (GPT-4o Balanced) ===")
        self.logger.info(f"ç²¾ç¡®åº¦: {precision:.3f} ({precision*100:.1f}%)")
        self.logger.info(f"å¬å›ç‡: {recall:.3f} ({recall*100:.1f}%)")
        self.logger.info(f"F1åˆ†æ•°: {f1:.3f} ({f1*100:.1f}%)")
        self.logger.info(f"å‡†ç¡®ç‡: {accuracy:.3f} ({accuracy*100:.1f}%)")
        self.logger.info(f"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}, ERROR: {errors}")
        
        self.logger.info("=== ä¸GPT-4.1 Balancedå¯¹æ¯” ===")
        self.logger.info(f"F1åˆ†æ•°å¯¹æ¯”: GPT-4o={f1:.3f} vs GPT-4.1={0.712:.3f} (å·®å¼‚: {f1-0.712:+.3f})")
        self.logger.info(f"å¬å›ç‡å¯¹æ¯”: GPT-4o={recall:.3f} vs GPT-4.1={0.963:.3f} (å·®å¼‚: {recall-0.963:+.3f})")
        self.logger.info(f"ç²¾ç¡®åº¦å¯¹æ¯”: GPT-4o={precision:.3f} vs GPT-4.1={0.565:.3f} (å·®å¼‚: {precision-0.565:+.3f})")
        
        # ä¿å­˜æŒ‡æ ‡
        metrics_file = os.path.join(self.output_dir, f"run10_metrics_{self.timestamp}.json")
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result2/run10_gpt4o_balanced_100videos"
    
    # è¿è¡Œå®éªŒ
    experiment = GPT4oRun10BalancedExperiment(output_dir)
    experiment.run_experiment()
    
    print("ğŸ¯ Run 10: GPT-4o Balanced Version (100 Videos) å®éªŒå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("ğŸ“Š è¿™å°†æä¾›GPT-4oä¸GPT-4.1åœ¨ç›¸åŒBalanced promptä¸‹çš„ç›´æ¥å¯¹æ¯”!")