#!/usr/bin/env python3
"""
åˆ›å»ºæœ€ç»ˆçš„ä¸‰æ¨¡å‹å¯¹æ¯”æŠ¥å‘Šï¼šGPT-4o vs GPT-4.1 vs Gemini
åŸºäºGround Truthæ ‡ç­¾çš„å®Œæ•´è¯„ä¼°
"""

import os
import json
import csv
from datetime import datetime
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score

def load_ground_truth():
    """åŠ è½½Ground Truthæ ‡ç­¾"""
    ground_truth_path = "result/groundtruth_labels.csv"
    ground_truth = {}
    
    with open(ground_truth_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter='\t')
        for row in reader:
            if row['video_id'] and row['video_id'].endswith('.avi'):
                video_id = row['video_id'].replace('.avi', '')
                label = row['ground_truth_label']
                
                # è§£ææ ‡ç­¾ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ghost probing
                if 'ghost probing' in label.lower():
                    ground_truth[video_id] = 1  # æ­£ä¾‹
                else:
                    ground_truth[video_id] = 0  # è´Ÿä¾‹
    
    return ground_truth

def extract_ghost_probing_prediction(result_data):
    """ä»æ¨¡å‹ç»“æœä¸­æå–ghost probingé¢„æµ‹"""
    if not isinstance(result_data, list):
        return 0
    
    # æ£€æŸ¥æ‰€æœ‰æ®µè½çš„åˆ†æç»“æœ
    for segment in result_data:
        if not isinstance(segment, dict):
            continue
            
        # æ£€æŸ¥å¤šä¸ªå­—æ®µä¸­æ˜¯å¦æåˆ°ghost probingç›¸å…³å†…å®¹
        text_fields = []
        for field in ['summary', 'actions', 'key_actions', 'next_action', 'key_objects']:
            if field in segment and segment[field]:
                text_fields.append(str(segment[field]).lower())
        
        combined_text = ' '.join(text_fields)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ghost probingç›¸å…³å…³é”®è¯
        ghost_keywords = [
            'ghost probing', 'ghost', 'probing', 
            'sudden appearance', 'unexpected', 'emerging',
            'appearing suddenly', 'cuts in', 'cut in',
            'overtaking', 'lane change', 'dangerous',
            'risky maneuver', 'unsafe', 'sudden',
            'abrupt', 'intrusion', 'interference',
            'cuts into', 'merging aggressively'
        ]
        
        for keyword in ghost_keywords:
            if keyword in combined_text:
                return 1  # é¢„æµ‹ä¸ºæ­£ä¾‹
    
    return 0  # é¢„æµ‹ä¸ºè´Ÿä¾‹

def evaluate_model(model_name, result_dir, ground_truth):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    if not os.path.exists(result_dir):
        print(f"âš ï¸ æ¨¡å‹ç»“æœç›®å½•ä¸å­˜åœ¨: {result_dir}")
        return None
    
    # è·å–å¤„ç†çš„è§†é¢‘æ–‡ä»¶
    result_files = [f for f in os.listdir(result_dir) if f.endswith('.json')]
    processed_videos = []
    
    for filename in result_files:
        video_id = filename.replace('actionSummary_', '').replace('.json', '')
        if video_id in ground_truth:
            processed_videos.append(video_id)
    
    print(f"   {model_name}: æ‰¾åˆ° {len(processed_videos)} ä¸ªGround Truthè§†é¢‘")
    
    if len(processed_videos) == 0:
        print(f"   âš ï¸ {model_name}æ²¡æœ‰å¤„ç†Ground Truthè§†é¢‘")
        return None
    
    # è¯„ä¼°é¢„æµ‹ç»“æœ
    predictions = {}
    for video_id in processed_videos:
        result_file = os.path.join(result_dir, f"actionSummary_{video_id}.json")
        
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                result_data = json.load(f)
                predictions[video_id] = extract_ghost_probing_prediction(result_data)
        except Exception as e:
            print(f"   âš ï¸ åŠ è½½{video_id}å¤±è´¥: {e}")
            predictions[video_id] = 0
    
    # è®¡ç®—æŒ‡æ ‡
    y_true = [ground_truth[video_id] for video_id in processed_videos]
    y_pred = [predictions[video_id] for video_id in processed_videos]
    
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    # ç‰¹å¼‚æ€§å’Œå¹³è¡¡å‡†ç¡®ç‡
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    balanced_accuracy = (recall + specificity) / 2
    
    return {
        'model': model_name,
        'sample_size': len(processed_videos),
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'specificity': specificity,
        'balanced_accuracy': balanced_accuracy,
        'true_positives': int(tp),
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
        'processed_videos': processed_videos
    }

def create_final_comparison_report():
    """åˆ›å»ºæœ€ç»ˆçš„ä¸‰æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š"""
    print("ğŸ“Š åˆ›å»ºæœ€ç»ˆä¸‰æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š")
    print("=" * 60)
    
    # åŠ è½½Ground Truth
    ground_truth = load_ground_truth()
    print(f"ğŸ“ åŠ è½½Ground Truth: {len(ground_truth)} ä¸ªè§†é¢‘")
    
    # æ¨¡å‹é…ç½®
    models = {
        'GPT-4o': 'result/gpt4o-100-3rd',
        'GPT-4.1': 'result/gpt41-format-test',  # ä½¿ç”¨æ ¼å¼æµ‹è¯•ç»“æœ
        'Gemini': 'result/gemini-1.5-flash'
    }
    
    # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
    model_results = {}
    for model_name, result_dir in models.items():
        print(f"\nğŸ” è¯„ä¼° {model_name}...")
        metrics = evaluate_model(model_name, result_dir, ground_truth)
        if metrics:
            model_results[model_name.lower().replace('-', '').replace('.', '')] = metrics
            print(f"   âœ… {model_name} è¯„ä¼°å®Œæˆ")
        else:
            print(f"   âŒ {model_name} è¯„ä¼°å¤±è´¥")
    
    if not model_results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹è¯„ä¼°ç»“æœ")
        return None
    
    # åˆ›å»ºæŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    report = {
        "analysis_timestamp": datetime.now().isoformat(),
        "description": "Final comparison of GPT-4o, GPT-4.1, and Gemini models for ghost probing detection",
        "ground_truth_info": {
            "total_gt_samples": len(ground_truth),
            "positive_samples": sum(ground_truth.values()),
            "negative_samples": len(ground_truth) - sum(ground_truth.values())
        },
        "model_results": model_results,
        "evaluation_notes": {
            "gpt4o": "Complete evaluation on Ground Truth dataset",
            "gpt41": "Limited evaluation due to API content filtering issues",
            "gemini": "Complete evaluation on Ground Truth dataset"
        }
    }
    
    # ç¡®ä¿æ¯”è¾ƒç›®å½•å­˜åœ¨
    os.makedirs("result/comparison", exist_ok=True)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"result/comparison/final_three_models_comparison_{timestamp}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
    
    return report

def print_comparison_summary(report):
    """æ‰“å°å¯¹æ¯”æ€»ç»“"""
    print("\n" + "="*100)
    print("ğŸ† GPT-4o vs GPT-4.1 vs Gemini æœ€ç»ˆå¯¹æ¯”ç»“æœ")
    print("="*100)
    
    model_results = report["model_results"]
    
    print(f"ğŸ“ Ground Truthä¿¡æ¯:")
    print(f"   æ€»æ ·æœ¬æ•°: {report['ground_truth_info']['total_gt_samples']}")
    print(f"   æ­£ä¾‹æ ·æœ¬: {report['ground_truth_info']['positive_samples']}")
    print(f"   è´Ÿä¾‹æ ·æœ¬: {report['ground_truth_info']['negative_samples']}")
    
    print(f"\nğŸ“Š æ¨¡å‹æ ·æœ¬è¦†ç›–:")
    for model_key, metrics in model_results.items():
        print(f"   {metrics['model']}: {metrics['sample_size']} ä¸ªè§†é¢‘")
    
    print(f"\nğŸ¯ æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")
    print(f"{'æŒ‡æ ‡':<20} ", end="")
    for model_key, metrics in model_results.items():
        print(f"{metrics['model']:<15} ", end="")
    print()
    print("-" * (20 + 15 * len(model_results)))
    
    metrics_to_show = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'balanced_accuracy']
    for metric in metrics_to_show:
        print(f"{metric:<20} ", end="")
        for model_key, metrics in model_results.items():
            value = metrics[metric]
            print(f"{value:<15.4f} ", end="")
        print()
    
    print(f"\nğŸ“ˆ æ··æ·†çŸ©é˜µå¯¹æ¯”:")
    for model_key, metrics in model_results.items():
        tp, tn, fp, fn = metrics['true_positives'], metrics['true_negatives'], metrics['false_positives'], metrics['false_negatives']
        print(f"   {metrics['model']}: TP={tp}, TN={tn}, FP={fp}, FN={fn}")
    
    print(f"\nğŸ” æ¨¡å‹ç‰¹å¾åˆ†æ:")
    for model_key, metrics in model_results.items():
        model_name = metrics['model']
        precision = metrics['precision']
        recall = metrics['recall']
        f1 = metrics['f1_score']
        
        if precision > 0.7:
            precision_desc = "é«˜ç²¾ç¡®åº¦"
        elif precision > 0.5:
            precision_desc = "ä¸­ç­‰ç²¾ç¡®åº¦"
        else:
            precision_desc = "ä½ç²¾ç¡®åº¦"
            
        if recall > 0.7:
            recall_desc = "é«˜å¬å›ç‡"
        elif recall > 0.5:
            recall_desc = "ä¸­ç­‰å¬å›ç‡"
        else:
            recall_desc = "ä½å¬å›ç‡"
        
        print(f"   {model_name}: {precision_desc}, {recall_desc}, F1={f1:.4f}")
    
    print(f"\nğŸ’¡ å…³é”®å‘ç°:")
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_f1_model = max(model_results.items(), key=lambda x: x[1]['f1_score'])
    best_precision_model = max(model_results.items(), key=lambda x: x[1]['precision'])
    best_recall_model = max(model_results.items(), key=lambda x: x[1]['recall'])
    
    print(f"   ğŸ¥‡ æœ€ä½³F1åˆ†æ•°: {best_f1_model[1]['model']} ({best_f1_model[1]['f1_score']:.4f})")
    print(f"   ğŸ¯ æœ€ä½³ç²¾ç¡®åº¦: {best_precision_model[1]['model']} ({best_precision_model[1]['precision']:.4f})")
    print(f"   ğŸ” æœ€ä½³å¬å›ç‡: {best_recall_model[1]['model']} ({best_recall_model[1]['recall']:.4f})")
    
    print(f"\nâš ï¸ é‡è¦è¯´æ˜:")
    print("   GPT-4.1çš„è¯„ä¼°åŸºäºæœ‰é™æ ·æœ¬ï¼Œå®Œæ•´è¯„ä¼°éœ€è¦è§£å†³APIå†…å®¹è¿‡æ»¤é—®é¢˜")
    print("   GPT-4oå€¾å‘äºé«˜å¬å›ç‡ï¼Œé€‚åˆå®‰å…¨å…³é”®åº”ç”¨")
    print("   Geminiæä¾›æ›´å¹³è¡¡çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡")
    
    print("="*100)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æœ€ç»ˆä¸‰æ¨¡å‹å¯¹æ¯”è¯„ä¼°")
    print("=" * 60)
    
    # åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
    report = create_final_comparison_report()
    
    if report:
        # æ‰“å°æ€»ç»“
        print_comparison_summary(report)
        
        print(f"\nğŸ‰ æœ€ç»ˆå¯¹æ¯”è¯„ä¼°å®Œæˆï¼")
        print("   è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° result/comparison/ ç›®å½•")
    else:
        print("âŒ æœ€ç»ˆå¯¹æ¯”è¯„ä¼°å¤±è´¥")

if __name__ == "__main__":
    main()