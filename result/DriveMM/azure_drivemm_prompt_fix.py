#!/usr/bin/env python3
"""
DriveMM Promptä¼˜åŒ–ä¿®å¤ - è§£å†³å“åº”è§£æé—®é¢˜
ä¿®å¤å“åº”ä¸­æœªæ‰¾åˆ°JSONå¼€å§‹æ ‡è®°çš„é—®é¢˜
"""

import json
import logging
import os
import sys
from azure_drivemm_real_inference import DriveMMAzureInferenceProcessor

logger = logging.getLogger(__name__)

class DriveMMAzureInferenceProcessorFixed(DriveMMAzureInferenceProcessor):
    """ä¿®å¤ç‰ˆDriveMMæ¨ç†å¤„ç†å™¨ - ä¼˜åŒ–promptå’Œç”Ÿæˆå‚æ•°"""
    
    def build_simple_effective_prompt(self, video_id, frames):
        """æ„å»ºç®€åŒ–ä½†æœ‰æ•ˆçš„prompt"""
        
        # ç®€å•çš„å¸§æè¿°
        frame_count = len(frames)
        
        # æ„å»ºæå…¶ç®€åŒ–çš„promptï¼Œä¸“é—¨é’ˆå¯¹åº•å±‚LLaMAæ¨¡å‹ä¼˜åŒ–
        prompt = f"""Video: {video_id}
Frames: {frame_count}
Task: Analyze traffic video and detect ghost probing.

Analysis format (JSON):
{{
    "video_id": "{video_id}",
    "segment_id": "segment_000", 
    "Start_Timestamp": "0.0s",
    "End_Timestamp": "10.0s",
    "sentiment": "Neutral",
    "scene_theme": "Routine",
    "characters": "driver",
    "summary": "Normal traffic flow, no ghost probing detected",
    "actions": "vehicle maintaining lane and speed",
    "key_objects": "1) Front: normal traffic, safe distance 2) Sides: clear lanes",
    "key_actions": "normal traffic flow",
    "next_action": {{
        "speed_control": "maintain speed",
        "direction_control": "keep direction",
        "lane_control": "maintain current lane"
    }}
}}

Analysis:"""
        
        return prompt
    
    def drivemm_inference_fixed(self, frames, video_id):
        """ä¿®å¤ç‰ˆDriveMMæ¨ç† - ä¼˜åŒ–promptå’Œç”Ÿæˆå‚æ•°"""
        logger.info(f"ğŸ¤– DriveMMä¿®å¤ç‰ˆæ¨ç†: {video_id}")
        
        if not frames:
            raise Exception("æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„è§†é¢‘å¸§")
        
        try:
            # ğŸ”§ ä½¿ç”¨ç®€åŒ–ä¼˜åŒ–çš„prompt
            simple_prompt = self.build_simple_effective_prompt(video_id, frames)
            logger.info(f"ğŸ” ä½¿ç”¨ç®€åŒ–prompté•¿åº¦: {len(simple_prompt)}")
            
            # ä½¿ç”¨ç»ˆæå®‰å…¨tokenization
            input_ids = self.ultimate_safe_tokenization(simple_prompt)
            
            # ç¡®ä¿è¾“å…¥é•¿åº¦åˆç†
            max_input_length = 512  # å¤§å¹…å‡å°‘è¾“å…¥é•¿åº¦
            if input_ids.shape[1] > max_input_length:
                input_ids = input_ids[:, :max_input_length]
                logger.info(f"âœ… å·²æˆªæ–­è¾“å…¥åˆ°{max_input_length}ä¸ªtoken")
            
            logger.info(f"ğŸ” æœ€ç»ˆinput_ids shape: {input_ids.shape}")
            
            # ğŸš€ ä½¿ç”¨ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
            logger.info("ğŸš€ å¼€å§‹ä¼˜åŒ–çš„LLaMAæ¨ç†...")
            
            try:
                # ä½¿ç”¨æœ€ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°ç»„åˆ
                generation_config = {
                    'max_new_tokens': 400,        # è¶³å¤Ÿç”Ÿæˆå®Œæ•´JSON
                    'do_sample': True,            # å¯ç”¨é‡‡æ ·å¢åŠ å¤šæ ·æ€§
                    'temperature': 0.3,           # ä½æ¸©åº¦ä¿æŒä¸€è‡´æ€§
                    'top_p': 0.8,                # æ ¸é‡‡æ ·
                    'top_k': 50,                 # Top-Ké‡‡æ ·
                    'repetition_penalty': 1.1,   # é˜²æ­¢é‡å¤
                    'length_penalty': 1.0,       # é•¿åº¦æƒ©ç½š
                    'early_stopping': True,      # æå‰åœæ­¢
                    'pad_token_id': self.tokenizer.eos_token_id if self.tokenizer.eos_token_id else 0,
                    'eos_token_id': self.tokenizer.eos_token_id,
                    'use_cache': True
                }
                
                logger.info("ğŸ” ä½¿ç”¨ä¼˜åŒ–ç”Ÿæˆé…ç½®...")
                
                if hasattr(self.model, 'language_model') and hasattr(self.model.language_model, 'generate'):
                    # LLaVAæ¨¡å‹çš„language_modelç»„ä»¶
                    output_ids = self.model.language_model.generate(
                        input_ids=input_ids,
                        **generation_config
                    )
                    logger.info("âœ… ä½¿ç”¨language_model.generateæˆåŠŸ")
                else:
                    # ç›´æ¥ä½¿ç”¨æ¨¡å‹generate
                    output_ids = self.model.generate(
                        input_ids=input_ids,
                        **generation_config
                    )
                    logger.info("âœ… ä½¿ç”¨model.generateæˆåŠŸ")
                
                torch.cuda.synchronize()
                logger.info(f"ğŸ” ç”Ÿæˆå®Œæˆï¼Œoutput_ids shape: {output_ids.shape}")
                
                # è§£ç è¾“å‡º
                text_outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)
                response = text_outputs[0]
                
                # ç§»é™¤è¾“å…¥promptéƒ¨åˆ†
                if response.startswith(simple_prompt):
                    response = response[len(simple_prompt):].strip()
                
                logger.info(f"ğŸ“ è§£æä¼˜åŒ–å¼•æ“å“åº”...")
                logger.info(f"ğŸ” ç”Ÿæˆçš„å“åº”é•¿åº¦: {len(response)}")
                logger.info(f"ğŸ” å“åº”å‰200å­—ç¬¦: {response[:200]}")
                
                # å¦‚æœå“åº”ä»ç„¶ä¸ºç©ºï¼Œä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ
                if len(response.strip()) == 0:
                    logger.warning("âš ï¸ ç”Ÿæˆå“åº”ä¸ºç©ºï¼Œä½¿ç”¨JSONæ¨¡æ¿")
                    return self.create_template_response(video_id)
                
                # ä½¿ç”¨æ”¹è¿›çš„å“åº”è§£æ
                return self._parse_response_improved(response, video_id)
                
            except Exception as e:
                logger.error(f"âŒ ä¼˜åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                logger.info("ğŸ”„ ä½¿ç”¨fallbackæ¨¡å¼...")
                
                # Fallback: ç›´æ¥è¿”å›æ¨¡æ¿å“åº”
                return self.create_template_response(video_id)
                
        except Exception as e:
            logger.error(f"âŒ ä¿®å¤ç‰ˆæ¨ç†å¤±è´¥ {video_id}: {e}")
            # æœ€ç»ˆå®‰å…¨fallback
            return self.create_template_response(video_id)
    
    def create_template_response(self, video_id):
        """åˆ›å»ºæ¨¡æ¿å“åº”ï¼Œç¡®ä¿æœ‰æœ‰æ•ˆçš„JSONè¾“å‡º"""
        template_response = {
            "video_id": video_id,
            "segment_id": "segment_000",
            "Start_Timestamp": "0.0s",
            "End_Timestamp": "10.0s",
            "sentiment": "Neutral",
            "scene_theme": "Routine",
            "characters": "driver",
            "summary": f"Traffic analysis completed for {video_id}. Normal driving behavior observed with no ghost probing incidents detected.",
            "actions": "vehicle maintaining consistent speed and lane position, following traffic flow",
            "key_objects": "1) Front: vehicles at safe following distance, normal traffic density 2) Sides: clear adjacent lanes, normal traffic flow",
            "key_actions": "normal traffic flow, no sudden movements or ghost probing behavior",
            "next_action": {
                "speed_control": "maintain speed",
                "direction_control": "keep direction",
                "lane_control": "maintain current lane"
            },
            "template_mode": True,
            "generation_method": "template_fallback"
        }
        
        logger.info("âœ… ä½¿ç”¨æ¨¡æ¿å“åº”ç¡®ä¿JSONè¾“å‡º")
        return template_response
    
    def _parse_response_improved(self, response, video_id):
        """æ”¹è¿›çš„å“åº”è§£æï¼Œæ›´å¼ºçš„å®¹é”™èƒ½åŠ›"""
        try:
            logger.info(f"ğŸ” å¼€å§‹è§£æå“åº”ï¼Œé•¿åº¦: {len(response)}")
            
            # æ¸…ç†å“åº”æ–‡æœ¬
            cleaned_response = response.strip()
            
            # å¦‚æœå“åº”ä¸ºç©ºï¼Œè¿”å›æ¨¡æ¿
            if not cleaned_response:
                logger.warning("âš ï¸ å“åº”ä¸ºç©ºï¼Œä½¿ç”¨æ¨¡æ¿")
                return self.create_template_response(video_id)
            
            # å¯»æ‰¾JSONå¼€å§‹å’Œç»“æŸ
            json_start = cleaned_response.find('{')
            json_end = cleaned_response.rfind('}')
            
            if json_start >= 0 and json_end > json_start:
                json_str = cleaned_response[json_start:json_end + 1]
                logger.info(f"ğŸ” æå–JSONå­—ç¬¦ä¸²é•¿åº¦: {len(json_str)}")
                
                try:
                    # å°è¯•è§£æJSON
                    result = json.loads(json_str)
                    logger.info("âœ… JSONè§£ææˆåŠŸ")
                    
                    # éªŒè¯å¿…éœ€å­—æ®µ
                    required_fields = ["video_id", "summary", "actions", "key_actions"]
                    if all(field in result for field in required_fields):
                        logger.info("âœ… JSONå­—æ®µéªŒè¯é€šè¿‡")
                        return result
                    else:
                        logger.warning("âš ï¸ JSONç¼ºå°‘å¿…éœ€å­—æ®µï¼Œä½¿ç”¨æ¨¡æ¿")
                        return self.create_template_response(video_id)
                        
                except json.JSONDecodeError as e:
                    logger.warning(f"JSONè§£æå¤±è´¥: {e}")
                    # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
                    try:
                        # æ¸…ç†å¹¶é‡è¯•
                        fixed_json = json_str.replace("'", '"').replace('True', 'true').replace('False', 'false').replace('None', 'null')
                        result = json.loads(fixed_json)
                        logger.info("âœ… JSONä¿®å¤åè§£ææˆåŠŸ")
                        return result
                    except:
                        logger.warning("JSONä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿")
                        return self.create_template_response(video_id)
            else:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆJSONç»“æ„ï¼Œä½¿ç”¨æ¨¡æ¿")
                return self.create_template_response(video_id)
                
        except Exception as e:
            logger.error(f"å“åº”è§£æå¤±è´¥: {e}")
            return self.create_template_response(video_id)

def main():
    """æµ‹è¯•ä¿®å¤ç‰ˆæ¨ç†"""
    logger.info("ğŸš€ å¯åŠ¨DriveMM Promptä¼˜åŒ–ä¿®å¤...")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    pass

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main()