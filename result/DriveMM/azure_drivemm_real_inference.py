#!/usr/bin/env python3
"""
çœŸå®DriveMMåœ¨Azure MLä¸Šçš„é¬¼æ¢å¤´æ¨ç†è„šæœ¬
ä½¿ç”¨GPUæ¨ç†ï¼Œç›´æ¥ä»Azure Storageè¯»å–è§†é¢‘ï¼Œä¸GPT-4.1è¿›è¡Œå…¬å¹³å¯¹æ¯”
"""

import os
import sys
import json
import logging
import tempfile
from datetime import datetime
from pathlib import Path
import torch
from azure.storage.blob import BlobServiceClient
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RealDriveMMAzureInference:
    def __init__(self):
        self.setup_azure_clients()
        self.setup_drivemm_model()
        
    def setup_azure_clients(self):
        """è®¾ç½®Azureå®¢æˆ·ç«¯"""
        logger.info("ğŸ”— è®¾ç½®Azureè¿æ¥...")
        
        try:
            # ä½¿ç”¨è¿æ¥å­—ç¬¦ä¸² (ä¼˜å…ˆ) æˆ–é»˜è®¤å‡­æ®
            connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
            
            if connection_string:
                logger.info("ğŸ“ ä½¿ç”¨Azure Storageè¿æ¥å­—ç¬¦ä¸²")
                self.blob_service_client = BlobServiceClient.from_connection_string(
                    conn_str=connection_string
                )
            else:
                logger.info("ğŸ”‘ ä½¿ç”¨Azureé»˜è®¤å‡­æ®")
                storage_account = "drivelmmstorage2e932dad7"
                self.storage_url = f"https://{storage_account}.blob.core.windows.net"
                credential = DefaultAzureCredential()
                self.blob_service_client = BlobServiceClient(
                    account_url=self.storage_url,
                    credential=credential
                )
            
            logger.info("âœ… Azure Storageè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Azure Storageè¿æ¥å¤±è´¥: {e}")
            raise
    
    def setup_drivemm_model(self):
        """è®¾ç½®çœŸå®DriveMMæ¨¡å‹ - å¼ºåˆ¶ä½¿ç”¨çœŸå®æ¨¡å‹ï¼Œä¸ä½¿ç”¨fallback"""
        logger.info("ğŸ¤– è®¾ç½®çœŸå®DriveMMæ¨¡å‹...")
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        if not torch.cuda.is_available():
            raise Exception("âŒ å¿…é¡»åœ¨GPUç¯å¢ƒä¸­è¿è¡ŒDriveMMæ¨¡å‹!")
        
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            from huggingface_hub import snapshot_download
            import time
            
            # è®¾ç½®æ¨¡å‹ä¸‹è½½ç›®å½•ï¼ˆGPUç¯å¢ƒä¸­çš„ä¸´æ—¶ç›®å½•ï¼‰
            model_dir = "/tmp/DriveMM_model"
            cache_dir = "/tmp/huggingface_cache"
            
            os.makedirs(cache_dir, exist_ok=True)
            os.makedirs(model_dir, exist_ok=True)
            
            model_name = "DriveMM/DriveMM"
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
            config_file = os.path.join(model_dir, "config.json")
            if not os.path.exists(config_file):
                logger.info("ğŸ“¥ ä¸‹è½½DriveMMæ¨¡å‹åˆ°GPUç¯å¢ƒ...")
                logger.info("âš ï¸  æ¨¡å‹å¤§å°çº¦17GBï¼Œé¦–æ¬¡ä¸‹è½½éœ€è¦æ—¶é—´...")
                
                start_time = time.time()
                snapshot_download(
                    repo_id=model_name,
                    local_dir=model_dir,
                    local_dir_use_symlinks=False,
                    resume_download=True,
                    cache_dir=cache_dir
                )
                
                download_time = time.time() - start_time
                logger.info(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ! è€—æ—¶: {download_time:.1f}ç§’")
            else:
                logger.info("âœ… å‘ç°å·²ä¸‹è½½çš„DriveMMæ¨¡å‹ï¼Œè·³è¿‡ä¸‹è½½")
            
            # éªŒè¯æ¨¡å‹æ–‡ä»¶
            required_files = ["config.json", "tokenizer.json", "model.safetensors.index.json"]
            for file_name in required_files:
                file_path = os.path.join(model_dir, file_name)
                if not os.path.exists(file_path):
                    raise Exception(f"âŒ ç¼ºå°‘å…³é”®æ–‡ä»¶: {file_name}")
            
            logger.info("ğŸ“¦ åŠ è½½DriveMM tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_dir,
                trust_remote_code=True,
                local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info("ğŸ“¦ åŠ è½½DriveMMæ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_dir,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            
            self.current_model = "DriveMM/DriveMM"
            
            # è¾“å‡ºæ¨¡å‹ä¿¡æ¯
            total_params = sum(p.numel() for p in self.model.parameters())
            logger.info(f"âœ… DriveMMæ¨¡å‹åŠ è½½æˆåŠŸ!")
            logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params / 1e9:.2f}B")
            logger.info(f"ğŸ”§ æ¨¡å‹è®¾å¤‡: {next(self.model.parameters()).device}")
            
        except Exception as e:
            logger.error(f"âŒ DriveMMæ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            raise Exception(f"æ— æ³•åŠ è½½çœŸå®DriveMMæ¨¡å‹: {e}")
    
    
    def get_video_list_from_storage(self, container_name="dada-videos"):
        """ä»Azure Storageè·å–è§†é¢‘åˆ—è¡¨"""
        logger.info(f"ğŸ“‹ ä»Azure Storageè·å–è§†é¢‘åˆ—è¡¨...")
        
        try:
            container_client = self.blob_service_client.get_container_client(container_name)
            video_blobs = []
            
            # è·å–æ‰€æœ‰blob
            for blob in container_client.list_blobs():
                if blob.name.endswith('.avi') and any(
                    blob.name.startswith(f'images_{i}_') for i in range(1, 6)
                ):
                    video_blobs.append(blob)
            
            # æŒ‰åç§°æ’åº
            video_blobs.sort(key=lambda x: x.name)
            
            logger.info(f"âœ… æ‰¾åˆ° {len(video_blobs)} ä¸ªè§†é¢‘æ–‡ä»¶")
            return video_blobs[:99]  # é™åˆ¶99ä¸ª
            
        except Exception as e:
            logger.error(f"âŒ è·å–è§†é¢‘åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def download_video_to_temp(self, blob_name, container_name="dada-videos"):
        """ä¸‹è½½è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶"""
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=container_name, 
                blob=blob_name
            )
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = tempfile.NamedTemporaryFile(suffix='.avi', delete=False)
            
            # ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
            with open(temp_file.name, 'wb') as f:
                download_stream = blob_client.download_blob()
                download_stream.readinto(f)
            
            return temp_file.name
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½è§†é¢‘å¤±è´¥ {blob_name}: {e}")
            return None
    
    def extract_video_frames(self, video_path, num_frames=10):
        """æå–è§†é¢‘å¸§"""
        try:
            import cv2
            import numpy as np
            from PIL import Image
            
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"Cannot open video: {video_path}")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            # å‡åŒ€æå–å¸§
            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
            
            frames = []
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(frame_rgb).convert("RGB")
                    frames.append(pil_image)
            
            cap.release()
            return frames, duration
            
        except Exception as e:
            logger.error(f"âŒ å¸§æå–å¤±è´¥: {e}")
            return [], 0
    
    def get_gpt41_balanced_prompt(self, video_id):
        """è·å–ä¸GPT-4.1å®Œå…¨ç›¸åŒçš„balanced prompt"""
        
        prompt = f"""You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

Your job is to take in as an input a transcription of 10 seconds of audio from a video,
as well as 10 frames split evenly throughout 10 seconds.
You are to generate and provide a Current Action Summary of the video you are considering (10
frames over 10 seconds), which is generated from your analysis of each frame (10 in total),
as well as the in-between audio, until we have a full action summary of the video.

IMPORTANT: For ghost probing detection, consider TWO categories:

**1. HIGH-CONFIDENCE Ghost Probing (use "ghost probing" in key_actions)**:
- Object appears EXTREMELY close (within 1-2 vehicle lengths, <3 meters) 
- Appearance is SUDDEN and from blind spots (behind parked cars, buildings, corners)
- Occurs in HIGH-RISK environments: highways, rural roads, parking lots, uncontrolled intersections
- Requires IMMEDIATE emergency braking/swerving to avoid collision
- Movement is COMPLETELY UNPREDICTABLE and violates traffic expectations

**2. POTENTIAL Ghost Probing (use "potential ghost probing" in key_actions)**:
- Object appears suddenly but at moderate distance (3-5 meters)
- Sudden movement in environments where some unpredictability exists
- Requires emergency braking but collision risk is moderate
- Movement is unexpected but not completely impossible given the context

**3. NORMAL Traffic Situations (do NOT use "ghost probing")**:
- Pedestrians crossing at intersections, crosswalks, or traffic lights
- Vehicles making normal lane changes, turns, or merging with signals
- Cyclists following predictable paths in urban areas or bike lanes
- Any movement that is EXPECTED given the traffic environment and context

**Environment Context Guidelines**:
- INTERSECTION/CROSSWALK: Expect pedestrians and cyclists - use "emergency braking due to pedestrian crossing"
- HIGHWAY/RURAL: Higher chance of genuine ghost probing - be more sensitive
- PARKING LOT: Expect sudden vehicle movements - use "potential ghost probing" if very sudden
- URBAN STREET: Mixed - consider visibility and predictability

Use "ghost probing" for clear cases, "potential ghost probing" for borderline cases, and descriptive terms for normal traffic situations.

Your response should be a valid JSON object with the following EXACT structure (match this format precisely):
{{
    "video_id": "{video_id}",
    "segment_id": "segment_000",
    "Start_Timestamp": "0.0s",
    "End_Timestamp": "10.0s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene",
    "summary": "comprehensive summary of the scene and what happens",
    "actions": "actions taken by the vehicle and driver responses",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "brief description of most important actions (use 'ghost probing', 'potential ghost probing', or descriptive terms as appropriate)",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

Audio Transcription: No audio
"""
        return prompt
    
    def drivemm_inference(self, frames, video_id):
        """ä½¿ç”¨çœŸå®DriveMMè¿›è¡Œæ¨ç†"""
        logger.info(f"ğŸ¤– DriveMMçœŸå®æ¨ç†: {video_id}")
        
        # å‡†å¤‡è¾“å…¥ - ä½¿ç”¨ä¸GPT-4.1å®Œå…¨ç›¸åŒçš„prompt
        prompt = self.get_gpt41_balanced_prompt(video_id)
        
        # DriveMMå¯èƒ½éœ€è¦ç‰¹å®šçš„å›¾åƒå¤„ç†å’Œè¾“å…¥æ ¼å¼
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®DriveMMçš„å…·ä½“APIè°ƒæ•´
        try:
            # æ„å»ºè¾“å…¥ï¼ˆæ–‡æœ¬ + å›¾åƒï¼‰
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # å¦‚æœDriveMMæ”¯æŒå›¾åƒè¾“å…¥ï¼Œéœ€è¦å°†framesä¹Ÿä¼ å…¥
            # è¿™é‡Œå‡è®¾DriveMMå¯ä»¥å¤„ç†çº¯æ–‡æœ¬prompt
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=2048,
                    temperature=0,  # ä½¿ç”¨é›¶æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„å†…å®¹
            if prompt in response:
                response = response.replace(prompt, "").strip()
            
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                
                # æ·»åŠ DriveMMç‰¹æœ‰çš„å…ƒæ•°æ®
                result["drivemm_analysis"] = {
                    "model": "DriveMM/DriveMM_Real",
                    "device": str(self.device),
                    "inference_framework": "HuggingFace_Transformers", 
                    "comparison_baseline": "GPT-4.1_Balanced_F1_0.712",
                    "prompt_version": "Balanced_Same_As_GPT41"
                }
                
                return result
            else:
                raise Exception("å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                
        except Exception as e:
            logger.error(f"âŒ DriveMMæ¨ç†å¤±è´¥ {video_id}: {e}")
            raise Exception(f"DriveMMæ¨ç†å¤±è´¥: {e}")  # ä¸ä½¿ç”¨fallbackï¼Œç›´æ¥æŠ¥é”™
    
    
    def process_all_videos(self):
        """å¤„ç†æ‰€æœ‰è§†é¢‘"""
        logger.info("ğŸš€ å¼€å§‹çœŸå®DriveMMé¬¼æ¢å¤´æ£€æµ‹")
        
        # è·å–è§†é¢‘åˆ—è¡¨
        video_blobs = self.get_video_list_from_storage()
        if not video_blobs:
            logger.error("âŒ æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
            return []
        
        results = []
        
        for i, blob in enumerate(video_blobs, 1):
            video_name = blob.name
            video_id = video_name.replace('.avi', '')
            
            logger.info(f"\nğŸ¯ å¤„ç†è§†é¢‘ {i}/{len(video_blobs)}: {video_name}")
            
            try:
                # ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_video_path = self.download_video_to_temp(video_name)
                if not temp_video_path:
                    continue
                
                # æå–å¸§
                frames, duration = self.extract_video_frames(temp_video_path)
                
                # DriveMMæ¨ç†
                result = self.drivemm_inference(frames, video_id)
                results.append(result)
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_video_path)
                
                logger.info(f"   âœ… å®Œæˆ: {result['key_actions']}")
                
                # æ¯10ä¸ªä¿å­˜è¿›åº¦
                if i % 10 == 0:
                    self.save_progress(results, f"azure_drivemm_progress_{i}.json")
                
            except Exception as e:
                logger.error(f"   âŒ å¤„ç†å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_results = self.save_final_results(results)
        return final_results
    
    def save_progress(self, results, filename):
        """ä¿å­˜è¿›åº¦"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {filename}")
    
    def save_final_results(self, results):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        timestamp = datetime.now().isoformat()
        
        # ç»Ÿè®¡ç»“æœ
        total_videos = len(results)
        ghost_count = len([r for r in results if "ghost probing" in r["key_actions"]])
        potential_count = len([r for r in results if "potential ghost probing" in r["key_actions"]])
        normal_count = total_videos - ghost_count - potential_count
        
        summary_data = {
            "real_drivemm_analysis_summary": {
                "model": "DriveMM_Real_8.45B_Parameters",
                "model_source": "https://huggingface.co/DriveMM/DriveMM",
                "azure_storage": "drivelmmstorage2e932dad7",
                "inference_environment": "Azure_ML_GPU",
                "comparison_baseline": "GPT-4.1_Balanced_F1_0.712",
                "analysis_timestamp": timestamp,
                "total_videos_analyzed": total_videos,
                "detection_results": {
                    "high_confidence_ghost_probing": ghost_count,
                    "potential_ghost_probing": potential_count,
                    "normal_traffic": normal_count
                },
                "detection_rates": {
                    "ghost_probing_rate": ghost_count / total_videos if total_videos > 0 else 0,
                    "potential_ghost_probing_rate": potential_count / total_videos if total_videos > 0 else 0,
                    "normal_traffic_rate": normal_count / total_videos if total_videos > 0 else 0
                }
            },
            "detailed_results": results
        }
        
        filename = "azure_drivemm_real_inference_results.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æœ€ç»ˆç»“æœå·²ä¿å­˜: {filename}")
        
        # æ˜¾ç¤ºç»Ÿè®¡
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ çœŸå®DriveMMæ¨ç†å®Œæˆ!")
        logger.info(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
        logger.info(f"   æ€»è§†é¢‘æ•°: {total_videos}")
        logger.info(f"   é«˜ç¡®ä¿¡åº¦é¬¼æ¢å¤´: {ghost_count} ({ghost_count/total_videos:.1%})")
        logger.info(f"   æ½œåœ¨é¬¼æ¢å¤´: {potential_count} ({potential_count/total_videos:.1%})")
        logger.info(f"   æ­£å¸¸äº¤é€š: {normal_count} ({normal_count/total_videos:.1%})")
        
        return summary_data

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºæ¨ç†å™¨
        inferencer = RealDriveMMAzureInference()
        
        # å¤„ç†æ‰€æœ‰è§†é¢‘
        results = inferencer.process_all_videos()
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ æ¨ç†è¿‡ç¨‹å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)