#!/usr/bin/env python
"""
è®¾ç½®DriveMMåœ¨Azure MLä¸Šçš„æ¨ç†ç¯å¢ƒ
"""

import os
import json
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Environment, CommandJob, AmlCompute
from azure.identity import DefaultAzureCredential

def setup_drivemm_azure():
    """è®¾ç½®DriveMM Azure MLç¯å¢ƒ"""
    
    # ä»config.jsonè¯»å–é…ç½®
    with open('config.json', 'r') as f:
        config = json.load(f)
    
    # åˆå§‹åŒ–Azure MLå®¢æˆ·ç«¯
    credential = DefaultAzureCredential()
    ml_client = MLClient(
        credential=credential,
        subscription_id=config['subscription_id'],
        resource_group_name=config['resource_group'],
        workspace_name=config['workspace_name']
    )
    
    print("âœ… Azure MLå®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
    
    # 1. åˆ›å»ºGPUè®¡ç®—é›†ç¾¤ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    print("ğŸ–¥ï¸ æ£€æŸ¥GPUè®¡ç®—é›†ç¾¤...")
    
    try:
        compute = ml_client.compute.get(config['compute_target'])
        print(f"âœ… æ‰¾åˆ°ç°æœ‰é›†ç¾¤: {compute.name}")
    except:
        print("âš ï¸ åˆ›å»ºæ–°çš„GPUé›†ç¾¤...")
        compute_config = AmlCompute(
            name=config['compute_target'],
            type="amlcompute",
            size="Standard_NC24ads_A100_v4",  # A100 GPU
            min_instances=0,
            max_instances=2,
            idle_time_before_scale_down=1800
        )
        
        compute = ml_client.compute.begin_create_or_update(compute_config).result()
        print(f"âœ… GPUé›†ç¾¤åˆ›å»ºæˆåŠŸ: {compute.name}")
    
    # 2. åˆ›å»ºDriveMMç¯å¢ƒ
    print("ğŸ³ åˆ›å»ºDriveMMæ¨ç†ç¯å¢ƒ...")
    
    environment = Environment(
        name="drivemm-inference-env",
        description="DriveMMæ¨ç†ç¯å¢ƒï¼Œæ”¯æŒGPU",
        conda_file="azure_drivemm_environment.yml",
        image="mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04:latest"
    )
    
    try:
        env = ml_client.environments.create_or_update(environment)
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ: {env.name}")
    except Exception as e:
        print(f"âš ï¸ ç¯å¢ƒå¯èƒ½å·²å­˜åœ¨: {e}")
    
    # 3. æäº¤DriveMMæ¨ç†ä½œä¸š
    print("ğŸš€ æäº¤DriveMMæ¨ç†ä½œä¸š...")
    
    job = CommandJob(
        experiment_name=config['experiment_name'],
        display_name="DriveMM_Real_GPU_Inference",
        description="ä½¿ç”¨çœŸå®DriveMMæ¨¡å‹åœ¨GPUä¸Šè¿›è¡Œæ¨ç†",
        compute=config['compute_target'],
        environment="drivemm-inference-env:latest",
        command="python azure_drivemm_real_inference.py",
        code="./",
        inputs={
            "storage_connection": os.getenv("AZURE_STORAGE_CONNECTION_STRING", "")
        },
        outputs={
            "results": "./outputs/drivemm_results/"
        },
        resources={
            "instance_count": 1,
            "shm_size": "16g"
        },
        timeout=7200  # 2å°æ—¶
    )
    
    try:
        submitted_job = ml_client.jobs.create_or_update(job)
        print(f"âœ… ä½œä¸šæäº¤æˆåŠŸ!")
        print(f"ğŸ“Š ä½œä¸šåç§°: {submitted_job.name}")
        print(f"ğŸ”— ç›‘æ§é“¾æ¥: {submitted_job.studio_url}")
        return submitted_job
    except Exception as e:
        print(f"âŒ ä½œä¸šæäº¤å¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    setup_drivemm_azure()
