#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆDriveMMåˆ†æè„šæœ¬
å°è¯•è·å–æ›´å®Œæ•´çš„Azure MLä½œä¸šç»“æœå¹¶è¿›è¡Œæ·±åº¦åˆ†æ
"""

import json
import csv
import os
import subprocess
import logging
from datetime import datetime
from typing import Dict, List, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_drivemm_analysis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedDriveMAnalyzer:
    """å¢å¼ºç‰ˆDriveMMåˆ†æå™¨"""
    
    def __init__(self):
        self.azure_config = {
            "subscription_id": "0d3f39ba-7349-4bd7-8122-649ff18f0a4a",
            "resource_group": "drivelm-rg",
            "workspace_name": "drivelm-ml-workspace",
            "storage_account": "drivelmmstorage2e932dad7"
        }
        self.job_id = "neat_tail_fndr1mjp80"
        
    def check_azure_ml_job_status(self) -> Dict:
        """æ£€æŸ¥Azure MLä½œä¸šçŠ¶æ€"""
        try:
            cmd = [
                "az", "ml", "job", "show",
                "--name", self.job_id,
                "--resource-group", self.azure_config["resource_group"],
                "--workspace-name", self.azure_config["workspace_name"],
                "--query", "properties",
                "-o", "json"
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                job_info = json.loads(result.stdout)
                logger.info(f"âœ… ä½œä¸šçŠ¶æ€: {job_info.get('status', 'Unknown')}")
                return job_info
            else:
                logger.error(f"âŒ è·å–ä½œä¸šçŠ¶æ€å¤±è´¥: {result.stderr}")
                return {}
                
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ä½œä¸šçŠ¶æ€å¤±è´¥: {str(e)}")
            return {}
            
    def search_azure_storage_results(self) -> List[str]:
        """æœç´¢Azureå­˜å‚¨ä¸­çš„ç»“æœæ–‡ä»¶"""
        possible_files = []
        
        # æœç´¢å¯èƒ½çš„å®¹å™¨å’Œè·¯å¾„
        containers = [
            "azureml",
            f"azureml-blobstore-{self.azure_config['subscription_id'].replace('-', '')}",
            "dada-videos",
            "wisead-videos"
        ]
        
        for container in containers:
            try:
                cmd = [
                    "az", "storage", "blob", "list",
                    "--account-name", self.azure_config["storage_account"],
                    "--container-name", container,
                    "--query", f"[?contains(name,'{self.job_id}') || contains(name,'drivemm') || contains(name,'inference')].{{name:name,size:properties.contentLength}}",
                    "-o", "json"
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    files = json.loads(result.stdout)
                    for file_info in files:
                        possible_files.append({
                            "container": container,
                            "name": file_info["name"],
                            "size": file_info["size"]
                        })
                        
            except Exception as e:
                logger.warning(f"âš ï¸ æœç´¢å®¹å™¨ {container} å¤±è´¥: {str(e)}")
                
        return possible_files
        
    def download_potential_results(self, files: List[Dict]) -> List[str]:
        """ä¸‹è½½æ½œåœ¨çš„ç»“æœæ–‡ä»¶"""
        downloaded_files = []
        
        for file_info in files:
            if file_info["name"].endswith('.json') and file_info["size"] > 100:
                try:
                    local_path = f"downloaded_{file_info['name'].replace('/', '_')}"
                    
                    cmd = [
                        "az", "storage", "blob", "download",
                        "--account-name", self.azure_config["storage_account"],
                        "--container-name", file_info["container"],
                        "--name", file_info["name"],
                        "--file", local_path
                    ]
                    
                    result = subprocess.run(cmd, capture_output=True, text=True)
                    if result.returncode == 0:
                        downloaded_files.append(local_path)
                        logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {local_path}")
                    else:
                        logger.warning(f"âš ï¸ ä¸‹è½½å¤±è´¥: {file_info['name']}")
                        
                except Exception as e:
                    logger.error(f"âŒ ä¸‹è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
                    
        return downloaded_files
        
    def analyze_log_for_insights(self, log_path: str) -> Dict:
        """åˆ†ææ—¥å¿—æ–‡ä»¶ä»¥è·å–æ›´å¤šè§è§£"""
        insights = {
            "total_videos_attempted": 0,
            "successful_predictions": 0,
            "failed_predictions": 0,
            "error_patterns": [],
            "model_responses": []
        }
        
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # åˆ†æè§†é¢‘å¤„ç†æ•°é‡
            video_patterns = [
                "ğŸ¤– DriveMMä¼˜åŒ–æ¨ç†:",
                "å¤„ç†è§†é¢‘:",
                "images_"
            ]
            
            for pattern in video_patterns:
                count = content.count(pattern)
                if count > 0:
                    insights["total_videos_attempted"] = max(
                        insights["total_videos_attempted"], count
                    )
                    
            # åˆ†æé”™è¯¯æ¨¡å¼
            error_patterns = [
                "æœªæ‰¾åˆ°JSONå¼€å§‹æ ‡è®°",
                "ä½¿ç”¨æ¨¡æ¿å“åº”",
                "JSONè§£æå¤±è´¥",
                "ç”Ÿæˆå¤±è´¥"
            ]
            
            for pattern in error_patterns:
                if pattern in content:
                    insights["error_patterns"].append(pattern)
                    
            # æå–æ¨¡å‹å“åº”
            import re
            response_pattern = r"ğŸ” æœ€ç»ˆå“åº”å‰\d+å­—ç¬¦: (.*?)(?=\n|$)"
            matches = re.findall(response_pattern, content)
            insights["model_responses"] = matches[:5]  # å‰5ä¸ªå“åº”
            
        except Exception as e:
            logger.error(f"âŒ æ—¥å¿—åˆ†æå¤±è´¥: {str(e)}")
            
        return insights
        
    def generate_comprehensive_report(self, 
                                    job_info: Dict,
                                    storage_files: List[Dict],
                                    log_insights: Dict) -> str:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        
        report = []
        report.append("=" * 100)
        report.append("ğŸ” DriveMM Azure MLä½œä¸šç»¼åˆåˆ†ææŠ¥å‘Š")
        report.append(f"ğŸ“… åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ğŸ†” ä½œä¸šID: {self.job_id}")
        report.append("=" * 100)
        report.append("")
        
        # ä½œä¸šä¿¡æ¯
        report.append("ğŸ¯ ä½œä¸šä¿¡æ¯")
        if job_info:
            report.append(f"   çŠ¶æ€: {job_info.get('status', 'Unknown')}")
            report.append(f"   åˆ›å»ºæ—¶é—´: {job_info.get('createdDateTime', 'Unknown')}")
            report.append(f"   ç»“æŸæ—¶é—´: {job_info.get('endDateTime', 'Unknown')}")
            report.append(f"   è®¡ç®—ç›®æ ‡: {job_info.get('computeId', 'Unknown')}")
        else:
            report.append("   âŒ æ— æ³•è·å–ä½œä¸šä¿¡æ¯")
        report.append("")
        
        # å­˜å‚¨æ–‡ä»¶
        report.append("ğŸ“ å­˜å‚¨æ–‡ä»¶åˆ†æ")
        if storage_files:
            report.append(f"   å‘ç° {len(storage_files)} ä¸ªç›¸å…³æ–‡ä»¶:")
            for file_info in storage_files:
                report.append(f"   - {file_info['name']} ({file_info['size']} bytes)")
        else:
            report.append("   âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœæ–‡ä»¶")
        report.append("")
        
        # æ—¥å¿—è§è§£
        report.append("ğŸ“Š æ—¥å¿—åˆ†æè§è§£")
        if log_insights:
            report.append(f"   å°è¯•å¤„ç†è§†é¢‘æ•°: {log_insights.get('total_videos_attempted', 0)}")
            report.append(f"   æˆåŠŸé¢„æµ‹æ•°: {log_insights.get('successful_predictions', 0)}")
            report.append(f"   å¤±è´¥é¢„æµ‹æ•°: {log_insights.get('failed_predictions', 0)}")
            
            if log_insights.get('error_patterns'):
                report.append("   é”™è¯¯æ¨¡å¼:")
                for pattern in log_insights['error_patterns']:
                    report.append(f"     - {pattern}")
                    
            if log_insights.get('model_responses'):
                report.append("   æ¨¡å‹å“åº”ç¤ºä¾‹:")
                for i, response in enumerate(log_insights['model_responses'][:3]):
                    report.append(f"     {i+1}. {response[:80]}...")
        report.append("")
        
        # é—®é¢˜è¯Šæ–­
        report.append("ğŸ”§ é—®é¢˜è¯Šæ–­")
        problems = []
        
        if not storage_files:
            problems.append("ç»“æœæ–‡ä»¶æœªæ‰¾åˆ° - å¯èƒ½ä¿å­˜å¤±è´¥")
            
        if "æœªæ‰¾åˆ°JSONå¼€å§‹æ ‡è®°" in log_insights.get('error_patterns', []):
            problems.append("JSONè§£æå¤±è´¥ - æ¨¡å‹è¾“å‡ºæ ¼å¼é—®é¢˜")
            
        if "ä½¿ç”¨æ¨¡æ¿å“åº”" in log_insights.get('error_patterns', []):
            problems.append("ä½¿ç”¨æ¨¡æ¿å“åº” - å¯èƒ½å¯¼è‡´ç»“æœä¸å‡†ç¡®")
            
        if log_insights.get('total_videos_attempted', 0) < 10:
            problems.append("å¤„ç†è§†é¢‘æ•°è¿‡å°‘ - å¯èƒ½ä½œä¸šæå‰ç»ˆæ­¢")
            
        if problems:
            for problem in problems:
                report.append(f"   âŒ {problem}")
        else:
            report.append("   âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
        report.append("")
        
        # å»ºè®®
        report.append("ğŸ’¡ å»ºè®®")
        suggestions = [
            "æ£€æŸ¥Azure MLä½œä¸šé…ç½®ï¼Œç¡®ä¿è¾“å‡ºè·¯å¾„æ­£ç¡®",
            "æ”¹è¿›æ¨¡å‹è¾“å‡ºæ ¼å¼ï¼Œç¡®ä¿JSONç»“æ„åŒ–è¾“å‡º",
            "å¢åŠ é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œé¿å…ä¾èµ–æ¨¡æ¿å“åº”",
            "æ‰©å¤§æµ‹è¯•æ•°æ®é›†ï¼Œè‡³å°‘åŒ…å«50-100ä¸ªè§†é¢‘",
            "è€ƒè™‘ä½¿ç”¨æ›´ç¨³å®šçš„æ¨ç†æ¡†æ¶"
        ]
        
        for suggestion in suggestions:
            report.append(f"   â€¢ {suggestion}")
        report.append("")
        
        report.append("=" * 100)
        
        return "\n".join(report)
        
    def run_comprehensive_analysis(self) -> None:
        """è¿è¡Œç»¼åˆåˆ†æ"""
        logger.info("ğŸš€ å¼€å§‹DriveMMç»¼åˆåˆ†æ...")
        
        # 1. æ£€æŸ¥ä½œä¸šçŠ¶æ€
        job_info = self.check_azure_ml_job_status()
        
        # 2. æœç´¢å­˜å‚¨æ–‡ä»¶
        storage_files = self.search_azure_storage_results()
        logger.info(f"ğŸ“ å‘ç° {len(storage_files)} ä¸ªç›¸å…³æ–‡ä»¶")
        
        # 3. ä¸‹è½½æ½œåœ¨ç»“æœ
        downloaded_files = self.download_potential_results(storage_files)
        logger.info(f"â¬‡ï¸ ä¸‹è½½äº† {len(downloaded_files)} ä¸ªæ–‡ä»¶")
        
        # 4. åˆ†ææ—¥å¿—
        log_insights = {}
        log_path = "azure_ml_outputs/artifacts/user_logs/std_log.txt"
        if os.path.exists(log_path):
            log_insights = self.analyze_log_for_insights(log_path)
            logger.info("ğŸ“Š æ—¥å¿—åˆ†æå®Œæˆ")
        
        # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = self.generate_comprehensive_report(
            job_info, storage_files, log_insights
        )
        
        # 6. ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"comprehensive_drivemm_analysis_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        print(report)
        logger.info(f"âœ… ç»¼åˆåˆ†æå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜è‡³: {report_file}")
        
        return downloaded_files

def main():
    """ä¸»å‡½æ•°"""
    analyzer = EnhancedDriveMAnalyzer()
    analyzer.run_comprehensive_analysis()

if __name__ == "__main__":
    main()