#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Gemini Promptæ”¹è¿›æ•ˆæœç®€åŒ–åˆ†æ - ä¿®å¤ç‰ˆ
"""

import os
import json
import pandas as pd
from sklearn.metrics import confusion_matrix
import numpy as np

def load_gemini_balanced_results():
    """åŠ è½½ä½¿ç”¨å¹³è¡¡ç‰ˆpromptçš„Gemini 2.0 Flashç»“æœ"""
    gemini_dir = "result/gemini-balanced-full"
    results = {}
    
    for filename in os.listdir(gemini_dir):
        if filename.startswith("actionSummary_") and filename.endswith(".json"):
            video_id = filename.replace("actionSummary_", "").replace(".json", "")
            if video_id.startswith("dada_"):
                video_id = video_id.replace("dada_", "images_")
            
            with open(os.path.join(gemini_dir, filename), 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            key_actions = []
            for segment in data:
                if isinstance(segment, dict) and 'key_actions' in segment:
                    key_actions.append(segment['key_actions'])
            
            results[video_id] = key_actions
    
    return results

def load_gemini_original_results():
    """åŠ è½½åŸå§‹promptçš„Gemini 1.5 Flashç»“æœ"""
    gemini_dir = "result/gemini-1.5-flash"
    results = {}
    
    for filename in os.listdir(gemini_dir):
        if filename.startswith("actionSummary_") and filename.endswith(".json"):
            video_id = filename.replace("actionSummary_", "").replace(".json", "")
            
            try:
                with open(os.path.join(gemini_dir, filename), 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                key_actions = []
                for segment in data:
                    if isinstance(segment, dict) and 'key_actions' in segment:
                        key_actions.append(segment['key_actions'])
                
                results[video_id] = key_actions
            except:
                continue
    
    return results

def load_ground_truth():
    """åŠ è½½Ground Truthæ ‡ç­¾"""
    df = pd.read_csv('result/groundtruth_labels.csv', sep='\t')
    ground_truth = {}
    
    for _, row in df.iterrows():
        video_id = row['video_id'].replace('.avi', '')
        label = row['ground_truth_label']
        ground_truth[video_id] = 'none' if label == 'none' else 'ghost probing'
    
    return ground_truth

def evaluate_results(results, ground_truth):
    """è¯„ä¼°ç»“æœ"""
    y_true = []
    y_pred = []
    
    for video_id in sorted(results.keys()):
        if video_id in ground_truth:
            gt_label = ground_truth[video_id]
            y_true.append(gt_label)
            
            predictions = results[video_id]
            has_ghost_probing = any('ghost probing' in str(pred).lower() for pred in predictions)
            pred_label = 'ghost probing' if has_ghost_probing else 'none'
            y_pred.append(pred_label)
    
    return y_true, y_pred

def calculate_metrics(y_true, y_pred):
    """è®¡ç®—æŒ‡æ ‡"""
    y_true_binary = [1 if label == 'ghost probing' else 0 for label in y_true]
    y_pred_binary = [1 if label == 'ghost probing' else 0 for label in y_pred]
    
    cm = confusion_matrix(y_true_binary, y_pred_binary)
    tn, fp, fn, tp = cm.ravel()
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'accuracy': accuracy,
        'specificity': specificity,
        'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn
    }

def main():
    print("ğŸ” Gemini Promptæ”¹è¿›æ•ˆæœåˆ†æ")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    balanced_results = load_gemini_balanced_results()
    original_results = load_gemini_original_results()
    ground_truth = load_ground_truth()
    
    print(f"ğŸ“Š Gemini 2.0 Flash + å¹³è¡¡ç‰ˆPrompt: {len(balanced_results)} ä¸ªè§†é¢‘")
    print(f"ğŸ“Š Gemini 1.5 Flash + åŸå§‹Prompt: {len(original_results)} ä¸ªè§†é¢‘")
    
    # æ‰¾åˆ°å…±åŒè§†é¢‘
    common_videos = set(balanced_results.keys()) & set(original_results.keys()) & set(ground_truth.keys())
    print(f"ğŸ“¹ å¯å¯¹æ¯”çš„å…±åŒè§†é¢‘: {len(common_videos)} ä¸ª")
    
    # è¯„ä¼°
    balanced_subset = {vid: balanced_results[vid] for vid in common_videos}
    original_subset = {vid: original_results[vid] for vid in common_videos}
    gt_subset = {vid: ground_truth[vid] for vid in common_videos}
    
    y_true_orig, y_pred_orig = evaluate_results(original_subset, gt_subset)
    y_true_bal, y_pred_bal = evaluate_results(balanced_subset, gt_subset)
    
    metrics_original = calculate_metrics(y_true_orig, y_pred_orig)
    metrics_balanced = calculate_metrics(y_true_bal, y_pred_bal)
    
    print(f"\nğŸ¯ å¯¹æ¯”ç»“æœ ({len(common_videos)} ä¸ªè§†é¢‘):")
    print("-" * 50)
    
    print("ğŸ¤– Gemini 1.5 Flash + åŸå§‹Prompt:")
    print(f"  ğŸ“Š ç²¾ç¡®åº¦: {metrics_original['precision']:.3f}")
    print(f"  ğŸ“Š å¬å›ç‡: {metrics_original['recall']:.3f}")
    print(f"  ğŸ“Š F1åˆ†æ•°: {metrics_original['f1']:.3f}")
    print(f"  ğŸ“Š å‡†ç¡®ç‡: {metrics_original['accuracy']:.3f}")
    print(f"  ğŸ“Š ç‰¹å¼‚æ€§: {metrics_original['specificity']:.3f}")
    
    print("\nğŸ¤– Gemini 2.0 Flash + å¹³è¡¡ç‰ˆPrompt:")
    print(f"  ğŸ“Š ç²¾ç¡®åº¦: {metrics_balanced['precision']:.3f}")
    print(f"  ğŸ“Š å¬å›ç‡: {metrics_balanced['recall']:.3f}")
    print(f"  ğŸ“Š F1åˆ†æ•°: {metrics_balanced['f1']:.3f}")
    print(f"  ğŸ“Š å‡†ç¡®ç‡: {metrics_balanced['accuracy']:.3f}")
    print(f"  ğŸ“Š ç‰¹å¼‚æ€§: {metrics_balanced['specificity']:.3f}")
    
    print(f"\nğŸ“ˆ æ”¹è¿›æ•ˆæœ:")
    improvements = {}
    for metric in ['precision', 'recall', 'f1', 'accuracy', 'specificity']:
        old_val = metrics_original[metric]
        new_val = metrics_balanced[metric]
        change = new_val - old_val
        pct_change = (change / old_val * 100) if old_val > 0 else 0
        
        status = "ğŸ“ˆ æ”¹è¿›" if change > 0 else "ğŸ“‰ ä¸‹é™" if change < 0 else "â¡ï¸ æŒå¹³"
        print(f"  {metric.capitalize()}: {old_val:.3f} â†’ {new_val:.3f} ({change:+.3f}, {pct_change:+.1f}%) {status}")
        
        improvements[metric] = change > 0
    
    improved_count = sum(improvements.values())
    print(f"\nğŸ¯ æ€»ç»“:")
    print(f"  ğŸ“Š æ”¹è¿›æŒ‡æ ‡: {improved_count}/5")
    print(f"  ğŸ“ˆ æ”¹è¿›ç‡: {improved_count/5*100:.1f}%")
    
    # é‡è¦å‘ç°
    print(f"\nğŸ’¡ é‡è¦å‘ç°:")
    print(f"  ğŸ”„ æ¨¡å‹å‡çº§: Gemini 1.5 Flash â†’ Gemini 2.0 Flash")
    print(f"  ğŸ“ Promptä¼˜åŒ–: åŸå§‹Prompt â†’ å¹³è¡¡ç‰ˆPrompt")
    print(f"  ğŸ¯ ç²¾ç¡®åº¦æå‡: {metrics_balanced['precision'] - metrics_original['precision']:+.3f}")
    print(f"  ğŸ“‰ å¬å›ç‡å˜åŒ–: {metrics_balanced['recall'] - metrics_original['recall']:+.3f}")
    print(f"  âš–ï¸ ç‰¹å¼‚æ€§æå‡: {metrics_balanced['specificity'] - metrics_original['specificity']:+.3f}")
    
    # ç­–ç•¥åˆ†æ
    if metrics_balanced['precision'] > metrics_original['precision'] and metrics_balanced['specificity'] > metrics_original['specificity']:
        print(f"\nğŸ“Š ç­–ç•¥å˜åŒ–: ä»é«˜å¬å›ç­–ç•¥è½¬å‘å¹³è¡¡ç­–ç•¥")
        print(f"  âœ… å‡å°‘è¯¯æŠ¥ (å‡é˜³æ€§)")
        print(f"  âš ï¸ ç•¥å¾®å¢åŠ æ¼æŠ¥ (å‡é˜´æ€§)")
        print(f"  ğŸ¯ æ›´é€‚åˆç²¾ç¡®æ£€æµ‹åœºæ™¯")

if __name__ == "__main__":
    main()