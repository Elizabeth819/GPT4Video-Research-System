#!/usr/bin/env python3
"""
Fix and Resubmit Azure ML LLaVA Job Script
ä¿®å¤å¹¶é‡æ–°æäº¤Azure ML LLaVAä½œä¸š
æ–‡ä»¶è·¯å¾„: /Users/wanmeng/repository/GPT4Video-cobra-auto/result/LlaVA/fix_and_resubmit_job.py

Based on the failure analysis of job 'crimson_boniato_k1kg8q62fr', this script:
1. Validates all required files exist
2. Submits the job using the correct YAML configuration
3. Provides monitoring and recovery options
"""

import os
import sys
import logging
from pathlib import Path
from datetime import datetime

try:
    from azure.ai.ml import MLClient, command, Input, Output
    from azure.identity import DefaultAzureCredential
    from azure.core.exceptions import HttpResponseError
except ImportError:
    print("âŒ Azure ML SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install azure-ai-ml")
    sys.exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class LLaVAJobFixer:
    """LLaVAä½œä¸šä¿®å¤å’Œé‡æ–°æäº¤å™¨"""
    
    def __init__(self, 
                 subscription_id: str = "0d3f39ba-7349-4bd7-8122-649ff18f0a4a",
                 resource_group: str = "llava-resourcegroup", 
                 workspace_name: str = "llava-workspace"):
        """
        åˆå§‹åŒ–Azure MLå®¢æˆ·ç«¯
        """
        self.subscription_id = subscription_id
        self.resource_group = resource_group
        self.workspace_name = workspace_name
        self.base_path = Path(__file__).parent
        
        try:
            # åˆå§‹åŒ–MLå®¢æˆ·ç«¯
            credential = DefaultAzureCredential()
            self.ml_client = MLClient(
                credential=credential,
                subscription_id=subscription_id,
                resource_group_name=resource_group,
                workspace_name=workspace_name
            )
            
            logger.info(f"âœ… Azure MLå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"ğŸ“‹ å·¥ä½œåŒº: {workspace_name}")
            
        except Exception as e:
            logger.error(f"âŒ Azure MLå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def validate_required_files(self) -> bool:
        """éªŒè¯æ‰€éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        logger.info("ğŸ” éªŒè¯å¿…éœ€æ–‡ä»¶...")
        
        required_files = [
            'requirements.txt',
            'llava_ghost_probing_batch.py',
            'llava_ghost_probing_detector.py',
            'azure_ml_llava_ghost_probing.yml'
        ]
        
        missing_files = []
        for file_name in required_files:
            file_path = self.base_path / file_name
            if not file_path.exists():
                missing_files.append(file_name)
            else:
                logger.info(f"âœ… {file_name}")
        
        if missing_files:
            logger.error(f"âŒ ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {missing_files}")
            return False
        
        logger.info("âœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶å­˜åœ¨")
        return True
    
    def create_fixed_job_config(self, 
                              job_name: str = None,
                              compute_name: str = "llava-a100-low-priority",
                              limit: int = 100,
                              save_interval: int = 10) -> dict:
        """åˆ›å»ºä¿®å¤åçš„ä½œä¸šé…ç½®"""
        if job_name is None:
            timestamp = datetime.now().strftime('%m%d_%H%M%S')
            job_name = f"llava-ghost-probing-fixed-{timestamp}"
        
        logger.info(f"ğŸ”§ åˆ›å»ºä¿®å¤åçš„ä½œä¸šé…ç½®: {job_name}")
        
        # ä¿®å¤åçš„å•è¡Œå‘½ä»¤ï¼ˆè§£å†³multiline YAMLé—®é¢˜ï¼‰
        fixed_command = (
            "echo 'ğŸš€ å¼€å§‹LLaVAé¬¼æ¢å¤´æ£€æµ‹ä½œä¸š' && "
            "echo 'ğŸ“‹ å®‰è£…ä¾èµ–åŒ…...' && "
            "pip install --upgrade pip && "
            "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu117 && "
            "pip install transformers==4.37.0 accelerate tokenizers sentencepiece && "
            "pip install decord opencv-python pillow && "
            "pip install numpy pandas tqdm scikit-learn matplotlib seaborn && "
            "pip install pyyaml python-dotenv && "
            "echo 'âœ… ä¾èµ–å®‰è£…å®Œæˆ' && "
            "echo 'ğŸ” æ£€æŸ¥GPUå¯ç”¨æ€§...' && "
            "python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}'); print(f'GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\"}')\" && "
            "echo 'ğŸ“ æ£€æŸ¥å·¥ä½œç›®å½•æ–‡ä»¶:' && ls -la && "
            "echo 'ğŸ¬ å¼€å§‹æ‰¹å¤„ç†è§†é¢‘...' && "
            f"python llava_ghost_probing_batch.py --video-folder ./inputs/video_data --output-folder ./outputs/llava_ghost_probing_results --limit {limit} --save-interval {save_interval} && "
            "echo 'âœ… LLaVAé¬¼æ¢å¤´æ£€æµ‹ä½œä¸šå®Œæˆ'"
        )
        
        job_config = {
            "display_name": job_name,
            "description": f"Fixed LLaVA-NeXT Ghost Probing Detection on {limit} DADA Videos",
            "command": fixed_command,
            "environment": "azureml:AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu:10",
            "compute": compute_name,
            "inputs": {
                "video_data": Input(
                    type="uri_folder",
                    path="azureml://datastores/workspaceblobstore/paths/DADA-100-videos/",
                    mode="ro_mount"
                )
            },
            "outputs": {
                "results": Output(
                    type="uri_folder",
                    path=f"azureml://datastores/workspaceblobstore/paths/llava-ghost-probing-fixed/{job_name}/",
                    mode="rw_mount"
                )
            },
            "environment_variables": {
                "CUDA_VISIBLE_DEVICES": "0",
                "HF_HOME": "/tmp/huggingface",
                "TORCH_HOME": "/tmp/torch",
                "TRANSFORMERS_CACHE": "/tmp/transformers",
                "PYTHONPATH": "/mnt/azureml/cr/j/*/exe/wd"
            },
            "tags": {
                "model": "LLaVA-Video-7B-Qwen2",
                "task": "ghost_probing_detection",
                "dataset": "DADA-100-videos",
                "status": "fixed",
                "previous_job": "crimson_boniato_k1kg8q62fr"
            },
            "experiment_name": "llava-ghost-probing-fixed"
        }
        
        return job_config
    
    def submit_fixed_job(self, 
                        job_name: str = None,
                        compute_name: str = "llava-a100-low-priority",
                        limit: int = 100,
                        save_interval: int = 10,
                        dry_run: bool = False) -> str:
        """æäº¤ä¿®å¤åçš„ä½œä¸š"""
        try:
            # 1. éªŒè¯æ–‡ä»¶
            if not self.validate_required_files():
                raise Exception("å¿…éœ€æ–‡ä»¶éªŒè¯å¤±è´¥")
            
            # 2. åˆ›å»ºä½œä¸šé…ç½®
            job_config = self.create_fixed_job_config(job_name, compute_name, limit, save_interval)
            job_name = job_config["display_name"]
            
            logger.info(f"ğŸš€ å‡†å¤‡æäº¤ä¿®å¤åçš„LLaVAä½œä¸š: {job_name}")
            logger.info(f"ğŸ¬ å¤„ç†è§†é¢‘æ•°é‡: {limit}ä¸ª")
            logger.info(f"ğŸ’¾ ä¿å­˜é—´éš”: {save_interval}ä¸ªè§†é¢‘")
            
            # 3. åˆ›å»ºå‘½ä»¤ä½œä¸šå¯¹è±¡
            job = command(
                display_name=job_config["display_name"],
                description=job_config["description"],
                command=job_config["command"],
                environment=job_config["environment"],
                compute=job_config["compute"],
                code=".",  # ä½¿ç”¨å½“å‰ç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
                inputs=job_config["inputs"],
                outputs=job_config["outputs"],
                environment_variables=job_config["environment_variables"],
                tags=job_config["tags"],
                experiment_name=job_config["experiment_name"]
            )
            
            if dry_run:
                logger.info("ğŸ§ª Dry Runæ¨¡å¼ - ä½œä¸šé…ç½®éªŒè¯")
                logger.info("âœ… ä¿®å¤åä½œä¸šé…ç½®éªŒè¯é€šè¿‡")
                logger.info(f"ğŸ“ ä½œä¸šåç§°: {job_name}")
                logger.info(f"ğŸ–¥ï¸  è®¡ç®—é›†ç¾¤: {compute_name}")
                logger.info(f"ğŸ¬ è§†é¢‘æ•°é‡: {limit}")
                logger.info("ğŸ’¡ ä½¿ç”¨ --no-dry-run æäº¤çœŸå®ä½œä¸š")
                
                # æ˜¾ç¤ºä¿®å¤ç‚¹
                logger.info("\\nğŸ”§ ä¸»è¦ä¿®å¤ç‚¹:")
                logger.info("1. âœ… ä½¿ç”¨å•è¡Œå‘½ä»¤é¿å…YAMLè§£æé—®é¢˜")
                logger.info("2. âœ… åŒ…å«æ‰€æœ‰å¿…éœ€æ–‡ä»¶(requirements.txt, *.py)")
                logger.info("3. âœ… æ·»åŠ è°ƒè¯•ä¿¡æ¯(ls -la, GPUæ£€æŸ¥)")
                logger.info("4. âœ… ä½¿ç”¨æ­£ç¡®çš„è§†é¢‘é™åˆ¶å’Œä¿å­˜é—´éš”")
                
                return f"dry-run-{job_name}"
            
            # 4. æäº¤ä½œä¸š
            logger.info("ğŸ“¤ æ­£åœ¨æäº¤ä¿®å¤åä½œä¸šåˆ°Azure ML...")
            submitted_job = self.ml_client.jobs.create_or_update(job)
            
            logger.info(f"âœ… ä¿®å¤ä½œä¸šæäº¤æˆåŠŸ!")
            logger.info(f"ğŸ†” ä½œä¸šID: {submitted_job.name}")
            logger.info(f"ğŸ“Š ä½œä¸šçŠ¶æ€: {submitted_job.status}")
            logger.info(f"ğŸ”— Azure ML Studioé“¾æ¥: {submitted_job.studio_url}")
            
            # 5. æä¾›ç›‘æ§å»ºè®®
            logger.info("\\nğŸ’¡ ç›‘æ§å»ºè®®:")
            logger.info(f"   æ£€æŸ¥çŠ¶æ€: python {__file__} --action status --job-name {submitted_job.name}")
            logger.info(f"   å®æ—¶ç›‘æ§: python monitor_job.py --job-name {submitted_job.name}")
            logger.info("\\nğŸ¯ é¢„è®¡å®Œæˆæ—¶é—´: 2-3å°æ—¶ (100ä¸ªè§†é¢‘)")
            
            return submitted_job.name
            
        except Exception as e:
            logger.error(f"âŒ æäº¤ä¿®å¤ä½œä¸šå¤±è´¥: {e}")
            raise
    
    def cancel_failed_job(self, failed_job_name: str = "crimson_boniato_k1kg8q62fr"):
        """å–æ¶ˆå¤±è´¥çš„ä½œä¸šï¼ˆæ¸…ç†èµ„æºï¼‰"""
        try:
            logger.info(f"ğŸ—‘ï¸ å–æ¶ˆå¤±è´¥ä½œä¸š: {failed_job_name}")
            
            # è·å–ä½œä¸šçŠ¶æ€
            job = self.ml_client.jobs.get(failed_job_name)
            if job.status in ["Running", "Starting", "Preparing"]:
                self.ml_client.jobs.cancel(failed_job_name)
                logger.info(f"âœ… ä½œä¸šå·²å–æ¶ˆ: {failed_job_name}")
            else:
                logger.info(f"â„¹ï¸ ä½œä¸šçŠ¶æ€ä¸º {job.status}ï¼Œæ— éœ€å–æ¶ˆ")
                
        except Exception as e:
            logger.warning(f"âš ï¸ å–æ¶ˆä½œä¸šå¤±è´¥: {e}")
    
    def check_job_status(self, job_name: str):
        """æ£€æŸ¥ä½œä¸šçŠ¶æ€"""
        try:
            job = self.ml_client.jobs.get(job_name)
            
            print(f"\\nğŸ“Š ä½œä¸šçŠ¶æ€æŠ¥å‘Š: {job_name}")
            print("="*60)
            print(f"çŠ¶æ€: {job.status}")
            print(f"å¼€å§‹æ—¶é—´: {job.creation_context.created_at}")
            print(f"Studioé“¾æ¥: {job.studio_url}")
            
            if job.status == "Completed":
                print("âœ… ä½œä¸šå·²å®Œæˆ!")
            elif job.status == "Failed":
                print("âŒ ä½œä¸šå¤±è´¥!")
                print("ğŸ’¡ è¯·æ£€æŸ¥æ—¥å¿—æˆ–é‡æ–°è¿è¡Œä¿®å¤è„šæœ¬")
            elif job.status in ["Running", "Preparing", "Starting"]:
                print("ğŸ”„ ä½œä¸šæ­£åœ¨è¿è¡Œä¸­...")
                print(f"ğŸ’¡ ç›‘æ§å‘½ä»¤: python monitor_job.py --job-name {job_name}")
            else:
                print(f"ğŸ“‹ å½“å‰çŠ¶æ€: {job.status}")
            
            print("="*60)
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ä½œä¸šçŠ¶æ€å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='LLaVAä½œä¸šä¿®å¤å’Œé‡æ–°æäº¤å·¥å…·')
    parser.add_argument('--action', choices=['fix', 'status', 'cancel'], 
                       default='fix', help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--job-name', type=str, help='ä½œä¸šåç§°')
    parser.add_argument('--compute', type=str, default='llava-a100-low-priority',
                       help='è®¡ç®—é›†ç¾¤åç§°')
    parser.add_argument('--limit', type=int, default=100,
                       help='å¤„ç†è§†é¢‘æ•°é‡é™åˆ¶')
    parser.add_argument('--save-interval', type=int, default=10,
                       help='ä¿å­˜é—´éš”')
    parser.add_argument('--dry-run', action='store_true', default=True,
                       help='æµ‹è¯•æ¨¡å¼ï¼ŒéªŒè¯é…ç½®ä½†ä¸æäº¤ä½œä¸š')
    parser.add_argument('--no-dry-run', action='store_true',
                       help='å…³é—­æµ‹è¯•æ¨¡å¼ï¼Œæäº¤çœŸå®ä½œä¸š')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºä¿®å¤å™¨
        fixer = LLaVAJobFixer()
        
        if args.action == 'fix':
            # ç¡®å®šæ˜¯å¦ä¸ºdry runæ¨¡å¼
            is_dry_run = args.dry_run and not args.no_dry_run
            
            # å–æ¶ˆå¤±è´¥çš„ä½œä¸š
            fixer.cancel_failed_job("crimson_boniato_k1kg8q62fr")
            
            # æäº¤ä¿®å¤åçš„ä½œä¸š
            job_id = fixer.submit_fixed_job(
                job_name=args.job_name,
                compute_name=args.compute,
                limit=args.limit,
                save_interval=args.save_interval,
                dry_run=is_dry_run
            )
            
            print(f"\\nâœ… ä¿®å¤ä½œä¸šæäº¤æˆåŠŸ: {job_id}")
            
        elif args.action == 'status':
            # æ£€æŸ¥ä½œä¸šçŠ¶æ€
            if not args.job_name:
                print("âŒ è¯·æŒ‡å®šä½œä¸šåç§°: --job-name JOB_NAME")
                sys.exit(1)
            fixer.check_job_status(args.job_name)
            
        elif args.action == 'cancel':
            # å–æ¶ˆä½œä¸š
            job_name = args.job_name or "crimson_boniato_k1kg8q62fr"
            fixer.cancel_failed_job(job_name)
            
    except Exception as e:
        logger.error(f"âŒ æ“ä½œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()