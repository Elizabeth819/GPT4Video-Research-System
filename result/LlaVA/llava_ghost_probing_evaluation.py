#!/usr/bin/env python3
"""
LLaVA Ghost Probing Detection Evaluation Script
è¯„ä¼°LLaVAé¬¼æ¢å¤´æ£€æµ‹ç»“æœä¸ground truthçš„å¯¹æ¯”åˆ†æ
è®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰æŒ‡æ ‡
æ–‡ä»¶è·¯å¾„: /Users/wanmeng/repository/GPT4Video-cobra-auto/result/LlaVA/llava_ghost_probing_evaluation.py
"""

import os
import json
import csv
import logging
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/wanmeng/repository/GPT4Video-cobra-auto/result/LlaVA/llava_evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LLaVAGhostProbingEvaluator:
    """LLaVAé¬¼æ¢å¤´æ£€æµ‹è¯„ä¼°å™¨"""
    
    def __init__(self, 
                 groundtruth_file: str = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/groundtruth_labels.csv",
                 output_folder: str = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/LlaVA/evaluation_results"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            groundtruth_file: Ground truthæ ‡ç­¾æ–‡ä»¶è·¯å¾„
            output_folder: è¯„ä¼°ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹
        """
        self.groundtruth_file = Path(groundtruth_file)
        self.output_folder = Path(output_folder)
        self.output_folder.mkdir(parents=True, exist_ok=True)
        
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åŠ è½½ground truth
        self.ground_truth = self._load_ground_truth()
        
        logger.info(f"âœ… è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“‹ Ground truthæ ‡ç­¾: {len(self.ground_truth)}")
        logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {self.output_folder}")
    
    def _load_ground_truth(self) -> Dict[str, str]:
        """åŠ è½½ground truthæ ‡ç­¾"""
        try:
            ground_truth = {}
            
            # è¯»å–TSVæ ¼å¼çš„ground truthæ–‡ä»¶
            with open(self.groundtruth_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f, delimiter='\t')
                for row in reader:
                    video_id = row['video_id'].replace('.avi', '')  # ç§»é™¤æ‰©å±•åç»Ÿä¸€æ ¼å¼
                    label = row['ground_truth_label']
                    ground_truth[video_id] = label
            
            logger.info(f"æˆåŠŸåŠ è½½{len(ground_truth)}ä¸ªground truthæ ‡ç­¾")
            return ground_truth
            
        except Exception as e:
            logger.error(f"åŠ è½½ground truthå¤±è´¥: {e}")
            return {}
    
    def _standardize_labels(self, gt_label: str, pred_label: str) -> Tuple[str, str]:
        """
        æ ‡å‡†åŒ–æ ‡ç­¾æ ¼å¼
        
        Args:
            gt_label: Ground truthæ ‡ç­¾
            pred_label: é¢„æµ‹æ ‡ç­¾
            
        Returns:
            (æ ‡å‡†åŒ–çš„gt_label, æ ‡å‡†åŒ–çš„pred_label)
        """
        # æ ‡å‡†åŒ–ground truthæ ‡ç­¾
        if gt_label.lower() == 'none':
            gt_standardized = 'normal'
        elif 'ghost probing' in gt_label.lower():
            gt_standardized = 'ghost_probing'
        else:
            gt_standardized = 'normal'
        
        # æ ‡å‡†åŒ–é¢„æµ‹æ ‡ç­¾
        if pred_label == 'ghost_probing':
            pred_standardized = 'ghost_probing'
        elif pred_label == 'potential_ghost_probing':
            pred_standardized = 'ghost_probing'  # å°†æ½œåœ¨é¬¼æ¢å¤´å½’ç±»ä¸ºé¬¼æ¢å¤´
        else:
            pred_standardized = 'normal'
        
        return gt_standardized, pred_standardized
    
    def evaluate_results(self, llava_results_file: str) -> Dict:
        """
        è¯„ä¼°LLaVAç»“æœ
        
        Args:
            llava_results_file: LLaVAç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        logger.info(f"ğŸ¯ å¼€å§‹è¯„ä¼°LLaVAç»“æœ: {llava_results_file}")
        
        try:
            # åŠ è½½LLaVAç»“æœ
            with open(llava_results_file, 'r', encoding='utf-8') as f:
                llava_data = json.load(f)
            
            # æå–ç»“æœæ•°æ®
            if 'results' in llava_data:
                results = llava_data['results']
            else:
                results = llava_data
            
            # å‡†å¤‡è¯„ä¼°æ•°æ®
            evaluation_data = []
            matched_count = 0
            
            for result in results:
                video_id = result['video_id']
                pred_label = result['ghost_probing_label']
                
                # æŸ¥æ‰¾å¯¹åº”çš„ground truth
                if video_id in self.ground_truth:
                    gt_label = self.ground_truth[video_id]
                    
                    # æ ‡å‡†åŒ–æ ‡ç­¾
                    gt_std, pred_std = self._standardize_labels(gt_label, pred_label)
                    
                    evaluation_data.append({
                        'video_id': video_id,
                        'ground_truth_raw': gt_label,
                        'prediction_raw': pred_label,
                        'ground_truth': gt_std,
                        'prediction': pred_std,
                        'confidence': result.get('confidence', 0.0),
                        'processing_time': result.get('processing_time', 0.0),
                        'correct': gt_std == pred_std
                    })
                    matched_count += 1
                else:
                    logger.warning(f"è§†é¢‘{video_id}åœ¨ground truthä¸­æœªæ‰¾åˆ°")
            
            logger.info(f"âœ… æˆåŠŸåŒ¹é…{matched_count}ä¸ªè§†é¢‘è¿›è¡Œè¯„ä¼°")
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            metrics = self._calculate_metrics(evaluation_data)
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            self._save_evaluation_results(evaluation_data, metrics, llava_results_file)
            
            # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
            self._generate_visualizations(evaluation_data, metrics)
            
            return metrics
            
        except Exception as e:
            logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            return {}
    
    def _calculate_metrics(self, evaluation_data: List[Dict]) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        try:
            # æå–çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
            y_true = [item['ground_truth'] for item in evaluation_data]
            y_pred = [item['prediction'] for item in evaluation_data]
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred)
            
            # å¯¹äºäºŒåˆ†ç±»ï¼ˆghost_probing vs normalï¼‰
            precision = precision_score(y_true, y_pred, pos_label='ghost_probing', average='binary')
            recall = recall_score(y_true, y_pred, pos_label='ghost_probing', average='binary')
            f1 = f1_score(y_true, y_pred, pos_label='ghost_probing', average='binary')
            
            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_true, y_pred, labels=['normal', 'ghost_probing'])
            
            # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
            class_report = classification_report(y_true, y_pred, output_dict=True)
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_videos = len(evaluation_data)
            correct_predictions = sum([item['correct'] for item in evaluation_data])
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            gt_ghost_count = sum([1 for gt in y_true if gt == 'ghost_probing'])
            gt_normal_count = sum([1 for gt in y_true if gt == 'normal'])
            pred_ghost_count = sum([1 for pred in y_pred if pred == 'ghost_probing'])
            pred_normal_count = sum([1 for pred in y_pred if pred == 'normal'])
            
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
            avg_confidence = np.mean([item['confidence'] for item in evaluation_data])
            avg_processing_time = np.mean([item['processing_time'] for item in evaluation_data])
            
            metrics = {
                'model': 'LLaVA-Video-7B-Qwen2',
                'evaluation_timestamp': self.timestamp,
                'dataset_info': {
                    'total_videos': total_videos,
                    'ground_truth_ghost_probing': gt_ghost_count,
                    'ground_truth_normal': gt_normal_count,
                    'predicted_ghost_probing': pred_ghost_count,
                    'predicted_normal': pred_normal_count
                },
                'performance_metrics': {
                    'accuracy': round(accuracy, 4),
                    'precision': round(precision, 4),
                    'recall': round(recall, 4),
                    'f1_score': round(f1, 4),
                    'correct_predictions': correct_predictions,
                    'average_confidence': round(avg_confidence, 4),
                    'average_processing_time': round(avg_processing_time, 2)
                },
                'confusion_matrix': {
                    'matrix': cm.tolist(),
                    'labels': ['normal', 'ghost_probing'],
                    'true_negatives': int(cm[0, 0]),
                    'false_positives': int(cm[0, 1]),
                    'false_negatives': int(cm[1, 0]),
                    'true_positives': int(cm[1, 1])
                },
                'classification_report': class_report,
                'comparison_with_gpt41_balanced': {
                    'gpt41_f1_score': 0.712,
                    'gpt41_recall': 0.963,
                    'gpt41_precision': 0.565,
                    'llava_vs_gpt41_f1_diff': round(f1 - 0.712, 4),
                    'llava_vs_gpt41_recall_diff': round(recall - 0.963, 4),
                    'llava_vs_gpt41_precision_diff': round(precision - 0.565, 4)
                }
            }
            
            logger.info("ğŸ“Š è¯„ä¼°æŒ‡æ ‡è®¡ç®—å®Œæˆ:")
            logger.info(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
            logger.info(f"  ç²¾ç¡®ç‡: {precision:.4f}")
            logger.info(f"  å¬å›ç‡: {recall:.4f}")
            logger.info(f"  F1åˆ†æ•°: {f1:.4f}")
            
            return metrics
            
        except Exception as e:
            logger.error(f"æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {}
    
    def _save_evaluation_results(self, evaluation_data: List[Dict], metrics: Dict, results_file: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        try:
            # ä¿å­˜è¯¦ç»†è¯„ä¼°æ•°æ®
            detailed_file = self.output_folder / f"llava_detailed_evaluation_{self.timestamp}.json"
            detailed_results = {
                'source_file': results_file,
                'evaluation_data': evaluation_data,
                'metrics': metrics
            }
            
            with open(detailed_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_results, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜CSVæ ¼å¼çš„å¯¹æ¯”ç»“æœ
            csv_file = self.output_folder / f"llava_evaluation_comparison_{self.timestamp}.csv"
            df = pd.DataFrame(evaluation_data)
            df.to_csv(csv_file, index=False, encoding='utf-8')
            
            # ä¿å­˜ç®€åŒ–çš„æŒ‡æ ‡æŠ¥å‘Š
            metrics_file = self.output_folder / f"llava_metrics_summary_{self.timestamp}.json"
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ è¯„ä¼°ç»“æœä¿å­˜å®Œæˆ:")
            logger.info(f"  è¯¦ç»†ç»“æœ: {detailed_file}")
            logger.info(f"  å¯¹æ¯”CSV: {csv_file}")
            logger.info(f"  æŒ‡æ ‡æ€»ç»“: {metrics_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}")
    
    def _generate_visualizations(self, evaluation_data: List[Dict], metrics: Dict):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'LLaVA Ghost Probing Detection Evaluation Results\n{self.timestamp}', fontsize=16)
            
            # 1. æ··æ·†çŸ©é˜µ
            cm = np.array(metrics['confusion_matrix']['matrix'])
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Normal', 'Ghost Probing'],
                       yticklabels=['Normal', 'Ghost Probing'],
                       ax=axes[0, 0])
            axes[0, 0].set_title('Confusion Matrix')
            axes[0, 0].set_xlabel('Predicted')
            axes[0, 0].set_ylabel('Actual')
            
            # 2. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
            metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
            metrics_values = [
                metrics['performance_metrics']['accuracy'],
                metrics['performance_metrics']['precision'],
                metrics['performance_metrics']['recall'],
                metrics['performance_metrics']['f1_score']
            ]
            
            bars = axes[0, 1].bar(metrics_names, metrics_values, color=['skyblue', 'lightgreen', 'orange', 'lightcoral'])
            axes[0, 1].set_title('Performance Metrics')
            axes[0, 1].set_ylabel('Score')
            axes[0, 1].set_ylim(0, 1)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, metrics_values):
                axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                               f'{value:.3f}', ha='center', va='bottom')
            
            # 3. ä¸GPT-4.1çš„å¯¹æ¯”
            comparison_data = metrics['comparison_with_gpt41_balanced']
            models = ['LLaVA-Video', 'GPT-4.1 Balanced']
            f1_scores = [metrics['performance_metrics']['f1_score'], comparison_data['gpt41_f1_score']]
            recall_scores = [metrics['performance_metrics']['recall'], comparison_data['gpt41_recall']]
            precision_scores = [metrics['performance_metrics']['precision'], comparison_data['gpt41_precision']]
            
            x = np.arange(len(models))
            width = 0.25
            
            axes[1, 0].bar(x - width, f1_scores, width, label='F1 Score', color='lightblue')
            axes[1, 0].bar(x, recall_scores, width, label='Recall', color='lightgreen')
            axes[1, 0].bar(x + width, precision_scores, width, label='Precision', color='lightcoral')
            
            axes[1, 0].set_title('LLaVA vs GPT-4.1 Balanced Comparison')
            axes[1, 0].set_ylabel('Score')
            axes[1, 0].set_xticks(x)
            axes[1, 0].set_xticklabels(models)
            axes[1, 0].legend()
            axes[1, 0].set_ylim(0, 1)
            
            # 4. ç½®ä¿¡åº¦åˆ†å¸ƒ
            confidences = [item['confidence'] for item in evaluation_data]
            correct_confidences = [item['confidence'] for item in evaluation_data if item['correct']]
            incorrect_confidences = [item['confidence'] for item in evaluation_data if not item['correct']]
            
            axes[1, 1].hist([correct_confidences, incorrect_confidences], 
                           bins=20, alpha=0.7, label=['Correct', 'Incorrect'],
                           color=['green', 'red'])
            axes[1, 1].set_title('Confidence Distribution')
            axes[1, 1].set_xlabel('Confidence Score')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].legend()
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_file = self.output_folder / f"llava_evaluation_visualization_{self.timestamp}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨ä¿å­˜: {plot_file}")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {e}")
    
    def compare_with_other_models(self, llava_results: str, other_results: Dict[str, str]):
        """
        ä¸å…¶ä»–æ¨¡å‹ç»“æœè¿›è¡Œå¯¹æ¯”
        
        Args:
            llava_results: LLaVAç»“æœæ–‡ä»¶è·¯å¾„
            other_results: å…¶ä»–æ¨¡å‹ç»“æœæ–‡ä»¶è·¯å¾„å­—å…¸ {'model_name': 'result_file_path'}
        """
        logger.info("ğŸ”„ å¼€å§‹å¤šæ¨¡å‹å¯¹æ¯”åˆ†æ")
        
        try:
            # è¯„ä¼°LLaVA
            llava_metrics = self.evaluate_results(llava_results)
            
            # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„æŒ‡æ ‡
            all_metrics = {'LLaVA-Video': llava_metrics}
            
            for model_name, result_file in other_results.items():
                if os.path.exists(result_file):
                    metrics = self.evaluate_results(result_file)
                    all_metrics[model_name] = metrics
                else:
                    logger.warning(f"æ¨¡å‹{model_name}çš„ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {result_file}")
            
            # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
            self._generate_comparison_report(all_metrics)
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡å‹å¯¹æ¯”å¤±è´¥: {e}")
    
    def _generate_comparison_report(self, all_metrics: Dict):
        """ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š"""
        try:
            comparison_report = {
                'timestamp': self.timestamp,
                'models_compared': list(all_metrics.keys()),
                'comparison_metrics': {}
            }
            
            # æå–å…³é”®æŒ‡æ ‡è¿›è¡Œå¯¹æ¯”
            for model_name, metrics in all_metrics.items():
                if 'performance_metrics' in metrics:
                    comparison_report['comparison_metrics'][model_name] = {
                        'accuracy': metrics['performance_metrics']['accuracy'],
                        'precision': metrics['performance_metrics']['precision'],
                        'recall': metrics['performance_metrics']['recall'],
                        'f1_score': metrics['performance_metrics']['f1_score']
                    }
            
            # æ‰¾å‡ºæœ€ä½³æ€§èƒ½
            best_models = {}
            for metric in ['accuracy', 'precision', 'recall', 'f1_score']:
                best_score = 0
                best_model = None
                for model_name, metrics in comparison_report['comparison_metrics'].items():
                    if metrics[metric] > best_score:
                        best_score = metrics[metric]
                        best_model = model_name
                best_models[metric] = {'model': best_model, 'score': best_score}
            
            comparison_report['best_performing'] = best_models
            
            # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
            report_file = self.output_folder / f"multi_model_comparison_{self.timestamp}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(comparison_report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“Š å¤šæ¨¡å‹å¯¹æ¯”æŠ¥å‘Šä¿å­˜: {report_file}")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='LLaVAé¬¼æ¢å¤´æ£€æµ‹è¯„ä¼°')
    parser.add_argument('--llava-results', required=True,
                       help='LLaVAç»“æœJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--groundtruth-file',
                       default='/Users/wanmeng/repository/GPT4Video-cobra-auto/result/groundtruth_labels.csv',
                       help='Ground truthæ ‡ç­¾æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-folder',
                       default='/Users/wanmeng/repository/GPT4Video-cobra-auto/result/LlaVA/evaluation_results',
                       help='è¯„ä¼°ç»“æœè¾“å‡ºæ–‡ä»¶å¤¹')
    parser.add_argument('--compare-with', nargs='*',
                       help='å…¶ä»–æ¨¡å‹ç»“æœæ–‡ä»¶è·¯å¾„åˆ—è¡¨')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = LLaVAGhostProbingEvaluator(
        groundtruth_file=args.groundtruth_file,
        output_folder=args.output_folder
    )
    
    # è¯„ä¼°LLaVAç»“æœ
    metrics = evaluator.evaluate_results(args.llava_results)
    
    if metrics:
        print("="*60)
        print("LLaVA Ghost Probing Detection Evaluation Results")
        print("="*60)
        print(f"å‡†ç¡®ç‡: {metrics['performance_metrics']['accuracy']:.4f}")
        print(f"ç²¾ç¡®ç‡: {metrics['performance_metrics']['precision']:.4f}")
        print(f"å¬å›ç‡: {metrics['performance_metrics']['recall']:.4f}")
        print(f"F1åˆ†æ•°: {metrics['performance_metrics']['f1_score']:.4f}")
        print("="*60)
        
        # ä¸GPT-4.1å¯¹æ¯”
        comparison = metrics['comparison_with_gpt41_balanced']
        print("ä¸GPT-4.1 Balancedå¯¹æ¯”:")
        print(f"F1åˆ†æ•°å·®å¼‚: {comparison['llava_vs_gpt41_f1_diff']:+.4f}")
        print(f"å¬å›ç‡å·®å¼‚: {comparison['llava_vs_gpt41_recall_diff']:+.4f}")
        print(f"ç²¾ç¡®ç‡å·®å¼‚: {comparison['llava_vs_gpt41_precision_diff']:+.4f}")
        print("="*60)
    
    # å¤šæ¨¡å‹å¯¹æ¯”ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.compare_with:
        other_results = {}
        for i, result_file in enumerate(args.compare_with):
            model_name = f"Model_{i+1}"
            other_results[model_name] = result_file
        evaluator.compare_with_other_models(args.llava_results, other_results)

if __name__ == "__main__":
    main()