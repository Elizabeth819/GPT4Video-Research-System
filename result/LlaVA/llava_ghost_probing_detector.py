#!/usr/bin/env python3
"""
LLaVA-NeXT Ghost Probing Detection Script
åŸºäºLLaVA-Video-7B-Qwen2æ¨¡å‹çš„é¬¼æ¢å¤´è§†é¢‘æ‰“æ ‡ç³»ç»Ÿ
ä½¿ç”¨ä¸GPT-4.1ç›¸åŒçš„å¹³è¡¡æç¤ºè¯ä»¥ç¡®ä¿è¯„ä¼°ä¸€è‡´æ€§
æ–‡ä»¶è·¯å¾„: /Users/wanmeng/repository/GPT4Video-cobra-auto/result/LlaVA/llava_ghost_probing_detector.py
"""

import os
import sys
import json
import logging
import warnings
import torch
import numpy as np
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from pathlib import Path
import copy

# å¯¼å…¥LLaVA-NeXTç›¸å…³æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'LLaVA-NeXT'))
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX
from llava.conversation import conv_templates, SeparatorStyle
from PIL import Image
from decord import VideoReader, cpu

warnings.filterwarnings("ignore")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llava_ghost_probing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LLaVAGhostProbingDetector:
    """LLaVA-NeXTé¬¼æ¢å¤´æ£€æµ‹å™¨"""
    
    def __init__(self, 
                 model_name: str = "lmms-lab/LLaVA-Video-7B-Qwen2",
                 max_frames: int = 64,
                 device: str = "cuda"):
        """
        åˆå§‹åŒ–LLaVAé¬¼æ¢å¤´æ£€æµ‹å™¨
        
        Args:
            model_name: LLaVAæ¨¡å‹åç§°
            max_frames: æœ€å¤§å¸§æ•°
            device: è®¡ç®—è®¾å¤‡
        """
        self.model_name = model_name
        self.max_frames = max_frames
        self.device = device
        
        logger.info(f"åˆå§‹åŒ–LLaVA Ghost Probing Detector")
        logger.info(f"æ¨¡å‹: {model_name}")
        logger.info(f"æœ€å¤§å¸§æ•°: {max_frames}")
        logger.info(f"è®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½LLaVAæ¨¡å‹"""
        try:
            logger.info("æ­£åœ¨åŠ è½½LLaVAæ¨¡å‹...")
            
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            self.tokenizer, self.model, self.image_processor, self.max_length = load_pretrained_model(
                self.model_name, 
                None, 
                "llava_qwen", 
                torch_dtype="bfloat16", 
                device_map="auto"
            )
            
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # å¯¹è¯æ¨¡æ¿
            self.conv_template = "qwen_1_5"
            
            logger.info("âœ… LLaVAæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ LLaVAæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_video(self, video_path: str, fps: int = 1, force_sample: bool = True) -> Tuple[np.ndarray, str, float]:
        """
        åŠ è½½è§†é¢‘å¹¶æå–å…³é”®å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            fps: å¸§ç‡é‡‡æ ·é¢‘ç‡
            force_sample: æ˜¯å¦å¼ºåˆ¶é‡‡æ ·åˆ°max_framesæ•°é‡
            
        Returns:
            spare_frames: æå–çš„è§†é¢‘å¸§
            frame_time: å¸§æ—¶é—´å­—ç¬¦ä¸²
            video_time: è§†é¢‘æ€»æ—¶é•¿
        """
        try:
            if self.max_frames == 0:
                return np.zeros((1, 336, 336, 3)), "0.00s", 0.0
            
            # ä½¿ç”¨decordè¯»å–è§†é¢‘
            vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)
            total_frame_num = len(vr)
            video_time = total_frame_num / vr.get_avg_fps()
            
            # è®¡ç®—é‡‡æ ·é—´éš”
            sample_fps = round(vr.get_avg_fps() / fps)
            frame_idx = [i for i in range(0, len(vr), sample_fps)]
            frame_time = [i / vr.get_avg_fps() for i in frame_idx]
            
            # å¦‚æœå¸§æ•°è¶…è¿‡max_framesæˆ–å¼ºåˆ¶é‡‡æ ·ï¼Œè¿›è¡Œå‡åŒ€é‡‡æ ·
            if len(frame_idx) > self.max_frames or force_sample:
                uniform_sampled_frames = np.linspace(0, total_frame_num - 1, self.max_frames, dtype=int)
                frame_idx = uniform_sampled_frames.tolist()
                frame_time = [i / vr.get_avg_fps() for i in frame_idx]
            
            # æ ¼å¼åŒ–æ—¶é—´å­—ç¬¦ä¸²
            frame_time_str = ",".join([f"{i:.2f}s" for i in frame_time])
            
            # æå–å¸§
            spare_frames = vr.get_batch(frame_idx).asnumpy()
            
            logger.info(f"è§†é¢‘åŠ è½½å®Œæˆ: {len(spare_frames)}å¸§, æ€»æ—¶é•¿{video_time:.2f}ç§’")
            
            return spare_frames, frame_time_str, video_time
            
        except Exception as e:
            logger.error(f"è§†é¢‘åŠ è½½å¤±è´¥ {video_path}: {e}")
            raise
    
    def create_ghost_probing_prompt(self, 
                                  video_id: str, 
                                  frame_time_str: str, 
                                  video_time: float, 
                                  num_frames: int) -> str:
        """
        åˆ›å»ºé¬¼æ¢å¤´æ£€æµ‹æç¤ºè¯ - ä½¿ç”¨ä¸GPT-4.1ç›¸åŒçš„å¹³è¡¡æç¤ºè¯
        
        Args:
            video_id: è§†é¢‘ID
            frame_time_str: å¸§æ—¶é—´å­—ç¬¦ä¸²
            video_time: è§†é¢‘æ€»æ—¶é•¿
            num_frames: å¸§æ•°é‡
            
        Returns:
            æ ¼å¼åŒ–çš„æç¤ºè¯
        """
        
        # ğŸ”§ ä¸GPT-4.1å®Œå…¨ç›¸åŒçš„å¹³è¡¡æç¤ºè¯
        system_content = f"""You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

Your job is to take in as an input a transcription of {video_time:.1f} seconds of audio from a video,
as well as {num_frames} frames split evenly throughout {video_time:.1f} seconds.
You are to generate and provide a Current Action Summary of the video you are considering ({num_frames}
frames over {video_time:.1f} seconds), which is generated from your analysis of each frame ({num_frames} in total),
as well as the in-between audio, until we have a full action summary of the video.

IMPORTANT: For ghost probing detection, consider TWO categories:

**1. HIGH-CONFIDENCE Ghost Probing (use "ghost probing" in key_actions)**:
- Object appears EXTREMELY close (within 1-2 vehicle lengths, <3 meters) 
- Appearance is SUDDEN and from blind spots (behind parked cars, buildings, corners)
- Occurs in HIGH-RISK environments: highways, rural roads, parking lots, uncontrolled intersections
- Requires IMMEDIATE emergency braking/swerving to avoid collision
- Movement is COMPLETELY UNPREDICTABLE and violates traffic expectations

**2. POTENTIAL Ghost Probing (use "potential ghost probing" in key_actions)**:
- Object appears suddenly but at moderate distance (3-5 meters)
- Sudden movement in environments where some unpredictability exists
- Requires emergency braking but collision risk is moderate
- Movement is unexpected but not completely impossible given the context

**3. NORMAL Traffic Situations (do NOT use "ghost probing")**:
- Pedestrians crossing at intersections, crosswalks, or traffic lights
- Vehicles making normal lane changes, turns, or merging with signals
- Cyclists following predictable paths in urban areas or bike lanes
- Any movement that is EXPECTED given the traffic environment and context

**Environment Context Guidelines**:
- INTERSECTION/CROSSWALK: Expect pedestrians and cyclists - use "emergency braking due to pedestrian crossing"
- HIGHWAY/RURAL: Higher chance of genuine ghost probing - be more sensitive
- PARKING LOT: Expect sudden vehicle movements - use "potential ghost probing" if very sudden
- URBAN STREET: Mixed - consider visibility and predictability

Use "ghost probing" for clear cases, "potential ghost probing" for borderline cases, and descriptive terms for normal traffic situations.

Your response should be a valid JSON object with the following EXACT structure (match this format precisely):
{{
    "video_id": "{video_id}",
    "segment_id": "Segment_001",
    "Start_Timestamp": "0.0s",
    "End_Timestamp": "{video_time:.1f}s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene",
    "summary": "comprehensive summary of the scene and what happens",
    "actions": "actions taken by the vehicle and driver responses",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "brief description of most important actions (use 'ghost probing', 'potential ghost probing', or descriptive terms as appropriate)",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

The video lasts for {video_time:.2f} seconds, and {num_frames} frames are uniformly sampled from it. These frames are located at {frame_time_str}. Please answer the following questions related to this video.

Audio Transcription: No audio available for this analysis.
"""
        
        return system_content
    
    def analyze_video(self, video_path: str, video_id: Optional[str] = None) -> Optional[Dict]:
        """
        åˆ†æå•ä¸ªè§†é¢‘è¿›è¡Œé¬¼æ¢å¤´æ£€æµ‹
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            video_id: è§†é¢‘IDï¼Œå¦‚æœä¸ºNoneåˆ™ä»æ–‡ä»¶åæå–
            
        Returns:
            åˆ†æç»“æœå­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # æå–è§†é¢‘ID
            if video_id is None:
                video_id = Path(video_path).stem
            
            logger.info(f"ğŸ¬ å¼€å§‹åˆ†æè§†é¢‘: {video_id}")
            
            # 1. åŠ è½½è§†é¢‘å¸§
            video_frames, frame_time_str, video_time = self.load_video(video_path)
            
            # 2. é¢„å¤„ç†è§†é¢‘å¸§
            video_tensor = self.image_processor.preprocess(video_frames, return_tensors="pt")["pixel_values"].to(self.device).bfloat16()
            video_list = [video_tensor]
            
            # 3. åˆ›å»ºæç¤ºè¯
            prompt_text = self.create_ghost_probing_prompt(
                video_id, frame_time_str, video_time, len(video_frames)
            )
            
            # 4. æ„å»ºå¯¹è¯
            question = DEFAULT_IMAGE_TOKEN + f"{prompt_text}\nPlease analyze this video for ghost probing detection."
            conv = copy.deepcopy(conv_templates[self.conv_template])
            conv.append_message(conv.roles[0], question)
            conv.append_message(conv.roles[1], None)
            prompt_question = conv.get_prompt()
            
            # 5. TokenåŒ–
            input_ids = tokenizer_image_token(
                prompt_question, 
                self.tokenizer, 
                IMAGE_TOKEN_INDEX, 
                return_tensors="pt"
            ).unsqueeze(0).to(self.device)
            
            # 6. ç”Ÿæˆå›å¤
            logger.info("ğŸ§  æ­£åœ¨è¿›è¡ŒLLaVAæ¨ç†...")
            with torch.no_grad():
                output = self.model.generate(
                    input_ids,
                    images=video_list,
                    modalities=["video"],
                    do_sample=False,
                    temperature=0,
                    max_new_tokens=4096,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # 7. è§£ç è¾“å‡º
            text_output = self.tokenizer.batch_decode(output, skip_special_tokens=True)[0].strip()
            
            # 8. æå–JSONç»“æœ
            json_result = self._extract_json_from_response(text_output)
            
            if json_result:
                logger.info(f"âœ… è§†é¢‘åˆ†ææˆåŠŸ: {video_id}")
                return json_result
            else:
                logger.error(f"âŒ æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆJSON: {video_id}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘åˆ†æå¤±è´¥ {video_path}: {e}")
            return None
    
    def _extract_json_from_response(self, response: str) -> Optional[Dict]:
        """
        ä»LLaVAå“åº”ä¸­æå–JSONç»“æœ
        
        Args:
            response: LLaVAåŸå§‹å“åº”
            
        Returns:
            è§£æçš„JSONå­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸä½ç½®
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx+1]
                
                # å°è¯•è§£æJSON
                result = json.loads(json_str)
                return result
            else:
                logger.warning("å“åº”ä¸­æœªæ‰¾åˆ°å®Œæ•´çš„JSONç»“æ„")
                return None
                
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            logger.debug(f"åŸå§‹å“åº”: {response}")
            return None
        except Exception as e:
            logger.error(f"JSONæå–å¤±è´¥: {e}")
            return None
    
    def extract_ghost_probing_label(self, analysis_result: Dict) -> Tuple[str, float]:
        """
        ä»åˆ†æç»“æœä¸­æå–é¬¼æ¢å¤´æ ‡ç­¾
        
        Args:
            analysis_result: LLaVAåˆ†æç»“æœ
            
        Returns:
            (æ ‡ç­¾, ç½®ä¿¡åº¦) - æ ‡ç­¾ä¸º"ghost_probing", "potential_ghost_probing", æˆ–"normal"
        """
        try:
            key_actions = analysis_result.get("key_actions", "").lower()
            
            # æ ¹æ®å…³é”®åŠ¨ä½œåˆ¤æ–­ç±»åˆ«
            if "ghost probing" in key_actions and "potential" not in key_actions:
                return "ghost_probing", 0.9
            elif "potential ghost probing" in key_actions:
                return "potential_ghost_probing", 0.7
            else:
                return "normal", 0.8
                
        except Exception as e:
            logger.error(f"æ ‡ç­¾æå–å¤±è´¥: {e}")
            return "normal", 0.5

def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    # æµ‹è¯•å•ä¸ªè§†é¢‘
    detector = LLaVAGhostProbingDetector()
    
    # æµ‹è¯•è§†é¢‘è·¯å¾„
    test_video = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos/images_1_001.avi"
    
    if os.path.exists(test_video):
        result = detector.analyze_video(test_video)
        if result:
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
            # æå–é¬¼æ¢å¤´æ ‡ç­¾
            label, confidence = detector.extract_ghost_probing_label(result)
            print(f"\né¬¼æ¢å¤´æ£€æµ‹ç»“æœ: {label} (ç½®ä¿¡åº¦: {confidence})")
        else:
            print("âŒ è§†é¢‘åˆ†æå¤±è´¥")
    else:
        print(f"âŒ æµ‹è¯•è§†é¢‘ä¸å­˜åœ¨: {test_video}")

if __name__ == "__main__":
    main()