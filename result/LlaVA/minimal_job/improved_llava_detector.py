#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆLLaVAé¬¼æ¢å¤´æ£€æµ‹å™¨
1. é™ä½æ£€æµ‹é˜ˆå€¼è‡³0.15
2. å¢åŠ æ—¶åºåˆ†æè¯†åˆ«çªç„¶å‡ºç°çš„ç‰©ä½“
"""

import json
import os
import gc
from pathlib import Path
from datetime import datetime
from PIL import Image
import logging
import torch
from typing import List, Dict, Tuple
import numpy as np
import time
import hashlib

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ImprovedLLaVADetector:
    """æ”¹è¿›ç‰ˆLLaVAé¬¼æ¢å¤´æ£€æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processed_videos = []
        
        # æ”¹è¿›çš„æ£€æµ‹é˜ˆå€¼ - ä»0.3é™è‡³0.15
        self.detection_thresholds = {
            'sudden_appearance': 0.15,  # çªç„¶å‡ºç°æ£€æµ‹é˜ˆå€¼
            'high_confidence': 0.20,    # é«˜ç½®ä¿¡åº¦é˜ˆå€¼
            'temporal_pattern': 0.12    # æ—¶åºæ¨¡å¼é˜ˆå€¼
        }
        
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ”¹è¿›ç‰ˆæ¨¡å‹...")
            logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
            logger.info(f"ğŸ¯ æ–°æ£€æµ‹é˜ˆå€¼: {self.detection_thresholds}")
            
            # ä½¿ç”¨CLIPè¿›è¡Œå›¾åƒç¼–ç 
            from transformers import CLIPProcessor, CLIPModel
            
            logger.info("ğŸ“¥ åŠ è½½CLIPæ¨¡å‹...")
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            
            # ç§»åˆ°GPU
            if torch.cuda.is_available():
                self.clip_model = self.clip_model.cuda()
                logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU: {torch.cuda.get_device_name()}")
            else:
                logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def extract_frames_enhanced(self, video_path: str, num_frames: int = 16) -> List[Image.Image]:
        """å¢å¼ºç‰ˆå¸§æå– - å¢åŠ å¸§æ•°ä»¥ä¾¿æ›´å¥½çš„æ—¶åºåˆ†æ"""
        
        logger.info(f"ğŸ¬ å¤„ç†è§†é¢‘: {Path(video_path).name}")
        overall_start = time.time()
        
        # éªŒè¯æ–‡ä»¶
        if not Path(video_path).exists():
            raise ValueError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        file_size = Path(video_path).stat().st_size
        logger.info(f"ğŸ“‚ æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB")
        
        try:
            import decord
            from decord import VideoReader
            
            # è®¾ç½®decord
            decord.bridge.set_bridge('native')
            
            # è¯»å–è§†é¢‘
            video_reader = VideoReader(str(video_path))
            total_frames = len(video_reader)
            
            logger.info(f"ğŸ“Š æ€»å¸§æ•°: {total_frames}")
            
            if total_frames == 0:
                raise ValueError(f"è§†é¢‘æ–‡ä»¶æ²¡æœ‰å¸§: {video_path}")
            
            # è·å–è§†é¢‘ä¿¡æ¯
            try:
                fps = video_reader.get_avg_fps()
                duration = total_frames / fps if fps > 0 else 0
                logger.info(f"ğŸ“Š å¸§ç‡: {fps:.2f} fps, æ—¶é•¿: {duration:.2f}ç§’")
            except Exception as e:
                logger.warning(f"âš ï¸  æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {e}")
            
            # å‡åŒ€åˆ†å¸ƒé€‰æ‹©å¸§
            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
            logger.info(f"ğŸ“Š é€‰æ‹©{num_frames}å¸§ç”¨äºæ—¶åºåˆ†æ: {frame_indices[:5].tolist()}...{frame_indices[-3:].tolist()}")
            
            # æå–å¸§
            frames = []
            for i, idx in enumerate(frame_indices):
                frame = video_reader[idx]
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                if hasattr(frame, 'asnumpy'):
                    frame_array = frame.asnumpy()
                elif isinstance(frame, torch.Tensor):
                    frame_array = frame.cpu().numpy()
                else:
                    frame_array = np.array(frame)
                
                # è½¬æ¢ä¸ºPIL Image
                pil_image = Image.fromarray(frame_array.astype(np.uint8))
                frames.append(pil_image)
                
                # æ¸…ç†ä¸´æ—¶å¯¹è±¡
                del frame, frame_array
            
            overall_time = time.time() - overall_start
            logger.info(f"âœ… æå–å®Œæˆ: {len(frames)}å¸§, ç”¨æ—¶{overall_time:.2f}ç§’")
            
            # è®°å½•å¤„ç†ä¿¡æ¯
            self.processed_videos.append({
                'video_path': video_path,
                'file_size_mb': file_size / 1024 / 1024,
                'total_frames': total_frames,
                'extracted_frames': len(frames),
                'extraction_time': overall_time
            })
            
            return frames
            
        except Exception as e:
            overall_time = time.time() - overall_start
            logger.error(f"âŒ å¸§æå–å¤±è´¥ ({overall_time:.2f}ç§’): {e}")
            raise
    
    def analyze_temporal_patterns(self, image_features: torch.Tensor) -> Dict:
        """åˆ†ææ—¶åºæ¨¡å¼ - è¯†åˆ«çªç„¶å‡ºç°çš„ç‰©ä½“"""
        
        logger.info("ğŸ• å¼€å§‹æ—¶åºæ¨¡å¼åˆ†æ...")
        
        # è®¡ç®—è¿ç»­å¸§é—´çš„ç‰¹å¾å˜åŒ–
        frame_changes = []
        for i in range(1, len(image_features)):
            diff = torch.cosine_similarity(
                image_features[i-1].unsqueeze(0),
                image_features[i].unsqueeze(0)
            ).item()
            change = 1 - diff
            frame_changes.append(change)
        
        frame_changes = np.array(frame_changes)
        
        # è®¡ç®—æ—¶åºç»Ÿè®¡ç‰¹å¾
        max_change = np.max(frame_changes)
        mean_change = np.mean(frame_changes)
        std_change = np.std(frame_changes)
        
        # æ£€æµ‹çªç„¶å˜åŒ–ï¼ˆå¼‚å¸¸å€¼æ£€æµ‹ï¼‰
        threshold = mean_change + 2 * std_change  # 2ä¸ªæ ‡å‡†å·®é˜ˆå€¼
        sudden_changes = frame_changes > threshold
        sudden_change_count = np.sum(sudden_changes)
        
        # æ£€æµ‹è¿ç»­å˜åŒ–æ¨¡å¼
        # å¯»æ‰¾è¿ç»­çš„é«˜å˜åŒ–åŒºåŸŸï¼ˆå¯èƒ½è¡¨ç¤ºç‰©ä½“å‡ºç°è¿‡ç¨‹ï¼‰
        high_change_mask = frame_changes > self.detection_thresholds['temporal_pattern']
        continuous_regions = self._find_continuous_regions(high_change_mask)
        
        # åˆ†æå˜åŒ–çš„æ¢¯åº¦ï¼ˆçªç„¶æ€§ï¼‰
        if len(frame_changes) > 1:
            change_gradient = np.gradient(frame_changes)
            max_gradient = np.max(np.abs(change_gradient))
            gradient_peaks = np.where(np.abs(change_gradient) > np.std(change_gradient) * 1.5)[0]
        else:
            max_gradient = 0
            gradient_peaks = []
        
        temporal_analysis = {
            'frame_changes': frame_changes.tolist(),
            'max_change': float(max_change),
            'mean_change': float(mean_change),
            'std_change': float(std_change),
            'sudden_change_threshold': float(threshold),
            'sudden_change_count': int(sudden_change_count),
            'sudden_change_indices': np.where(sudden_changes)[0].tolist(),
            'continuous_change_regions': continuous_regions,
            'max_gradient': float(max_gradient),
            'gradient_peaks': gradient_peaks.tolist(),
            'high_change_frames': np.where(high_change_mask)[0].tolist()
        }
        
        logger.info(f"ğŸ“Š æœ€å¤§å˜åŒ–: {max_change:.4f}, å¹³å‡å˜åŒ–: {mean_change:.4f}")
        logger.info(f"ğŸ“Š çªç„¶å˜åŒ–æ¬¡æ•°: {sudden_change_count}, è¿ç»­å˜åŒ–åŒºåŸŸ: {len(continuous_regions)}")
        logger.info(f"ğŸ“Š æœ€å¤§æ¢¯åº¦: {max_gradient:.4f}")
        
        return temporal_analysis
    
    def _find_continuous_regions(self, mask: np.ndarray) -> List[Tuple[int, int]]:
        """æ‰¾å‡ºè¿ç»­çš„TrueåŒºåŸŸ"""
        regions = []
        start = None
        
        for i, value in enumerate(mask):
            if value and start is None:
                start = i
            elif not value and start is not None:
                regions.append((start, i - 1))
                start = None
        
        # å¤„ç†åˆ°ç»“å°¾çš„æƒ…å†µ
        if start is not None:
            regions.append((start, len(mask) - 1))
        
        return regions
    
    def enhanced_ghost_detection(self, temporal_analysis: Dict) -> Dict:
        """å¢å¼ºç‰ˆé¬¼æ¢å¤´æ£€æµ‹é€»è¾‘"""
        
        max_change = temporal_analysis['max_change']
        mean_change = temporal_analysis['mean_change']
        sudden_change_count = temporal_analysis['sudden_change_count']
        continuous_regions = temporal_analysis['continuous_change_regions']
        max_gradient = temporal_analysis['max_gradient']
        
        # å¤šç»´åº¦æ£€æµ‹é€»è¾‘
        detection_scores = {
            'sudden_appearance': 0.0,
            'temporal_consistency': 0.0,
            'gradient_analysis': 0.0,
            'overall_confidence': 0.0
        }
        
        # 1. çªç„¶å‡ºç°æ£€æµ‹ï¼ˆé™ä½é˜ˆå€¼ï¼‰
        if max_change > self.detection_thresholds['sudden_appearance']:
            detection_scores['sudden_appearance'] = min(1.0, max_change / self.detection_thresholds['high_confidence'])
        
        # 2. æ—¶åºä¸€è‡´æ€§æ£€æµ‹
        if sudden_change_count > 0:
            # æœ‰çªç„¶å˜åŒ–ï¼Œå¢åŠ ç½®ä¿¡åº¦
            detection_scores['temporal_consistency'] = min(1.0, sudden_change_count / 3.0)
        
        # 3. æ¢¯åº¦åˆ†æ - æ£€æµ‹æ€¥å‰§å˜åŒ–
        if max_gradient > 0.05:  # æ¢¯åº¦é˜ˆå€¼
            detection_scores['gradient_analysis'] = min(1.0, max_gradient / 0.15)
        
        # 4. è¿ç»­å˜åŒ–åŒºåŸŸåˆ†æ
        if continuous_regions:
            # çŸ­è€Œé›†ä¸­çš„å˜åŒ–åŒºåŸŸå¯èƒ½è¡¨ç¤ºç‰©ä½“çªç„¶å‡ºç°
            region_lengths = [end - start + 1 for start, end in continuous_regions]
            avg_region_length = np.mean(region_lengths)
            
            # åå¥½è¾ƒçŸ­çš„è¿ç»­å˜åŒ–ï¼ˆçªç„¶å‡ºç°ç‰¹å¾ï¼‰
            if avg_region_length < 4:  # çŸ­å˜åŒ–åŒºåŸŸ
                detection_scores['temporal_consistency'] += 0.3
        
        # ç»¼åˆç½®ä¿¡åº¦è®¡ç®—
        detection_scores['overall_confidence'] = (
            detection_scores['sudden_appearance'] * 0.4 +
            detection_scores['temporal_consistency'] * 0.3 +
            detection_scores['gradient_analysis'] * 0.3
        )
        
        # å†³ç­–é€»è¾‘ï¼ˆé™ä½æ£€æµ‹é˜ˆå€¼ï¼‰
        if detection_scores['overall_confidence'] > 0.6:
            ghost_detected = "yes"
            confidence = min(0.95, detection_scores['overall_confidence'])
            ghost_type = "high_confidence"
            risk_level = "high"
        elif detection_scores['overall_confidence'] > 0.4:
            ghost_detected = "yes"
            confidence = detection_scores['overall_confidence']
            ghost_type = "potential"
            risk_level = "medium"
        elif max_change > self.detection_thresholds['temporal_pattern']:
            ghost_detected = "yes"
            confidence = max(0.3, detection_scores['overall_confidence'])
            ghost_type = "low_confidence"
            risk_level = "low"
        else:
            ghost_detected = "no"
            confidence = 0.2
            ghost_type = "none"
            risk_level = "low"
        
        result = {
            "ghost_probing_detected": ghost_detected,
            "confidence": round(confidence, 3),
            "ghost_type": ghost_type,
            "risk_level": risk_level,
            "detection_scores": detection_scores,
            "detection_reasoning": {
                "max_change_vs_threshold": f"{max_change:.4f} vs {self.detection_thresholds['sudden_appearance']}",
                "sudden_changes": sudden_change_count,
                "continuous_regions": len(continuous_regions),
                "max_gradient": max_gradient
            }
        }
        
        logger.info(f"ğŸ¯ æ£€æµ‹ç»“æœ: {ghost_detected} (ç½®ä¿¡åº¦: {confidence:.3f})")
        logger.info(f"ğŸ¯ æ£€æµ‹ç±»å‹: {ghost_type}, é£é™©çº§åˆ«: {risk_level}")
        
        return result
    
    def analyze_frames_enhanced(self, frames: List[Image.Image]) -> Dict:
        """å¢å¼ºç‰ˆå¸§åˆ†æ"""
        if not frames:
            raise ValueError("æ²¡æœ‰å¸§å¯åˆ†æ")
        
        try:
            logger.info(f"ğŸ” å¼€å§‹å¢å¼ºåˆ†æ{len(frames)}å¸§...")
            analysis_start = time.time()
            
            # ä½¿ç”¨CLIPæå–å›¾åƒç‰¹å¾
            inputs = self.clip_processor(images=frames, return_tensors="pt", padding=True)
            
            if torch.cuda.is_available():
                inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            with torch.no_grad():
                image_features = self.clip_model.get_image_features(**inputs)
            
            # æ—¶åºæ¨¡å¼åˆ†æ
            temporal_analysis = self.analyze_temporal_patterns(image_features)
            
            # å¢å¼ºç‰ˆé¬¼æ¢å¤´æ£€æµ‹
            detection_result = self.enhanced_ghost_detection(temporal_analysis)
            
            analysis_time = time.time() - analysis_start
            logger.info(f"ğŸ§  å¢å¼ºåˆ†æå®Œæˆ: {analysis_time:.4f}ç§’")
            
            # åˆå¹¶ç»“æœ
            result = {
                **detection_result,
                "summary": f"å¢å¼ºåˆ†æå®Œæˆï¼Œæœ€å¤§å¸§é—´å˜åŒ–: {temporal_analysis['max_change']:.4f}",
                "key_actions": f"æ—¶åºåˆ†æ{len(frames)}å¸§ï¼Œé™ä½é˜ˆå€¼æ£€æµ‹",
                "emergency_action_needed": "yes" if detection_result["ghost_probing_detected"] == "yes" else "no",
                "temporal_analysis": temporal_analysis,
                "analysis_time": round(analysis_time, 4)
            }
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºåˆ†æå¤±è´¥: {e}")
            raise
    
    def process_video(self, video_path: str) -> Dict:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        video_name = Path(video_path).stem
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_name}")
        
        start_time = datetime.now()
        
        try:
            # å¢å¼ºç‰ˆå¸§æå–ï¼ˆæ›´å¤šå¸§æ•°ç”¨äºæ—¶åºåˆ†æï¼‰
            frames = self.extract_frames_enhanced(video_path, num_frames=16)
            
            if not frames:
                raise ValueError("æ— æ³•æå–è§†é¢‘å¸§")
            
            # å¢å¼ºç‰ˆåˆ†æ
            analysis_result = self.analyze_frames_enhanced(frames)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # æ„å»ºç»“æœ
            result = {
                "video_id": video_name,
                "video_path": str(video_path),
                "ghost_probing_label": analysis_result.get("ghost_probing_detected", "no"),
                "confidence": analysis_result.get("confidence", 0.5),
                "ghost_type": analysis_result.get("ghost_type", "none"),
                "summary": analysis_result.get("summary", ""),
                "key_actions": analysis_result.get("key_actions", ""),
                "risk_level": analysis_result.get("risk_level", "low"),
                "emergency_action_needed": analysis_result.get("emergency_action_needed", "no"),
                "model": "CLIP-Enhanced-Temporal",
                "timestamp": datetime.now().isoformat(),
                "processing_time": processing_time,
                "frames_analyzed": len(frames),
                "device": self.device,
                "detection_scores": analysis_result.get("detection_scores", {}),
                "detection_reasoning": analysis_result.get("detection_reasoning", {}),
                "temporal_analysis": analysis_result.get("temporal_analysis", {}),
                "analysis_time": analysis_result.get("analysis_time", 0),
                "thresholds_used": self.detection_thresholds
            }
            
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {video_name} ({processing_time:.2f}s)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {video_name} - {e}")
            return {
                "video_id": video_name,
                "video_path": str(video_path),
                "error": str(e),
                "processing_time": (datetime.now() - start_time).total_seconds()
            }

def main():
    """æ”¹è¿›ç‰ˆä¸»å‡½æ•°"""
    print("ğŸš€ æ”¹è¿›ç‰ˆLLaVAé¬¼æ¢å¤´æ£€æµ‹ (é™ä½é˜ˆå€¼ + æ—¶åºåˆ†æ)")
    print("=" * 70)
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    
    # è·å–è§†é¢‘è·¯å¾„
    azureml_data_path = os.environ.get('AZUREML_DATAREFERENCE_video_data')
    
    possible_paths = []
    if azureml_data_path:
        possible_paths.append(azureml_data_path)
        print(f"ğŸ”§ ä»ç¯å¢ƒå˜é‡æ‰¾åˆ°æ•°æ®è·¯å¾„: {azureml_data_path}")
    
    possible_paths.extend(["./inputs/video_data", "./inputs", "."])
    
    video_files = []
    for path in possible_paths:
        try:
            p = Path(path)
            if p.exists():
                found_videos = list(p.glob("**/*.avi"))
                if found_videos:
                    # å¤„ç†å‰10ä¸ªè§†é¢‘è¿›è¡Œæµ‹è¯•
                    video_files = found_videos[:10]
                    print(f"âœ… åœ¨ {path} æ‰¾åˆ° {len(found_videos)} ä¸ªè§†é¢‘ï¼Œå¤„ç†å‰ {len(video_files)} ä¸ª")
                    break
        except Exception as e:
            print(f"âŒ æ£€æŸ¥è·¯å¾„ {path} æ—¶å‡ºé”™: {e}")
    
    if not video_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("./outputs/results", exist_ok=True)
    
    # åˆå§‹åŒ–æ”¹è¿›ç‰ˆæ£€æµ‹å™¨
    detector = ImprovedLLaVADetector()
    
    print(f"ğŸ¬ å¼€å§‹æ”¹è¿›ç‰ˆå¤„ç† {len(video_files)} ä¸ªè§†é¢‘...")
    print("=" * 70)
    
    # å¤„ç†è§†é¢‘
    results = []
    failed_count = 0
    
    for i, video_file in enumerate(video_files):
        try:
            print(f"\nğŸ“¹ å¤„ç†è§†é¢‘ {i+1}/{len(video_files)}: {Path(video_file).name}")
            result = detector.process_video(str(video_file))
            results.append(result)
            
            if 'error' in result:
                failed_count += 1
                print(f"âŒ å¤„ç†å¤±è´¥: {result['error']}")
            else:
                ghost_detected = result.get('ghost_probing_label', 'no')
                confidence = result.get('confidence', 0)
                print(f"âœ… æ£€æµ‹ç»“æœ: {ghost_detected} (ç½®ä¿¡åº¦: {confidence:.3f})")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è§†é¢‘å¤±è´¥: {e}")
            failed_count += 1
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜æ”¹è¿›ç‰ˆæ£€æµ‹ç»“æœ...")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    successful_results = [r for r in results if 'error' not in r]
    
    # ç»Ÿè®¡æ£€æµ‹ç»“æœ
    detected_count = sum(1 for r in successful_results if r.get('ghost_probing_label') == 'yes')
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    improved_result = {
        'metadata': {
            'model': 'CLIP-Enhanced-Temporal',
            'device': 'GPU' if torch.cuda.is_available() else 'CPU',
            'total_videos': len(video_files),
            'successful_videos': len(successful_results),
            'failed_videos': failed_count,
            'detected_ghost_probing': detected_count,
            'timestamp': timestamp,
            'improvements': [
                'Lowered detection threshold from 0.3 to 0.15',
                'Added temporal pattern analysis',
                'Increased frame count to 16 for better temporal analysis',
                'Multi-dimensional detection scoring'
            ]
        },
        'processed_videos_details': detector.processed_videos,
        'results': results
    }
    
    json_file = f"./outputs/results/improved_llava_results_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(improved_result, f, indent=2, ensure_ascii=False)
    
    # ç»Ÿè®¡
    if successful_results:
        avg_time = sum(r['processing_time'] for r in successful_results) / len(successful_results)
        avg_extraction_time = sum(v['extraction_time'] for v in detector.processed_videos) / len(detector.processed_videos)
    else:
        avg_time = avg_extraction_time = 0
    
    print("=" * 70)
    print("ğŸ‰ æ”¹è¿›ç‰ˆæ£€æµ‹å®Œæˆ!")
    print("=" * 70)
    print(f"ğŸ“Š æµ‹è¯•è§†é¢‘æ•°: {len(video_files)}")
    print(f"âœ… æˆåŠŸå¤„ç†: {len(successful_results)}")
    print(f"ğŸ¯ æ£€æµ‹åˆ°é¬¼æ¢å¤´: {detected_count}")
    print(f"âŒ å¤„ç†å¤±è´¥: {failed_count}")
    if successful_results:
        print(f"â±ï¸  å¹³å‡æ€»æ—¶é—´: {avg_time:.2f}ç§’/è§†é¢‘")
        print(f"â±ï¸  å¹³å‡æŠ½å¸§æ—¶é—´: {avg_extraction_time:.2f}ç§’/è§†é¢‘")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {json_file}")
    print("=" * 70)

if __name__ == "__main__":
    main()