#!/usr/bin/env python3
"""
è¯„ä¼°GPT-4.1å¹³è¡¡ç‰ˆ100è§†é¢‘æ£€æµ‹ç»“æœ
å¯¹æ¯”ground truthè®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡
è¾“å‡ºæ ¼å¼ä¸GPT-4.1ä¿æŒä¸€è‡´
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
from datetime import datetime

def load_ground_truth_labels(csv_path: str) -> Dict[str, bool]:
    """åŠ è½½ground truthæ ‡ç­¾"""
    try:
        df = pd.read_csv(csv_path, sep='\t')
        print(f"ğŸ“Š è¯»å–ground truthæ–‡ä»¶: {csv_path}")
        print(f"ğŸ“Š åˆ—å: {df.columns.tolist()}")
        
        gt_labels = {}
        ghost_count = 0
        
        for _, row in df.iterrows():
            video_id = str(row['video_id']).replace('.avi', '')
            label = str(row['ground_truth_label']).lower()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºé¬¼æ¢å¤´ (ä¸GPT-4.1è¯„ä¼°æ ‡å‡†ä¸€è‡´)
            has_ghost_probing = (
                'ghost probing' in label or 
                'ghost' in label or
                ('s:' in label and 'none' not in label and 'cut-in' not in label)
            )
            
            gt_labels[video_id] = has_ghost_probing
            if has_ghost_probing:
                ghost_count += 1
        
        print(f"ğŸ“Š åŠ è½½äº† {len(gt_labels)} ä¸ªground truthæ ‡ç­¾")
        print(f"ğŸ“Š Ground Truthåˆ†å¸ƒ:")
        print(f"   - é¬¼æ¢å¤´è§†é¢‘: {ghost_count}")
        print(f"   - æ­£å¸¸è§†é¢‘: {len(gt_labels) - ghost_count}")
        print(f"   - é¬¼æ¢å¤´æ¯”ä¾‹: {ghost_count/len(gt_labels)*100:.1f}%")
        
        return gt_labels
        
    except Exception as e:
        print(f"âŒ åŠ è½½ground truthå¤±è´¥: {e}")
        return {}

def load_gpt41_results(json_path: str) -> List[Dict]:
    """åŠ è½½GPT-4.1æ£€æµ‹ç»“æœ"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        results = data.get('results', [])
        metadata = data.get('metadata', {})
        
        print(f"ğŸ“Š åŠ è½½äº† {len(results)} ä¸ªGPT-4.1æ£€æµ‹ç»“æœ")
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: {metadata.get('model', 'Unknown')}")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {metadata.get('successful_videos', 0)}")
        print(f"ğŸ“Š å¤„ç†å¤±è´¥: {metadata.get('failed_videos', 0)}")
        
        return results
        
    except Exception as e:
        print(f"âŒ åŠ è½½GPT-4.1æ£€æµ‹ç»“æœå¤±è´¥: {e}")
        return []

def extract_ghost_probing_detection(result: Dict) -> Tuple[bool, bool, float]:
    """ä»GPT-4.1ç»“æœä¸­æå–é¬¼æ¢å¤´æ£€æµ‹ä¿¡æ¯"""
    
    key_actions = result.get('key_actions', '').lower()
    
    # æ£€æµ‹é¬¼æ¢å¤´å…³é”®è¯ (ä¸GPT-4.1æ ‡å‡†ä¸€è‡´)
    high_confidence_ghost = 'ghost probing' in key_actions and 'potential' not in key_actions
    potential_ghost = 'potential ghost probing' in key_actions
    
    # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
    confidence_score = 0.0
    if high_confidence_ghost:
        confidence_score = 0.9  # é«˜ç½®ä¿¡åº¦
    elif potential_ghost:
        confidence_score = 0.6  # ä¸­ç­‰ç½®ä¿¡åº¦
    elif any(keyword in key_actions for keyword in ['emergency', 'braking', 'sudden', 'avoid']):
        confidence_score = 0.3  # ä½ç½®ä¿¡åº¦
    else:
        confidence_score = 0.1  # åŸºçº¿ç½®ä¿¡åº¦
    
    # æ£€æµ‹é€»è¾‘: ä»»ä½•å½¢å¼çš„é¬¼æ¢å¤´éƒ½ç®—ä½œæ£€æµ‹åˆ°
    detected = high_confidence_ghost or potential_ghost
    
    return detected, high_confidence_ghost, confidence_score

def evaluate_gpt41_performance(gt_labels: Dict[str, bool], detection_results: List[Dict]) -> Dict:
    """è¯„ä¼°GPT-4.1æ£€æµ‹æ€§èƒ½ (ä¸GPT-4.1è¯„ä¼°æ–¹æ³•ä¸€è‡´)"""
    
    # å‡†å¤‡è¯„ä¼°æ•°æ®
    matched_results = []
    unmatched_videos = []
    
    for result in detection_results:
        video_id = result.get('video_id', '').replace('.avi', '')
        
        # è·³è¿‡å¤„ç†å¤±è´¥çš„è§†é¢‘
        if 'error' in result:
            continue
            
        if video_id in gt_labels:
            # æå–æ£€æµ‹ä¿¡æ¯
            detected, high_confidence, confidence = extract_ghost_probing_detection(result)
            
            # Ground truth
            ground_truth = gt_labels[video_id]
            
            matched_results.append({
                'video_id': video_id,
                'ground_truth': ground_truth,
                'detected': detected,
                'high_confidence': high_confidence,
                'confidence': confidence,
                'key_actions': result.get('key_actions', ''),
                'summary': result.get('summary', ''),
                'scene_theme': result.get('scene_theme', ''),
                'processing_time': result.get('processing_time', 0)
            })
        else:
            unmatched_videos.append(video_id)
    
    print(f"ğŸ“Š åŒ¹é…åˆ° {len(matched_results)} ä¸ªæœ‰ground truthçš„æ£€æµ‹ç»“æœ")
    if unmatched_videos:
        print(f"âš ï¸  {len(unmatched_videos)} ä¸ªè§†é¢‘æœªæ‰¾åˆ°ground truth")
    
    if not matched_results:
        return {"error": "æ²¡æœ‰åŒ¹é…çš„ç»“æœ"}
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tp = sum(1 for r in matched_results if r['ground_truth'] and r['detected'])      # True Positive
    tn = sum(1 for r in matched_results if not r['ground_truth'] and not r['detected'])  # True Negative  
    fp = sum(1 for r in matched_results if not r['ground_truth'] and r['detected'])      # False Positive
    fn = sum(1 for r in matched_results if r['ground_truth'] and not r['detected'])      # False Negative
    
    total = len(matched_results)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    accuracy = (tp + tn) / total if total > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # è¯¯æŠ¥ç‡ (False Positive Rate)
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    # åˆ†ææ£€æµ‹æ¡ˆä¾‹
    true_positives = [r for r in matched_results if r['ground_truth'] and r['detected']]
    false_positives = [r for r in matched_results if not r['ground_truth'] and r['detected']]
    false_negatives = [r for r in matched_results if r['ground_truth'] and not r['detected']]
    true_negatives = [r for r in matched_results if not r['ground_truth'] and not r['detected']]
    
    # é«˜ç½®ä¿¡åº¦æ£€æµ‹åˆ†æ
    high_confidence_tp = [r for r in true_positives if r['high_confidence']]
    high_confidence_fp = [r for r in false_positives if r['high_confidence']]
    
    # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
    avg_processing_time = np.mean([r['processing_time'] for r in matched_results if r['processing_time'] > 0])
    
    results = {
        'evaluation_summary': {
            'total_videos': total,
            'ground_truth_positives': tp + fn,
            'ground_truth_negatives': tn + fp,
            'detected_positives': tp + fp,
            'detected_negatives': tn + fn
        },
        'confusion_matrix': {
            'true_positive': tp,
            'true_negative': tn,
            'false_positive': fp,
            'false_negative': fn
        },
        'performance_metrics': {
            'accuracy': round(accuracy, 4),
            'precision': round(precision, 4),
            'recall': round(recall, 4),
            'specificity': round(specificity, 4),
            'f1_score': round(f1_score, 4),
            'false_positive_rate': round(fpr, 4)
        },
        'detailed_analysis': {
            'true_positives': true_positives,
            'false_positives': false_positives,
            'false_negatives': false_negatives,
            'true_negatives': true_negatives,
            'high_confidence_true_positives': high_confidence_tp,
            'high_confidence_false_positives': high_confidence_fp
        },
        'processing_stats': {
            'avg_processing_time': round(avg_processing_time, 2),
            'total_processing_time': round(sum(r['processing_time'] for r in matched_results), 2)
        },
        'detection_breakdown': {
            'total_ghost_detected': len([r for r in matched_results if r['detected']]),
            'high_confidence_ghost': len([r for r in matched_results if r['high_confidence']]),
            'potential_ghost': len([r for r in matched_results if r['detected'] and not r['high_confidence']])
        }
    }
    
    return results

def print_gpt41_evaluation_report(eval_results: Dict):
    """æ‰“å°GPT-4.1é£æ ¼çš„è¯„ä¼°æŠ¥å‘Š"""
    
    print("\n" + "="*100)
    print("ğŸ¯ GPT-4.1å¹³è¡¡ç‰ˆ100è§†é¢‘é¬¼æ¢å¤´æ£€æµ‹æ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
    print("="*100)
    
    if 'error' in eval_results:
        print(f"âŒ è¯„ä¼°é”™è¯¯: {eval_results['error']}")
        return
    
    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    summary = eval_results['evaluation_summary']
    cm = eval_results['confusion_matrix']
    metrics = eval_results['performance_metrics']
    processing = eval_results['processing_stats']
    breakdown = eval_results['detection_breakdown']
    
    print("ğŸ“Š æ•°æ®é›†æ¦‚è§ˆ:")
    print(f"   æ€»è§†é¢‘æ•°: {summary['total_videos']}")
    print(f"   Ground Truthé¬¼æ¢å¤´: {summary['ground_truth_positives']}")
    print(f"   Ground Truthæ­£å¸¸: {summary['ground_truth_negatives']}")
    print(f"   æ£€æµ‹å‡ºé¬¼æ¢å¤´: {breakdown['total_ghost_detected']}")
    print(f"   é«˜ç½®ä¿¡åº¦æ£€æµ‹: {breakdown['high_confidence_ghost']}")
    print(f"   æ½œåœ¨é¬¼æ¢å¤´æ£€æµ‹: {breakdown['potential_ghost']}")
    print()
    
    # æ··æ·†çŸ©é˜µ (GPT-4.1æ ¼å¼)
    print("ğŸ“‹ æ··æ·†çŸ©é˜µ:")
    print("                é¢„æµ‹ç»“æœ")
    print("              é¬¼æ¢å¤´  æ­£å¸¸")
    print("çœŸå®  é¬¼æ¢å¤´    {:3d}    {:3d}".format(cm['true_positive'], cm['false_negative']))
    print("æ ‡ç­¾  æ­£å¸¸      {:3d}    {:3d}".format(cm['false_positive'], cm['true_negative']))
    print()
    
    # æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡ (ä¸GPT-4.1å¯¹æ¯”æ ¼å¼)
    print("ğŸ¯ æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:")
    print(f"   âœ… F1åˆ†æ•°:     {metrics['f1_score']:.3f}")
    print(f"   ğŸ¯ å¬å›ç‡:     {metrics['recall']:.1%}  ({cm['true_positive']}/{summary['ground_truth_positives']})")
    print(f"   ğŸ¯ ç²¾ç¡®åº¦:     {metrics['precision']:.1%}  ({cm['true_positive']}/{breakdown['total_ghost_detected']})")
    print(f"   ğŸ“Š å‡†ç¡®ç‡:     {metrics['accuracy']:.1%}")
    print(f"   ğŸ“Š ç‰¹å¼‚æ€§:     {metrics['specificity']:.1%}")
    print(f"   âš ï¸  è¯¯æŠ¥ç‡:     {metrics['false_positive_rate']:.1%}")
    print()
    
    # å¤„ç†æ€§èƒ½
    print("â±ï¸  å¤„ç†æ€§èƒ½:")
    print(f"   å¹³å‡å¤„ç†æ—¶é—´: {processing['avg_processing_time']:.2f}ç§’/è§†é¢‘")
    print(f"   æ€»å¤„ç†æ—¶é—´: {processing['total_processing_time']:.2f}ç§’")
    print()
    
    # è¯¦ç»†æ¡ˆä¾‹åˆ†æ
    detailed = eval_results['detailed_analysis']
    
    print("ğŸ“ˆ æ£€æµ‹æ¡ˆä¾‹åˆ†æ:")
    
    # çœŸé˜³æ€§æ¡ˆä¾‹
    if detailed['true_positives']:
        print(f"\nâœ… æ­£ç¡®æ£€æµ‹çš„é¬¼æ¢å¤´ ({len(detailed['true_positives'])}ä¸ª):")
        for i, tp in enumerate(detailed['true_positives'][:10]):  # æ˜¾ç¤ºå‰10ä¸ª
            confidence_str = "é«˜ç½®ä¿¡åº¦" if tp['high_confidence'] else "æ½œåœ¨"
            print(f"   {i+1:2d}. {tp['video_id']}: {confidence_str} (ç½®ä¿¡åº¦: {tp['confidence']:.2f})")
            print(f"       å…³é”®åŠ¨ä½œ: {tp['key_actions'][:80]}...")
        if len(detailed['true_positives']) > 10:
            print(f"       ... è¿˜æœ‰ {len(detailed['true_positives']) - 10} ä¸ª")
    
    # å‡é˜³æ€§æ¡ˆä¾‹  
    if detailed['false_positives']:
        print(f"\nâŒ è¯¯æŠ¥çš„æ­£å¸¸è§†é¢‘ ({len(detailed['false_positives'])}ä¸ª):")
        for i, fp in enumerate(detailed['false_positives'][:10]):
            confidence_str = "é«˜ç½®ä¿¡åº¦" if fp['high_confidence'] else "æ½œåœ¨"
            print(f"   {i+1:2d}. {fp['video_id']}: {confidence_str} (ç½®ä¿¡åº¦: {fp['confidence']:.2f})")
            print(f"       å…³é”®åŠ¨ä½œ: {fp['key_actions'][:80]}...")
        if len(detailed['false_positives']) > 10:
            print(f"       ... è¿˜æœ‰ {len(detailed['false_positives']) - 10} ä¸ª")
    
    # å‡é˜´æ€§æ¡ˆä¾‹
    if detailed['false_negatives']:
        print(f"\nâš ï¸  æ¼æ£€çš„é¬¼æ¢å¤´ ({len(detailed['false_negatives'])}ä¸ª):")
        for i, fn in enumerate(detailed['false_negatives'][:10]):
            print(f"   {i+1:2d}. {fn['video_id']}: ç½®ä¿¡åº¦: {fn['confidence']:.2f}")
            print(f"       å…³é”®åŠ¨ä½œ: {fn['key_actions'][:80]}...")
        if len(detailed['false_negatives']) > 10:
            print(f"       ... è¿˜æœ‰ {len(detailed['false_negatives']) - 10} ä¸ª")
    
    print("\n" + "="*100)
    
    # ä¸GPT-4.1åŸºçº¿å¯¹æ¯”
    print("ğŸ“Š ä¸GPT-4.1åŸºçº¿å¯¹æ¯”:")
    print("   GPT-4.1åŸºçº¿æ€§èƒ½ (99è§†é¢‘):")
    print("   - F1åˆ†æ•°: 0.712")
    print("   - å¬å›ç‡: 96.3%") 
    print("   - ç²¾ç¡®åº¦: 56.5%")
    print("   - å‡†ç¡®ç‡: 57.6%")
    print("   - è¯¯æŠ¥ç‡: 88.9%")
    print()
    print("   å½“å‰æµ‹è¯•æ€§èƒ½ (100è§†é¢‘):")
    print(f"   - F1åˆ†æ•°: {metrics['f1_score']:.3f}")
    print(f"   - å¬å›ç‡: {metrics['recall']:.1%}")
    print(f"   - ç²¾ç¡®åº¦: {metrics['precision']:.1%}")
    print(f"   - å‡†ç¡®ç‡: {metrics['accuracy']:.1%}")
    print(f"   - è¯¯æŠ¥ç‡: {metrics['false_positive_rate']:.1%}")
    
    print("="*100)

def save_gpt41_format_results(eval_results: Dict, output_path: str):
    """ä¿å­˜GPT-4.1æ ¼å¼çš„ç»“æœ"""
    
    # åˆ›å»ºä¸GPT-4.1ä¸€è‡´çš„ç»“æœæ ¼å¼
    gpt41_format = {
        'evaluation_metadata': {
            'model': 'GPT-4.1-Balanced',
            'dataset': 'DADA-100-videos',
            'evaluation_date': datetime.now().isoformat(),
            'ground_truth_source': 'groundtruth_labels.csv',
            'evaluation_method': 'gpt41_compatible'
        },
        'performance_summary': eval_results['performance_metrics'],
        'confusion_matrix': eval_results['confusion_matrix'],
        'dataset_stats': eval_results['evaluation_summary'],
        'detection_breakdown': eval_results['detection_breakdown'],
        'processing_performance': eval_results['processing_stats'],
        'detailed_results': {
            'true_positives': eval_results['detailed_analysis']['true_positives'],
            'false_positives': eval_results['detailed_analysis']['false_positives'],
            'false_negatives': eval_results['detailed_analysis']['false_negatives'],
            'high_confidence_analysis': {
                'true_positives': eval_results['detailed_analysis']['high_confidence_true_positives'],
                'false_positives': eval_results['detailed_analysis']['high_confidence_false_positives']
            }
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(gpt41_format, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ GPT-4.1æ ¼å¼ç»“æœå·²ä¿å­˜: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¯„ä¼°GPT-4.1å¹³è¡¡ç‰ˆ100è§†é¢‘æ£€æµ‹ç»“æœ...")
    
    # æ–‡ä»¶è·¯å¾„
    gt_file = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/groundtruth_labels.csv"
    
    # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ç»“æœæ–‡ä»¶
    results_dir = Path("./outputs/results")
    if not results_dir.exists():
        results_dir = Path(".")
    
    gpt41_result_files = list(results_dir.glob("gpt41_balanced_100_videos_*.json"))
    
    if not gpt41_result_files:
        print("âŒ æœªæ‰¾åˆ°GPT-4.1ç»“æœæ–‡ä»¶")
        print("è¯·ç¡®ä¿å·²è¿è¡Œgpt41_balanced_100_videos.pyå¹¶ç”Ÿæˆäº†ç»“æœæ–‡ä»¶")
        return
    
    # ä½¿ç”¨æœ€æ–°çš„ç»“æœæ–‡ä»¶
    gpt41_results_file = max(gpt41_result_files, key=lambda p: p.stat().st_mtime)
    print(f"ğŸ“Š ä½¿ç”¨ç»“æœæ–‡ä»¶: {gpt41_results_file}")
    
    # æ£€æŸ¥ground truthæ–‡ä»¶
    if not Path(gt_file).exists():
        print(f"âŒ Ground truthæ–‡ä»¶ä¸å­˜åœ¨: {gt_file}")
        return
    
    # åŠ è½½æ•°æ®
    gt_labels = load_ground_truth_labels(gt_file)
    if not gt_labels:
        print("âŒ æ— æ³•åŠ è½½ground truthæ ‡ç­¾")
        return
    
    gpt41_results = load_gpt41_results(str(gpt41_results_file))
    if not gpt41_results:
        print("âŒ æ— æ³•åŠ è½½GPT-4.1æ£€æµ‹ç»“æœ")
        return
    
    # è¯„ä¼°æ€§èƒ½
    print("\nğŸ” å¼€å§‹æ€§èƒ½è¯„ä¼°...")
    eval_results = evaluate_gpt41_performance(gt_labels, gpt41_results)
    
    # æ‰“å°è¯„ä¼°æŠ¥å‘Š
    print_gpt41_evaluation_report(eval_results)
    
    # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜åŸå§‹è¯„ä¼°ç»“æœ
    detailed_output = f"gpt41_evaluation_detailed_{timestamp}.json"
    with open(detailed_output, 'w', encoding='utf-8') as f:
        json.dump(eval_results, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜GPT-4.1å…¼å®¹æ ¼å¼ç»“æœ
    gpt41_format_output = f"gpt41_evaluation_summary_{timestamp}.json"
    save_gpt41_format_results(eval_results, gpt41_format_output)
    
    print(f"\nğŸ“„ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜:")
    print(f"   è¯¦ç»†ç»“æœ: {detailed_output}")
    print(f"   GPT-4.1æ ¼å¼: {gpt41_format_output}")

if __name__ == "__main__":
    main()