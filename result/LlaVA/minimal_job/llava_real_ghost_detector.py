#!/usr/bin/env python3
"""
çœŸæ­£çš„LLaVAé¬¼æ¢å¤´æ£€æµ‹å™¨
ä½¿ç”¨LLaVA-NeXTæ¨¡å‹è¿›è¡Œè§†é¢‘å†…å®¹åˆ†æï¼Œåº”ç”¨å¹³è¡¡ç‰ˆGPT-4.1 prompt
"""

import json
import os
import sys
from pathlib import Path
from datetime import datetime
import cv2
import base64
import io
from PIL import Image
import logging
import torch
from typing import List, Dict, Optional, Tuple
import numpy as np

# å¯¼å…¥LLaVAç›¸å…³æ¨¡å—
try:
    sys.path.append('./LLaVA-NeXT')
    from llava.model.builder import load_pretrained_model
    from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token
    from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX
    from llava.conversation import conv_templates, SeparatorStyle
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥LLaVAæ¨¡å—: {e}")
    print("å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼è¿è¡Œ")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LLaVARealGhostDetector:
    """LLaVAçœŸå®é¬¼æ¢å¤´æ£€æµ‹å™¨"""
    
    def __init__(self, model_path: str = "lmms-lab/LLaVA-NeXT-Video-7B-DPO"):
        """
        åˆå§‹åŒ–LLaVAæ£€æµ‹å™¨
        
        Args:
            model_path: LLaVAæ¨¡å‹è·¯å¾„
        """
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.image_processor = None
        self.mock_mode = False
        
        # å¹³è¡¡ç‰ˆGPT-4.1é¬¼æ¢å¤´æ£€æµ‹prompt
        self.ghost_probing_prompt = """You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video. Focus on changes in relative positions, distances, and speeds of objects, particularly vehicles and pedestrians, and how these might indicate potential collision risks.

IMPORTANT: For ghost probing detection, consider TWO categories:

**1. HIGH-CONFIDENCE Ghost Probing (use "ghost probing" in key_actions)**:
- Object appears EXTREMELY close (within 1-2 vehicle lengths, <3 meters) 
- Appearance is SUDDEN and from blind spots (behind parked cars, buildings, corners)
- Occurs in HIGH-RISK environments: highways, rural roads, parking lots, uncontrolled intersections
- Requires IMMEDIATE emergency braking/swerving to avoid collision
- Movement is COMPLETELY UNPREDICTABLE and violates traffic expectations

**2. POTENTIAL Ghost Probing (use "potential ghost probing" in key_actions)**:
- Object appears suddenly but at moderate distance (3-5 meters)
- Sudden movement in environments where some unpredictability exists
- Requires emergency braking but collision risk is moderate
- Movement is unexpected but not completely impossible given the context

**3. NORMAL Traffic Situations (do NOT use "ghost probing")**:
- Pedestrians crossing at intersections, crosswalks, or traffic lights
- Vehicles making normal lane changes, turns, or merging with signals
- Cyclists following predictable paths in urban areas or bike lanes
- Any movement that is EXPECTED given the traffic environment and context

Your response should be a valid JSON object with the following structure:
{
    "ghost_probing_detected": "yes/no",
    "confidence": 0.0-1.0,
    "ghost_type": "high_confidence/potential/none",
    "summary": "brief description of the scene",
    "key_actions": "description of most important actions",
    "risk_level": "high/medium/low",
    "distance_estimate": "distance to closest threat in meters",
    "emergency_action_needed": "yes/no"
}

Analyze these sequential frames from a driving video:"""
        
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–LLaVAæ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LLaVAæ¨¡å‹...")
            
            # æ£€æŸ¥CUDAå¯ç”¨æ€§
            if torch.cuda.is_available():
                device = "cuda"
                logger.info(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            else:
                device = "cpu"
                logger.info("âš ï¸  ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
            
            # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            model_name = get_model_name_from_path(self.model_path)
            self.tokenizer, self.model, self.image_processor, context_len = load_pretrained_model(
                self.model_path, None, model_name, device=device
            )
            
            logger.info("âœ… LLaVAæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ LLaVAæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info("ğŸ”„ åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            self.mock_mode = True
    
    def extract_frames(self, video_path: str, num_frames: int = 8) -> List[Image.Image]:
        """
        ä»è§†é¢‘ä¸­æå–å…³é”®å¸§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            num_frames: æå–å¸§æ•°
            
        Returns:
            PIL Imageåˆ—è¡¨
        """
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps
            
            # å‡åŒ€åˆ†å¸ƒæå–å¸§
            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
            
            frames = []
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    # è½¬æ¢BGRåˆ°RGB
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(frame_rgb)
                    frames.append(pil_image)
            
            cap.release()
            logger.info(f"âœ… ä»è§†é¢‘ä¸­æå–äº†{len(frames)}å¸§ (æ—¶é•¿: {duration:.1f}s)")
            return frames
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘å¸§æå–å¤±è´¥: {e}")
            return []
    
    def analyze_frames_with_llava(self, frames: List[Image.Image]) -> Dict:
        """
        ä½¿ç”¨LLaVAæ¨¡å‹åˆ†æè§†é¢‘å¸§
        
        Args:
            frames: PIL Imageå¸§åˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if self.mock_mode or not frames:
            return self._mock_analysis()
        
        try:
            logger.info("ğŸ” æ­£åœ¨ä½¿ç”¨LLaVAåˆ†æè§†é¢‘å¸§...")
            
            # å¤„ç†å›¾åƒ
            image_tensors = process_images(frames, self.image_processor, self.model.config)
            if type(image_tensors) is list:
                image_tensors = [image.to(self.model.device, dtype=torch.float16) for image in image_tensors]
            else:
                image_tensors = image_tensors.to(self.model.device, dtype=torch.float16)
            
            # æ„å»ºå¯¹è¯prompt
            conv_mode = "llava_v1"
            conv = conv_templates[conv_mode].copy()
            
            # æ·»åŠ å›¾åƒtokenå’Œprompt
            inp = DEFAULT_IMAGE_TOKEN + '\n' + self.ghost_probing_prompt
            conv.append_message(conv.roles[0], inp)
            conv.append_message(conv.roles[1], None)
            prompt = conv.get_prompt()
            
            # Tokenizeè¾“å…¥
            input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
            
            # ç”Ÿæˆå“åº”
            with torch.inference_mode():
                output_ids = self.model.generate(
                    input_ids,
                    images=image_tensors,
                    image_sizes=[x.size for x in frames],
                    do_sample=True if 0.3 > 0 else False,
                    temperature=0.3,
                    top_p=None,
                    num_beams=1,
                    max_new_tokens=512,
                    use_cache=True
                )
            
            # è§£ç è¾“å‡º
            input_token_len = input_ids.shape[1]
            n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()
            if n_diff_input_output > 0:
                logger.warning(f"Inputså’Œoutputsä¸åŒ¹é…! {n_diff_input_output} tokensä¸åŒ.")
            
            outputs = self.tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
            outputs = outputs.strip()
            
            # å°è¯•è§£æJSONå“åº”
            try:
                result = json.loads(outputs)
                logger.info("âœ… LLaVAåˆ†æå®Œæˆï¼ŒæˆåŠŸè§£æJSONå“åº”")
                return result
            except json.JSONDecodeError:
                logger.warning("âš ï¸  LLaVAå“åº”ä¸æ˜¯æœ‰æ•ˆJSONï¼Œä½¿ç”¨æ–‡æœ¬è§£æ")
                return self._parse_text_response(outputs)
                
        except Exception as e:
            logger.error(f"âŒ LLaVAåˆ†æå¤±è´¥: {e}")
            return self._mock_analysis()
    
    def _parse_text_response(self, text: str) -> Dict:
        """è§£ææ–‡æœ¬å“åº”ä¸ºç»“æ„åŒ–æ•°æ®"""
        # åŸºäºå…³é”®è¯çš„ç®€å•è§£æ
        text_lower = text.lower()
        
        # æ£€æµ‹é¬¼æ¢å¤´å…³é”®è¯
        ghost_detected = any(keyword in text_lower for keyword in [
            'ghost probing', 'sudden appearance', 'emergency braking', 
            'collision risk', 'immediate threat'
        ])
        
        # ä¼°ç®—ç½®ä¿¡åº¦
        confidence = 0.8 if ghost_detected else 0.3
        
        # åˆ¤æ–­é¬¼æ¢å¤´ç±»å‹
        if 'high-confidence' in text_lower or 'extremely close' in text_lower:
            ghost_type = 'high_confidence'
        elif 'potential' in text_lower:
            ghost_type = 'potential'
        else:
            ghost_type = 'none'
        
        return {
            "ghost_probing_detected": "yes" if ghost_detected else "no",
            "confidence": confidence,
            "ghost_type": ghost_type,
            "summary": text[:200] + "..." if len(text) > 200 else text,
            "key_actions": "æ–‡æœ¬è§£æç»“æœ",
            "risk_level": "high" if ghost_detected else "low",
            "distance_estimate": "æœªçŸ¥",
            "emergency_action_needed": "yes" if ghost_detected else "no"
        }
    
    def _mock_analysis(self) -> Dict:
        """æ¨¡æ‹Ÿåˆ†æç»“æœï¼ˆå½“æ¨¡å‹æ— æ³•ä½¿ç”¨æ—¶ï¼‰"""
        return {
            "ghost_probing_detected": "no",
            "confidence": 0.5,
            "ghost_type": "none",
            "summary": "æ¨¡æ‹Ÿåˆ†æç»“æœ - æ­£å¸¸é©¾é©¶åœºæ™¯",
            "key_actions": "è½¦è¾†æ­£å¸¸è¡Œé©¶",
            "risk_level": "low",
            "distance_estimate": "5-10ç±³",
            "emergency_action_needed": "no"
        }
    
    def process_video(self, video_path: str) -> Dict:
        """
        å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ£€æµ‹ç»“æœ
        """
        video_name = Path(video_path).stem
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_name}")
        
        start_time = datetime.now()
        
        # æå–å…³é”®å¸§
        frames = self.extract_frames(video_path, num_frames=8)
        if not frames:
            return {
                "video_id": video_name,
                "error": "æ— æ³•æå–è§†é¢‘å¸§",
                "processing_time": 0
            }
        
        # ä½¿ç”¨LLaVAåˆ†æ
        analysis_result = self.analyze_frames_with_llava(frames)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        result = {
            "video_id": video_name,
            "video_path": str(video_path),
            "ghost_probing_label": analysis_result.get("ghost_probing_detected", "no"),
            "confidence": analysis_result.get("confidence", 0.5),
            "ghost_type": analysis_result.get("ghost_type", "none"),
            "summary": analysis_result.get("summary", ""),
            "key_actions": analysis_result.get("key_actions", ""),
            "risk_level": analysis_result.get("risk_level", "low"),
            "emergency_action_needed": analysis_result.get("emergency_action_needed", "no"),
            "model": "LLaVA-NeXT-Video-7B-DPO",
            "timestamp": datetime.now().isoformat(),
            "processing_time": processing_time,
            "method": "llava_real_analysis",
            "frames_analyzed": len(frames)
        }
        
        logger.info(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {video_name} ({processing_time:.1f}s)")
        return result

def main():
    """ä¸»å‡½æ•° - å¤„ç†100ä¸ªDADAè§†é¢‘"""
    print("ğŸš€ å¼€å§‹LLaVAçœŸå®é¬¼æ¢å¤´æ£€æµ‹...")
    
    # è·å–è§†é¢‘æ•°æ®è·¯å¾„
    azureml_data_path = os.environ.get('AZUREML_DATAREFERENCE_video_data')
    
    possible_paths = []
    if azureml_data_path:
        possible_paths.append(azureml_data_path)
        print(f"ğŸ”§ ä»ç¯å¢ƒå˜é‡æ‰¾åˆ°æ•°æ®è·¯å¾„: {azureml_data_path}")
    
    possible_paths.extend([
        "./inputs/video_data", 
        "./inputs",
        "."
    ])
    
    video_files = []
    video_folder = None
    
    for path in possible_paths:
        try:
            p = Path(path)
            if p.exists():
                found_videos = list(p.glob("**/*.avi"))
                if found_videos:
                    video_files = found_videos[:100]  # é™åˆ¶100ä¸ª
                    video_folder = p
                    print(f"âœ… åœ¨ {path} æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
                    break
        except Exception as e:
            print(f"âŒ æ£€æŸ¥è·¯å¾„ {path} æ—¶å‡ºé”™: {e}")
    
    if not video_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("./outputs/results", exist_ok=True)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = LLaVARealGhostDetector()
    
    print(f"ğŸ¬ å¼€å§‹å¤„ç† {len(video_files)} ä¸ªè§†é¢‘...")
    
    # å¤„ç†è§†é¢‘
    results = []
    for i, video_file in enumerate(video_files):
        try:
            result = detector.process_video(str(video_file))
            results.append(result)
            
            if (i + 1) % 10 == 0:
                print(f"ğŸ“Š å¤„ç†è¿›åº¦: {i+1}/{len(video_files)} ({(i+1)/len(video_files)*100:.1f}%)")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è§†é¢‘ {video_file} å¤±è´¥: {e}")
            # æ·»åŠ é”™è¯¯ç»“æœ
            results.append({
                "video_id": Path(video_file).stem,
                "error": str(e),
                "processing_time": 0
            })
    
    print("ğŸ’¾ ä¿å­˜ç»“æœæ–‡ä»¶...")
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # JSONæ ¼å¼ç»“æœ
    json_file = f"./outputs/results/llava_real_ghost_results_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'model': 'LLaVA-NeXT-Video-7B-DPO',
                'total_videos': len(results),
                'timestamp': timestamp,
                'video_folder': str(video_folder) if video_folder else 'not_found',
                'prompt_version': 'balanced_gpt41_compatible'
            },
            'results': results
        }, f, indent=2, ensure_ascii=False)
    
    # CSVæ ¼å¼ç»“æœ
    csv_file = f"./outputs/results/llava_real_ghost_results_{timestamp}.csv"
    with open(csv_file, 'w', encoding='utf-8') as f:
        f.write('video_id,ghost_probing_label,confidence,ghost_type,risk_level,processing_time,method\n')
        for r in results:
            if 'error' not in r:
                f.write(f"{r.get('video_id', '')},{r.get('ghost_probing_label', 'no')},{r.get('confidence', 0.5)},{r.get('ghost_type', 'none')},{r.get('risk_level', 'low')},{r.get('processing_time', 0)},{r.get('method', 'llava_real_analysis')}\n")
    
    # ç»Ÿè®¡ä¿¡æ¯
    successful_results = [r for r in results if 'error' not in r]
    ghost_count = len([r for r in successful_results if r.get('ghost_probing_label') == 'yes'])
    normal_count = len(successful_results) - ghost_count
    detection_rate = (ghost_count / len(successful_results)) * 100 if successful_results else 0
    avg_processing_time = sum(r.get('processing_time', 0) for r in successful_results) / len(successful_results) if successful_results else 0
    
    summary = {
        'total_videos': len(video_files),
        'successful_processed': len(successful_results),
        'failed_processed': len(results) - len(successful_results),
        'ghost_probing_detected': ghost_count, 
        'normal_videos': normal_count,
        'detection_rate_percent': round(detection_rate, 2),
        'average_processing_time': round(avg_processing_time, 2),
        'timestamp': timestamp,
        'files_generated': [json_file, csv_file]
    }
    
    summary_file = f"./outputs/results/llava_real_summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print("=" * 60)
    print("ğŸ‰ LLaVAçœŸå®é¬¼æ¢å¤´æ£€æµ‹å®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ“Š æ€»è§†é¢‘æ•°: {len(video_files)}")
    print(f"âœ… æˆåŠŸå¤„ç†: {len(successful_results)}")
    print(f"âŒ å¤„ç†å¤±è´¥: {len(results) - len(successful_results)}")
    print(f"ğŸš¨ é¬¼æ¢å¤´æ£€æµ‹: {ghost_count} ({detection_rate:.1f}%)")
    print(f"ğŸ“ˆ æ­£å¸¸è§†é¢‘: {normal_count} ({100-detection_rate:.1f}%)")
    print(f"â±ï¸  å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.1f}ç§’/è§†é¢‘")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {json_file}")
    print(f"ğŸ“Š CSVæ–‡ä»¶: {csv_file}")
    print(f"ğŸ“‹ ç»Ÿè®¡æ–‡ä»¶: {summary_file}")
    print("=" * 60)

if __name__ == "__main__":
    main()