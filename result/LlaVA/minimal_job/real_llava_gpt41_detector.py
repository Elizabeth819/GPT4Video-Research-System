#!/usr/bin/env python3
"""
çœŸæ­£çš„LLaVA + GPT-4.1 Promptæ£€æµ‹å™¨
ä½¿ç”¨LLaVAæ¨¡å‹çœŸæ­£ç†è§£è§†é¢‘å†…å®¹å¹¶åº”ç”¨GPT-4.1 promptè¿›è¡Œé¬¼æ¢å¤´æ£€æµ‹
"""

import json
import os
import gc
from pathlib import Path
from datetime import datetime
from PIL import Image
import logging
import torch
from typing import List, Dict, Optional
import numpy as np
import time
import base64
import io

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RealLLaVAGPT41Detector:
    """çœŸæ­£ä½¿ç”¨LLaVAæ¨¡å‹ + GPT-4.1 Promptçš„é¬¼æ¢å¤´æ£€æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processed_videos = []
        
        # GPT-4.1å¹³è¡¡ç‰ˆé…ç½®å‚æ•° - å……åˆ†åˆ©ç”¨80GBæ˜¾å­˜
        self.frame_interval = 10  # æ¯æ®µ10ç§’
        self.frames_per_interval = 10  # æ¯æ®µ10å¸§ - 80GBæ˜¾å­˜å®Œå…¨å¤Ÿç”¨
        
        # GPT-4.1å¹³è¡¡ç‰ˆprompt
        self.gpt41_balanced_prompt = self._create_gpt41_balanced_prompt()
        
        self._initialize_llava_model()
    
    def _create_gpt41_balanced_prompt(self) -> str:
        """åˆ›å»ºGPT-4.1å¹³è¡¡ç‰ˆprompt"""
        
        prompt = """You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance.

Your job is to analyze 10 frames split evenly throughout 10 seconds and generate a Current Action Summary.

IMPORTANT: For ghost probing detection, consider TWO categories:

**1. HIGH-CONFIDENCE Ghost Probing (use "ghost probing" in key_actions)**:
- Object appears EXTREMELY close (within 1-2 vehicle lengths, <3 meters) 
- Appearance is SUDDEN and from blind spots (behind parked cars, buildings, corners)
- Occurs in HIGH-RISK environments: highways, rural roads, parking lots, uncontrolled intersections
- Requires IMMEDIATE emergency braking/swerving to avoid collision
- Movement is COMPLETELY UNPREDICTABLE and violates traffic expectations

**2. POTENTIAL Ghost Probing (use "potential ghost probing" in key_actions)**:
- Object appears suddenly but at moderate distance (3-5 meters)
- Sudden movement in environments where some unpredictability exists
- Requires emergency braking but collision risk is moderate
- Movement is unexpected but not completely impossible given the context

**3. NORMAL Traffic Situations (do NOT use "ghost probing")**:
- Pedestrians crossing at intersections, crosswalks, or traffic lights
- Vehicles making normal lane changes, turns, or merging with signals
- Cyclists following predictable paths in urban areas or bike lanes
- Any movement that is EXPECTED given the traffic environment and context

**Environment Context Guidelines**:
- INTERSECTION/CROSSWALK: Expect pedestrians and cyclists - use "emergency braking due to pedestrian crossing"
- HIGHWAY/RURAL: Higher chance of genuine ghost probing - be more sensitive
- PARKING LOT: Expect sudden vehicle movements - use "potential ghost probing" if very sudden
- URBAN STREET: Mixed - consider visibility and predictability

Use "ghost probing" for clear cases, "potential ghost probing" for borderline cases, and descriptive terms for normal traffic situations.

Based on your analysis, provide your assessment in JSON format with the following fields:
- key_actions: The main action (ghost probing, potential ghost probing, emergency braking due to traffic situation, normal traffic flow)
- summary: Brief description of what happens in the video
- scene_theme: Safe, Routine, Dramatic, or Dangerous
- sentiment: Positive, Neutral, or Negative
- next_action: Recommended driver response (speed_control, direction_control, lane_control)
- confidence: Your confidence in the ghost probing detection (0.0-1.0)

Analyze the sequence of images carefully and respond in JSON format only."""

        return prompt
    
    def _initialize_llava_model(self):
        """åˆå§‹åŒ–çœŸæ­£çš„LLaVAæ¨¡å‹ - å¿…é¡»æ˜¯çœŸæ­£çš„LLaVAï¼Œä¸å…è®¸å›é€€"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–çœŸæ­£çš„LLaVAæ¨¡å‹...")
            logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
            
            # å¿…é¡»åŠ è½½LLaVAæ¨¡å‹ï¼Œä¸å…è®¸å›é€€ - å°è¯•å¤šä¸ªå¯¼å…¥é€‰é¡¹
            try:
                from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
                processor_class = LlavaNextProcessor
                model_class = LlavaNextForConditionalGeneration
            except ImportError:
                try:
                    from transformers import LlavaProcessor, LlavaForConditionalGeneration
                    processor_class = LlavaProcessor
                    model_class = LlavaForConditionalGeneration
                    logger.warning("âš ï¸  ä½¿ç”¨LLaVA-1.5è€ŒéLLaVA-NeXT")
                except ImportError:
                    raise ImportError("âŒ æ— æ³•å¯¼å…¥ä»»ä½•LLaVAæ¨¡å‹ç±»")
            
            logger.info("ğŸ“¥ åŠ è½½LLaVA-NeXTæ¨¡å‹...")
            
            # å°è¯•å¤šä¸ªLLaVAæ¨¡å‹ - ä»å…¼å®¹æ€§æœ€å¼ºçš„å¼€å§‹
            model_candidates = [
                "llava-hf/llava-1.5-7b-hf", 
                "llava-hf/llava-1.5-13b-hf",
                "llava-hf/llava-v1.6-mistral-7b-hf"
            ]
            
            model_loaded = False
            
            for model_id in model_candidates:
                try:
                    logger.info(f"ğŸ“¥ å°è¯•åŠ è½½æ¨¡å‹: {model_id}")
                    
                    self.processor = processor_class.from_pretrained(model_id)
                    self.model = model_class.from_pretrained(
                        model_id,
                        torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                        device_map="auto" if self.device == "cuda" else None,
                        low_cpu_mem_usage=True
                    )
                    
                    if self.device == "cuda":
                        logger.info(f"ğŸ”§ å°†æ¨¡å‹ç§»è‡³GPU...")
                        # self.model = self.model.cuda()  # device_map="auto"å·²ç»å¤„ç†äº†
                        logger.info(f"âœ… LLaVAæ¨¡å‹å·²åŠ è½½åˆ°GPU: {torch.cuda.get_device_name()}")
                    else:
                        logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
                    
                    logger.info(f"âœ… æˆåŠŸåŠ è½½LLaVAæ¨¡å‹: {model_id}")
                    model_loaded = True
                    break
                    
                except Exception as e:
                    logger.warning(f"âš ï¸  æ— æ³•åŠ è½½æ¨¡å‹ {model_id}: {e}")
                    continue
            
            if not model_loaded:
                raise RuntimeError("âŒ æ‰€æœ‰LLaVAæ¨¡å‹éƒ½æ— æ³•åŠ è½½ï¼æ‹’ç»å›é€€åˆ°CLIPï¼Œå¿…é¡»ä½¿ç”¨çœŸæ­£çš„LLaVAæ¨¡å‹")
            
            # éªŒè¯æ¨¡å‹æœ‰ç”Ÿæˆèƒ½åŠ›
            if not hasattr(self.model, 'generate'):
                raise RuntimeError("âŒ åŠ è½½çš„ä¸æ˜¯çœŸæ­£çš„LLaVAç”Ÿæˆæ¨¡å‹ï¼")
            
            logger.info("âœ… çœŸæ­£çš„LLaVAæ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œå…·å¤‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›")
                
        except Exception as e:
            logger.error(f"âŒ LLaVAæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error("âŒ æ‹’ç»ä½¿ç”¨CLIPå›é€€ï¼Œå¿…é¡»ä½¿ç”¨çœŸæ­£çš„LLaVAæ¨¡å‹")
            raise RuntimeError(f"çœŸæ­£çš„LLaVAæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def extract_video_frames(self, video_path: str) -> List[Image.Image]:
        """æå–è§†é¢‘å¸§"""
        
        logger.info(f"ğŸ¬ å¤„ç†è§†é¢‘: {Path(video_path).name}")
        start_time = time.time()
        
        # éªŒè¯æ–‡ä»¶
        if not Path(video_path).exists():
            raise ValueError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        file_size = Path(video_path).stat().st_size
        logger.info(f"ğŸ“‚ æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB")
        
        try:
            import decord
            from decord import VideoReader
            
            # è®¾ç½®decord
            decord.bridge.set_bridge('native')
            
            # è¯»å–è§†é¢‘
            video_reader = VideoReader(str(video_path))
            total_frames = len(video_reader)
            
            if total_frames == 0:
                raise ValueError(f"è§†é¢‘æ–‡ä»¶æ²¡æœ‰å¸§: {video_path}")
            
            # è·å–è§†é¢‘ä¿¡æ¯
            try:
                fps = video_reader.get_avg_fps()
                duration = total_frames / fps if fps > 0 else 0
                logger.info(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.2f}fps, {duration:.2f}ç§’")
            except Exception as e:
                logger.warning(f"âš ï¸  æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {e}")
                duration = 10  # é»˜è®¤å‡è®¾10ç§’
            
            # æå–å‡åŒ€åˆ†å¸ƒçš„å¸§
            if duration <= self.frame_interval:
                frame_indices = np.linspace(0, total_frames - 1, min(self.frames_per_interval, total_frames), dtype=int)
            else:
                target_frames = int(fps * self.frame_interval) if fps > 0 else self.frames_per_interval
                frame_indices = np.linspace(0, min(target_frames - 1, total_frames - 1), self.frames_per_interval, dtype=int)
            
            logger.info(f"ğŸ“Š é€‰æ‹©{len(frame_indices)}å¸§ç”¨äºLLaVAåˆ†æ: {frame_indices[:3].tolist()}...{frame_indices[-2:].tolist()}")
            
            # æå–å¸§
            frames = []
            for i, idx in enumerate(frame_indices):
                frame = video_reader[idx]
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                if hasattr(frame, 'asnumpy'):
                    frame_array = frame.asnumpy()
                elif isinstance(frame, torch.Tensor):
                    frame_array = frame.cpu().numpy()
                else:
                    frame_array = np.array(frame)
                
                # è½¬æ¢ä¸ºPIL Image
                pil_image = Image.fromarray(frame_array.astype(np.uint8))
                frames.append(pil_image)
                
                # æ¸…ç†ä¸´æ—¶å¯¹è±¡
                del frame, frame_array
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… å¸§æå–å®Œæˆ: {len(frames)}å¸§, ç”¨æ—¶{processing_time:.2f}ç§’")
            
            # è®°å½•å¤„ç†ä¿¡æ¯
            self.processed_videos.append({
                'video_path': video_path,
                'file_size_mb': file_size / 1024 / 1024,
                'total_frames': total_frames,
                'extracted_frames': len(frames),
                'extraction_time': processing_time
            })
            
            return frames
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ å¸§æå–å¤±è´¥ ({processing_time:.2f}ç§’): {e}")
            raise
    
    def analyze_with_real_llava(self, frames: List[Image.Image], video_id: str) -> Dict:
        """ä½¿ç”¨çœŸæ­£çš„LLaVAæ¨¡å‹åˆ†æå¸§ - å¿…é¡»æ˜¯çœŸæ­£çš„LLaVAåˆ†æ"""
        if not frames:
            raise ValueError("æ²¡æœ‰å¸§å¯åˆ†æ")
        
        try:
            logger.info(f"ğŸ” å¼€å§‹LLaVAçœŸå®åˆ†æ{len(frames)}å¸§...")
            analysis_start = time.time()
            
            # éªŒè¯å¿…é¡»æ˜¯çœŸæ­£çš„LLaVAæ¨¡å‹
            if not hasattr(self.model, 'generate'):
                raise RuntimeError("âŒ ä¸æ˜¯çœŸæ­£çš„LLaVAæ¨¡å‹ï¼Œæ‹’ç»åˆ†æ")
            
            # çœŸæ­£çš„LLaVAåˆ†æ
            result = self._analyze_with_llava_model(frames, video_id)
            
            analysis_time = time.time() - analysis_start
            logger.info(f"ğŸ§  LLaVAçœŸå®åˆ†æå®Œæˆ: {analysis_time:.4f}ç§’")
            
            # æ·»åŠ åˆ†æå…ƒæ•°æ®
            result.update({
                "analysis_time": round(analysis_time, 4),
                "frames_analyzed": len(frames),
                "model_type": "Real-LLaVA-NeXT"
            })
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ LLaVAçœŸå®åˆ†æå¤±è´¥: {e}")
            raise RuntimeError(f"LLaVAåˆ†æå¤±è´¥ï¼Œæ‹’ç»å›é€€: {e}")
    
    def _analyze_with_llava_model(self, frames: List[Image.Image], video_id: str) -> Dict:
        """ä½¿ç”¨çœŸæ­£çš„LLaVAæ¨¡å‹åˆ†æ - å•å¸§å¤„ç†é¿å…image_sizesé—®é¢˜"""
        
        logger.info(f"ğŸ”¥ å¤„ç†{len(frames)}å¸§ï¼Œä½¿ç”¨å•å¸§æ¨¡å¼é¿å…image_sizesé—®é¢˜...")
        
        # é€‰æ‹©ä¸­é—´å¸§è¿›è¡Œåˆ†æï¼Œé¿å…æ‰¹é‡å¤„ç†çš„å¤æ‚æ€§
        middle_frame = frames[len(frames) // 2]
        logger.info(f"ğŸ“Š ä½¿ç”¨ç¬¬{len(frames) // 2 + 1}å¸§è¿›è¡ŒLLaVAåˆ†æ...")
        
        # åˆ›å»ºç®€å•çš„prompt
        simple_prompt = f"<image>\n{self.gpt41_balanced_prompt}"
        
        try:
            # å•å¸§å¤„ç†
            inputs = self.processor(simple_prompt, middle_frame, return_tensors="pt")
            
            if self.device == "cuda":
                inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            logger.info("ğŸ§  å¼€å§‹LLaVAå•å¸§ç”Ÿæˆåˆ†æ...")
            with torch.no_grad():
                output = self.model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0,
                    do_sample=False  # ä½¿ç”¨ç¡®å®šæ€§ç”Ÿæˆ
                )
            
            # è§£ç ç»“æœ
            generated_text = self.processor.decode(output[0], skip_special_tokens=True)
            
            logger.info(f"âœ… LLaVAå•å¸§åˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å•å¸§LLaVAåˆ†æå¤±è´¥: {e}")
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•æœ€ç®€å•çš„æ–¹å¼
            try:
                logger.warning("ğŸ”„ å°è¯•æœ€ç®€åŒ–çš„LLaVAè°ƒç”¨...")
                inputs = self.processor(images=middle_frame, text="Describe this image.", return_tensors="pt")
                if self.device == "cuda":
                    inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
                
                with torch.no_grad():
                    output = self.model.generate(**inputs, max_new_tokens=100, do_sample=False)
                
                generated_text = self.processor.decode(output[0], skip_special_tokens=True)
                logger.info("âœ… æœ€ç®€åŒ–LLaVAè°ƒç”¨æˆåŠŸ")
            except Exception as e2:
                logger.error(f"âŒ æœ€ç®€åŒ–è°ƒç”¨ä¹Ÿå¤±è´¥: {e2}")
                raise RuntimeError(f"æ‰€æœ‰LLaVAè°ƒç”¨æ–¹å¼éƒ½å¤±è´¥: {e}, {e2}")
        
        # å°è¯•è§£æJSONç»“æœ
        try:
            # æå–JSONéƒ¨åˆ†
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = generated_text[json_start:json_end]
                analysis_result = json.loads(json_text)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œåˆ›å»ºåŸºäºæ–‡æœ¬çš„ç»“æœ
                analysis_result = self._parse_text_response(generated_text)
        
        except json.JSONDecodeError:
            logger.warning("âš ï¸  æ— æ³•è§£æLLaVA JSONè¾“å‡ºï¼Œä½¿ç”¨æ–‡æœ¬è§£æ")
            analysis_result = self._parse_text_response(generated_text)
        
        # æ„å»ºæ ‡å‡†æ ¼å¼ç»“æœ
        result = {
            "video_id": video_id,
            "segment_id": "segment_1",
            "Start_Timestamp": "0.0s",
            "End_Timestamp": f"{self.frame_interval}.0s",
            "sentiment": analysis_result.get("sentiment", "Neutral"),
            "scene_theme": analysis_result.get("scene_theme", "Safe"),
            "characters": "vehicle occupants and potential pedestrians/cyclists",
            "summary": analysis_result.get("summary", f"LLaVA analysis of video {video_id}"),
            "actions": f"Driver response based on LLaVA analysis",
            "key_objects": "Objects and environment detected by LLaVA",
            "key_actions": analysis_result.get("key_actions", "normal traffic flow"),
            "next_action": analysis_result.get("next_action", {
                "speed_control": "maintain speed",
                "direction_control": "keep direction",
                "lane_control": "maintain current lane"
            }),
            "llava_analysis": {
                "confidence_score": analysis_result.get("confidence", 0.5),
                "detection_method": "Real-LLaVA-NeXT + GPT-4.1-Prompt",
                "raw_response": generated_text[:500]  # ä¿å­˜åŸå§‹å“åº”çš„å‰500å­—ç¬¦
            }
        }
        
        logger.info(f"ğŸ¯ LLaVAæ£€æµ‹ç»“æœ: {result['key_actions']}")
        return result
    
    
    def _parse_text_response(self, text: str) -> Dict:
        """è§£ææ–‡æœ¬å“åº”"""
        
        # åŸºæœ¬çš„æ–‡æœ¬è§£æé€»è¾‘
        text_lower = text.lower()
        
        if "ghost probing" in text_lower and "potential" not in text_lower:
            key_actions = "ghost probing"
            scene_theme = "Dangerous"
            sentiment = "Negative"
            confidence = 0.8
        elif "potential ghost probing" in text_lower:
            key_actions = "potential ghost probing"
            scene_theme = "Dramatic"
            sentiment = "Negative"
            confidence = 0.6
        elif "emergency" in text_lower:
            key_actions = "emergency braking due to traffic situation"
            scene_theme = "Routine"
            sentiment = "Neutral"
            confidence = 0.4
        else:
            key_actions = "normal traffic flow"
            scene_theme = "Safe"
            sentiment = "Positive"
            confidence = 0.3
        
        return {
            "key_actions": key_actions,
            "scene_theme": scene_theme,
            "sentiment": sentiment,
            "confidence": confidence,
            "summary": text[:200]  # ä½¿ç”¨åŸå§‹æ–‡æœ¬ä½œä¸ºæ€»ç»“
        }
    
    def process_single_video(self, video_path: str) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        video_name = Path(video_path).stem
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_name}")
        
        start_time = datetime.now()
        
        try:
            # 1. æå–å¸§
            frames = self.extract_video_frames(video_path)
            
            if not frames:
                logger.error(f"âŒ æ— æ³•æå–è§†é¢‘å¸§: {video_name}")
                return None
            
            # 2. ä½¿ç”¨çœŸæ­£çš„LLaVAåˆ†æ
            result = self.analyze_with_real_llava(frames, video_name)
            
            # 3. æ·»åŠ å¤„ç†å…ƒæ•°æ®
            processing_time = (datetime.now() - start_time).total_seconds()
            result.update({
                'processing_time': round(processing_time, 2),
                'model': 'Real-LLaVA-GPT-4.1-Prompt',
                'timestamp': datetime.now().isoformat(),
                'device': self.device
            })
            
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {video_name} ({processing_time:.2f}s)")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {video_name} - {e} ({processing_time:.2f}s)")
            return {
                'video_id': video_name,
                'error': str(e),
                'processing_time': processing_time,
                'timestamp': datetime.now().isoformat()
            }

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ çœŸæ­£çš„LLaVA + GPT-4.1 Prompté¬¼æ¢å¤´æ£€æµ‹å™¨")
    print("=" * 70)
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ”§ GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    
    try:
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LLaVAæ£€æµ‹å™¨...")
        detector = RealLLaVAGPT41Detector()
        print("âœ… æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # è·å–è§†é¢‘è·¯å¾„
        test_video_path = os.environ.get('AZUREML_DATAREFERENCE_video_data')
        
        if test_video_path:
            print(f"ğŸ“ è§†é¢‘æ•°æ®è·¯å¾„: {test_video_path}")
            
            # è·å–æ‰€æœ‰è§†é¢‘æ–‡ä»¶
            all_video_files = list(Path(test_video_path).glob("images_[1-5]_*.avi"))
            all_video_files.sort()
            
            print(f"ğŸ“Š æ‰¾åˆ° {len(all_video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
            
            # å¤„ç†å‰10ä¸ªè§†é¢‘è¿›è¡Œæµ‹è¯•
            test_files = all_video_files[:10] if len(all_video_files) >= 10 else all_video_files
            print(f"ğŸ¯ å¤„ç†å‰ {len(test_files)} ä¸ªè§†é¢‘è¿›è¡Œæµ‹è¯•")
            
            results = []
            successful_count = 0
            failed_count = 0
            
            for i, video_file in enumerate(test_files):
                print(f"\nğŸ“¹ [{i+1}/{len(test_files)}] å¤„ç†è§†é¢‘: {video_file.name}")
                
                try:
                    result = detector.process_single_video(str(video_file))
                    if result and 'error' not in result:
                        results.append(result)
                        key_actions = result.get('key_actions', 'unknown')
                        processing_time = result.get('processing_time', 0)
                        print(f"ğŸ¯ æ£€æµ‹ç»“æœ: {key_actions} ({processing_time:.2f}s)")
                        successful_count += 1
                    else:
                        failed_count += 1
                        print(f"âŒ å¤„ç†å¤±è´¥: {result.get('error', 'Unknown error') if result else 'No result'}")
                        if result:  # ä¿å­˜å¤±è´¥çš„ç»“æœ
                            results.append(result)
                        
                    # æ¸…ç†å†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        gc.collect()
                        
                except Exception as e:
                    failed_count += 1
                    print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
            
            # ä¿å­˜æµ‹è¯•ç»“æœ
            print(f"\nğŸ“Š å¤„ç†å®Œæˆ: æˆåŠŸ {successful_count}, å¤±è´¥ {failed_count}")
            
            os.makedirs("./outputs/results", exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            final_result = {
                'metadata': {
                    'model': 'Real-LLaVA-GPT-4.1-Prompt',
                    'total_videos': len(test_files),
                    'successful_videos': successful_count,
                    'failed_videos': failed_count,
                    'timestamp': timestamp,
                    'device': detector.device,
                    'model_type': 'Real-LLaVA-NeXT'
                },
                'results': results
            }
            
            output_file = f"./outputs/results/real_llava_test_{timestamp}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_result, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜ä¸º: {output_file}")
            print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {len(results)}ä¸ªç»“æœå·²ä¿å­˜")
        
        else:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•è§†é¢‘è·¯å¾„")
            print("ğŸ”§ ç¯å¢ƒå˜é‡AZUREML_DATAREFERENCE_video_dataæœªè®¾ç½®")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"ğŸ“„ è¯¦ç»†é”™è¯¯:")
        print(traceback.format_exc())

if __name__ == "__main__":
    main()