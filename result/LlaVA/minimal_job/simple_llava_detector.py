#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆLLaVAé¬¼æ¢å¤´æ£€æµ‹å™¨
ä½¿ç”¨OpenAI CLIP + GPTæ¨¡å‹è¿›è¡Œè§†é¢‘åˆ†æ
"""

import json
import os
import sys
from pathlib import Path
from datetime import datetime
from PIL import Image
import logging
import torch
from typing import List, Dict, Optional, Tuple
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleLLaVADetector:
    """ç®€åŒ–ç‰ˆLLaVAé¬¼æ¢å¤´æ£€æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.processor = None
        
        # å¹³è¡¡ç‰ˆGPT-4.1é¬¼æ¢å¤´æ£€æµ‹prompt
        self.ghost_probing_prompt = """Analyze these sequential driving video frames for ghost probing detection.

Ghost probing means: sudden appearance of vehicles/pedestrians from blind spots at very close distance requiring immediate emergency action.

HIGH-CONFIDENCE Ghost Probing:
- Object appears within 1-2 vehicle lengths (<3 meters)
- SUDDEN appearance from blind spots
- Requires IMMEDIATE emergency braking
- Completely unpredictable movement

Respond with JSON:
{
    "ghost_probing_detected": "yes/no",
    "confidence": 0.0-1.0,
    "ghost_type": "high_confidence/potential/none",
    "summary": "brief description",
    "risk_level": "high/medium/low"
}"""
        
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹ - ä½¿ç”¨CLIP + GPT2"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–ç®€åŒ–ç‰ˆæ¨¡å‹...")
            logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
            
            # ä½¿ç”¨CLIPè¿›è¡Œå›¾åƒç¼–ç 
            from transformers import CLIPProcessor, CLIPModel, GPT2LMHeadModel, GPT2Tokenizer
            
            # åŠ è½½CLIPæ¨¡å‹
            logger.info("ğŸ“¥ åŠ è½½CLIPæ¨¡å‹...")
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            
            # åŠ è½½GPT2æ¨¡å‹ç”¨äºæ–‡æœ¬ç”Ÿæˆ
            logger.info("ğŸ“¥ åŠ è½½GPT2æ¨¡å‹...")
            self.gpt_model = GPT2LMHeadModel.from_pretrained("gpt2")
            self.gpt_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
            self.gpt_tokenizer.pad_token = self.gpt_tokenizer.eos_token
            
            # ç§»åˆ°GPU
            if torch.cuda.is_available():
                self.clip_model = self.clip_model.cuda()
                self.gpt_model = self.gpt_model.cuda()
                logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU: {torch.cuda.get_device_name()}")
            else:
                logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def extract_frames_with_decord(self, video_path: str, num_frames: int = 8) -> List[Image.Image]:
        """ä½¿ç”¨decordæå–è§†é¢‘å¸§"""
        try:
            import decord
            from decord import VideoReader
            
            # è®¾ç½®decordä½¿ç”¨native bridge
            decord.bridge.set_bridge('native')
            
            # è¯»å–è§†é¢‘
            video_reader = VideoReader(str(video_path))
            total_frames = len(video_reader)
            
            if total_frames == 0:
                raise ValueError(f"è§†é¢‘æ–‡ä»¶æ²¡æœ‰å¸§: {video_path}")
            
            # å‡åŒ€åˆ†å¸ƒé€‰æ‹©å¸§
            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
            
            # æå–å¸§
            frames = []
            for idx in frame_indices:
                frame = video_reader[idx]
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                if hasattr(frame, 'asnumpy'):
                    frame_array = frame.asnumpy()
                elif isinstance(frame, torch.Tensor):
                    frame_array = frame.cpu().numpy()
                else:
                    frame_array = np.array(frame)
                
                # è½¬æ¢ä¸ºPIL Image
                pil_image = Image.fromarray(frame_array.astype(np.uint8))
                frames.append(pil_image)
            
            logger.info(f"âœ… ä»è§†é¢‘ä¸­æå–äº†{len(frames)}å¸§")
            return frames
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘å¸§æå–å¤±è´¥: {e}")
            raise
    
    def analyze_frames_simple(self, frames: List[Image.Image], video_path: str) -> Dict:
        """ä½¿ç”¨ç®€åŒ–æ–¹æ³•åˆ†æè§†é¢‘å¸§"""
        if not frames:
            raise ValueError("æ²¡æœ‰å¸§å¯åˆ†æ")
        
        try:
            logger.info(f"ğŸ” æ­£åœ¨åˆ†æ{len(frames)}å¸§...")
            
            # ä½¿ç”¨CLIPæå–å›¾åƒç‰¹å¾
            inputs = self.clip_processor(images=frames, return_tensors="pt", padding=True)
            
            if torch.cuda.is_available():
                inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            with torch.no_grad():
                image_features = self.clip_model.get_image_features(**inputs)
            
            # åˆ†æå¸§é—´å˜åŒ–
            feature_changes = []
            for i in range(1, len(frames)):
                # è®¡ç®—ç›¸é‚»å¸§ä¹‹é—´çš„ç‰¹å¾å·®å¼‚
                diff = torch.cosine_similarity(
                    image_features[i-1].unsqueeze(0),
                    image_features[i].unsqueeze(0)
                ).item()
                feature_changes.append(1 - diff)  # å·®å¼‚è¶Šå¤§ï¼Œå€¼è¶Šé«˜
            
            # åŸºäºç‰¹å¾å˜åŒ–åˆ¤æ–­é¬¼æ¢å¤´
            max_change = max(feature_changes) if feature_changes else 0
            avg_change = sum(feature_changes) / len(feature_changes) if feature_changes else 0
            
            # ç®€å•çš„è§„åˆ™åˆ¤æ–­
            if max_change > 0.5 and avg_change > 0.3:
                ghost_detected = "yes"
                confidence = min(0.9, max_change)
                ghost_type = "high_confidence" if max_change > 0.7 else "potential"
                risk_level = "high" if max_change > 0.7 else "medium"
            elif max_change > 0.3:
                ghost_detected = "yes"
                confidence = max_change * 0.7
                ghost_type = "potential"
                risk_level = "medium"
            else:
                ghost_detected = "no"
                confidence = 0.2
                ghost_type = "none"
                risk_level = "low"
            
            result = {
                "ghost_probing_detected": ghost_detected,
                "confidence": round(confidence, 3),
                "ghost_type": ghost_type,
                "summary": f"è§†é¢‘åˆ†æå®Œæˆï¼Œæœ€å¤§å¸§é—´å˜åŒ–: {max_change:.3f}",
                "key_actions": f"æ£€æµ‹åˆ°{len(frames)}å¸§ï¼Œç‰¹å¾å˜åŒ–åˆ†æ",
                "risk_level": risk_level,
                "distance_estimate": "åŸºäºç‰¹å¾å˜åŒ–ä¼°ç®—",
                "emergency_action_needed": "yes" if ghost_detected == "yes" else "no",
                "max_frame_change": round(max_change, 3),
                "avg_frame_change": round(avg_change, 3)
            }
            
            logger.info(f"âœ… åˆ†æå®Œæˆ - é¬¼æ¢å¤´: {ghost_detected} (ç½®ä¿¡åº¦: {confidence:.3f})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¸§åˆ†æå¤±è´¥: {e}")
            raise
    
    def process_video(self, video_path: str) -> Dict:
        """å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶"""
        video_name = Path(video_path).stem
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_name}")
        
        start_time = datetime.now()
        
        try:
            # æå–è§†é¢‘å¸§
            frames = self.extract_frames_with_decord(video_path, num_frames=8)
            
            if not frames:
                raise ValueError("æ— æ³•æå–è§†é¢‘å¸§")
            
            # åˆ†æå¸§
            analysis_result = self.analyze_frames_simple(frames, video_path)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            result = {
                "video_id": video_name,
                "video_path": str(video_path),
                "ghost_probing_label": analysis_result.get("ghost_probing_detected", "no"),
                "confidence": analysis_result.get("confidence", 0.5),
                "ghost_type": analysis_result.get("ghost_type", "none"),
                "summary": analysis_result.get("summary", ""),
                "key_actions": analysis_result.get("key_actions", ""),
                "risk_level": analysis_result.get("risk_level", "low"),
                "emergency_action_needed": analysis_result.get("emergency_action_needed", "no"),
                "model": "CLIP-GPT2-Simple",
                "timestamp": datetime.now().isoformat(),
                "processing_time": processing_time,
                "method": "simple_feature_analysis",
                "frames_analyzed": len(frames),
                "device": self.device,
                "max_frame_change": analysis_result.get("max_frame_change", 0),
                "avg_frame_change": analysis_result.get("avg_frame_change", 0)
            }
            
            logger.info(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {video_name} ({processing_time:.1f}s)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è§†é¢‘{video_name}å¤±è´¥: {e}")
            return {
                "video_id": video_name,
                "video_path": str(video_path),
                "error": str(e),
                "processing_time": (datetime.now() - start_time).total_seconds()
            }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç®€åŒ–ç‰ˆLLaVAé¬¼æ¢å¤´æ£€æµ‹...")
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ“Š GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    
    # è·å–è§†é¢‘è·¯å¾„
    azureml_data_path = os.environ.get('AZUREML_DATAREFERENCE_video_data')
    
    possible_paths = []
    if azureml_data_path:
        possible_paths.append(azureml_data_path)
        print(f"ğŸ”§ ä»ç¯å¢ƒå˜é‡æ‰¾åˆ°æ•°æ®è·¯å¾„: {azureml_data_path}")
    
    possible_paths.extend(["./inputs/video_data", "./inputs", "."])
    
    video_files = []
    video_folder = None
    
    for path in possible_paths:
        try:
            p = Path(path)
            if p.exists():
                found_videos = list(p.glob("**/*.avi"))
                if found_videos:
                    video_files = found_videos[:100]
                    video_folder = p
                    print(f"âœ… åœ¨ {path} æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
                    break
        except Exception as e:
            print(f"âŒ æ£€æŸ¥è·¯å¾„ {path} æ—¶å‡ºé”™: {e}")
    
    if not video_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("./outputs/results", exist_ok=True)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = SimpleLLaVADetector()
    
    print(f"ğŸ¬ å¼€å§‹å¤„ç† {len(video_files)} ä¸ªè§†é¢‘...")
    
    # å¤„ç†è§†é¢‘
    results = []
    failed_count = 0
    
    for i, video_file in enumerate(video_files):
        try:
            result = detector.process_video(str(video_file))
            results.append(result)
            
            if 'error' in result:
                failed_count += 1
            
            if (i + 1) % 10 == 0:
                success_count = (i + 1) - failed_count
                print(f"ğŸ“Š è¿›åº¦: {i+1}/{len(video_files)} ({(i+1)/len(video_files)*100:.1f}%) - æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è§†é¢‘å¤±è´¥: {e}")
            failed_count += 1
            results.append({
                "video_id": Path(video_file).stem,
                "video_path": str(video_file),
                "error": str(e),
                "processing_time": 0
            })
    
    # ä¿å­˜ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»“æœæ–‡ä»¶...")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    successful_results = [r for r in results if 'error' not in r]
    
    # JSONç»“æœ
    json_file = f"./outputs/results/simple_llava_results_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'model': 'CLIP-GPT2-Simple',
                'device': 'GPU' if torch.cuda.is_available() else 'CPU',
                'total_videos': len(video_files),
                'successful_videos': len(successful_results),
                'failed_videos': failed_count,
                'timestamp': timestamp
            },
            'results': results
        }, f, indent=2, ensure_ascii=False)
    
    # CSVç»“æœ
    if successful_results:
        csv_file = f"./outputs/results/simple_llava_results_{timestamp}.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write('video_id,ghost_probing_label,confidence,ghost_type,risk_level,processing_time\n')
            for r in successful_results:
                f.write(f"{r['video_id']},{r['ghost_probing_label']},{r['confidence']:.3f},{r['ghost_type']},{r['risk_level']},{r['processing_time']:.1f}\n")
    
    # ç»Ÿè®¡
    if successful_results:
        ghost_count = len([r for r in successful_results if r['ghost_probing_label'] == 'yes'])
        detection_rate = (ghost_count / len(successful_results)) * 100
        avg_time = sum(r['processing_time'] for r in successful_results) / len(successful_results)
    else:
        ghost_count = detection_rate = avg_time = 0
    
    print("=" * 60)
    print("ğŸ‰ ç®€åŒ–ç‰ˆæ£€æµ‹å®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ“Š æ€»è§†é¢‘æ•°: {len(video_files)}")
    print(f"âœ… æˆåŠŸå¤„ç†: {len(successful_results)}")
    print(f"âŒ å¤„ç†å¤±è´¥: {failed_count}")
    if successful_results:
        print(f"ğŸš¨ é¬¼æ¢å¤´æ£€æµ‹: {ghost_count} ({detection_rate:.1f}%)")
        print(f"â±ï¸  å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.1f}ç§’/è§†é¢‘")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {json_file}")
    print("=" * 60)

if __name__ == "__main__":
    main()