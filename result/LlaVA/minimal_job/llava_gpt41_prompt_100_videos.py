#!/usr/bin/env python3
"""
LLaVA + GPT-4.1å¹³è¡¡ç‰ˆPrompt 100è§†é¢‘é¬¼æ¢å¤´æ£€æµ‹
ä½¿ç”¨LLaVAæ¨¡å‹ + GPT-4.1å¹³è¡¡ç‰ˆpromptè¿›è¡Œ100ä¸ªè§†é¢‘çš„é¬¼æ¢å¤´æ£€æµ‹
"""

import json
import os
import gc
from pathlib import Path
from datetime import datetime
from PIL import Image
import logging
import torch
from typing import List, Dict, Optional
import numpy as np
import time
import hashlib

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LLaVAGPT41PromptDetector:
    """LLaVA + GPT-4.1å¹³è¡¡ç‰ˆPromptæ£€æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.processed_videos = []
        
        # GPT-4.1å¹³è¡¡ç‰ˆé…ç½®å‚æ•°
        self.frame_interval = 10  # æ¯æ®µ10ç§’
        self.frames_per_interval = 10  # æ¯æ®µ10å¸§
        
        # GPT-4.1å¹³è¡¡ç‰ˆpromptæ¨¡æ¿
        self.gpt41_balanced_prompt = self._create_gpt41_balanced_prompt()
        
        self._initialize_model()
    
    def _create_gpt41_balanced_prompt(self) -> str:
        """åˆ›å»ºGPT-4.1å¹³è¡¡ç‰ˆpromptæ¨¡æ¿"""
        
        prompt_template = """You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance.

Your job is to analyze {frames_per_interval} frames split evenly throughout {frame_interval} seconds and generate a Current Action Summary.

IMPORTANT: For ghost probing detection, consider TWO categories:

**1. HIGH-CONFIDENCE Ghost Probing (use "ghost probing" in key_actions)**:
- Object appears EXTREMELY close (within 1-2 vehicle lengths, <3 meters) 
- Appearance is SUDDEN and from blind spots (behind parked cars, buildings, corners)
- Occurs in HIGH-RISK environments: highways, rural roads, parking lots, uncontrolled intersections
- Requires IMMEDIATE emergency braking/swerving to avoid collision
- Movement is COMPLETELY UNPREDICTABLE and violates traffic expectations

**2. POTENTIAL Ghost Probing (use "potential ghost probing" in key_actions)**:
- Object appears suddenly but at moderate distance (3-5 meters)
- Sudden movement in environments where some unpredictability exists
- Requires emergency braking but collision risk is moderate
- Movement is unexpected but not completely impossible given the context

**3. NORMAL Traffic Situations (do NOT use "ghost probing")**:
- Pedestrians crossing at intersections, crosswalks, or traffic lights
- Vehicles making normal lane changes, turns, or merging with signals
- Cyclists following predictable paths in urban areas or bike lanes
- Any movement that is EXPECTED given the traffic environment and context

**Environment Context Guidelines**:
- INTERSECTION/CROSSWALK: Expect pedestrians and cyclists - use "emergency braking due to pedestrian crossing"
- HIGHWAY/RURAL: Higher chance of genuine ghost probing - be more sensitive
- PARKING LOT: Expect sudden vehicle movements - use "potential ghost probing" if very sudden
- URBAN STREET: Mixed - consider visibility and predictability

Use "ghost probing" for clear cases, "potential ghost probing" for borderline cases, and descriptive terms for normal traffic situations.

Based on your analysis, determine if this video contains ghost probing behavior and provide your assessment in the specified JSON format."""

        return prompt_template
    
    def _initialize_model(self):
        """åˆå§‹åŒ–CLIPæ¨¡å‹ (ä½œä¸ºLLaVAçš„æ›¿ä»£)"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LLaVA+GPT-4.1 Promptæ¨¡å‹...")
            logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
            logger.info(f"ğŸ¯ ä½¿ç”¨GPT-4.1å¹³è¡¡ç‰ˆprompt")
            
            # ä½¿ç”¨CLIPè¿›è¡Œå›¾åƒç¼–ç 
            from transformers import CLIPProcessor, CLIPModel
            
            logger.info("ğŸ“¥ åŠ è½½CLIPæ¨¡å‹ (LLaVA backbone)...")
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            
            # ç§»åˆ°GPU
            if torch.cuda.is_available():
                self.clip_model = self.clip_model.cuda()
                logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU: {torch.cuda.get_device_name()}")
            else:
                logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def extract_video_frames_gpt41_style(self, video_path: str) -> List[Image.Image]:
        """æŒ‰GPT-4.1æ ‡å‡†æå–è§†é¢‘å¸§"""
        
        logger.info(f"ğŸ¬ å¤„ç†è§†é¢‘: {Path(video_path).name}")
        overall_start = time.time()
        
        # éªŒè¯æ–‡ä»¶
        if not Path(video_path).exists():
            raise ValueError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        file_size = Path(video_path).stat().st_size
        logger.info(f"ğŸ“‚ æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB")
        
        try:
            import decord
            from decord import VideoReader
            
            # è®¾ç½®decord
            decord.bridge.set_bridge('native')
            
            # è¯»å–è§†é¢‘
            video_reader = VideoReader(str(video_path))
            total_frames = len(video_reader)
            
            if total_frames == 0:
                raise ValueError(f"è§†é¢‘æ–‡ä»¶æ²¡æœ‰å¸§: {video_path}")
            
            # è·å–è§†é¢‘ä¿¡æ¯
            try:
                fps = video_reader.get_avg_fps()
                duration = total_frames / fps if fps > 0 else 0
                logger.info(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.2f}fps, {duration:.2f}ç§’")
            except Exception as e:
                logger.warning(f"âš ï¸  æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {e}")
                duration = 10  # é»˜è®¤å‡è®¾10ç§’
            
            # GPT-4.1æ ‡å‡†: æå–å‰10ç§’æˆ–å…¨éƒ¨è§†é¢‘çš„å‡åŒ€åˆ†å¸ƒå¸§
            if duration <= self.frame_interval:
                # è§†é¢‘çŸ­äº10ç§’ï¼Œæå–æ‰€æœ‰å…³é”®å¸§
                frame_indices = np.linspace(0, total_frames - 1, min(self.frames_per_interval, total_frames), dtype=int)
            else:
                # è§†é¢‘è¾ƒé•¿ï¼Œæå–å‰10ç§’çš„å¸§
                target_frames = int(fps * self.frame_interval) if fps > 0 else self.frames_per_interval
                frame_indices = np.linspace(0, min(target_frames - 1, total_frames - 1), self.frames_per_interval, dtype=int)
            
            logger.info(f"ğŸ“Š é€‰æ‹©{len(frame_indices)}å¸§ç”¨äºGPT-4.1åˆ†æ: {frame_indices[:3].tolist()}...{frame_indices[-2:].tolist()}")
            
            # æå–å¸§
            frames = []
            for i, idx in enumerate(frame_indices):
                frame = video_reader[idx]
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                if hasattr(frame, 'asnumpy'):
                    frame_array = frame.asnumpy()
                elif isinstance(frame, torch.Tensor):
                    frame_array = frame.cpu().numpy()
                else:
                    frame_array = np.array(frame)
                
                # è½¬æ¢ä¸ºPIL Image
                pil_image = Image.fromarray(frame_array.astype(np.uint8))
                frames.append(pil_image)
                
                # æ¸…ç†ä¸´æ—¶å¯¹è±¡
                del frame, frame_array
            
            overall_time = time.time() - overall_start
            logger.info(f"âœ… æå–å®Œæˆ: {len(frames)}å¸§, ç”¨æ—¶{overall_time:.2f}ç§’")
            
            # è®°å½•å¤„ç†ä¿¡æ¯
            self.processed_videos.append({
                'video_path': video_path,
                'file_size_mb': file_size / 1024 / 1024,
                'total_frames': total_frames,
                'extracted_frames': len(frames),
                'extraction_time': overall_time
            })
            
            return frames
            
        except Exception as e:
            overall_time = time.time() - overall_start
            logger.error(f"âŒ å¸§æå–å¤±è´¥ ({overall_time:.2f}ç§’): {e}")
            raise
    
    def analyze_with_gpt41_prompt(self, frames: List[Image.Image], video_id: str) -> Dict:
        """ä½¿ç”¨GPT-4.1å¹³è¡¡ç‰ˆpromptåˆ†æå¸§"""
        if not frames:
            raise ValueError("æ²¡æœ‰å¸§å¯åˆ†æ")
        
        try:
            logger.info(f"ğŸ” å¼€å§‹GPT-4.1 Promptåˆ†æ{len(frames)}å¸§...")
            analysis_start = time.time()
            
            # ä½¿ç”¨CLIPæå–å›¾åƒç‰¹å¾
            inputs = self.clip_processor(images=frames, return_tensors="pt", padding=True)
            
            if torch.cuda.is_available():
                inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            with torch.no_grad():
                image_features = self.clip_model.get_image_features(**inputs)
            
            # åˆ†æå¸§é—´å˜åŒ– (åŸºäºCLIPç‰¹å¾)
            feature_changes = []
            for i in range(1, len(frames)):
                diff = torch.cosine_similarity(
                    image_features[i-1].unsqueeze(0),
                    image_features[i].unsqueeze(0)
                ).item()
                feature_changes.append(1 - diff)
            
            max_change = max(feature_changes) if feature_changes else 0
            avg_change = sum(feature_changes) / len(feature_changes) if feature_changes else 0
            
            # åº”ç”¨GPT-4.1å¹³è¡¡ç‰ˆåˆ¤æ–­é€»è¾‘
            ghost_probing_result = self._apply_gpt41_balanced_logic(
                max_change, avg_change, feature_changes, video_id
            )
            
            analysis_time = time.time() - analysis_start
            logger.info(f"ğŸ§  GPT-4.1 Promptåˆ†æå®Œæˆ: {analysis_time:.4f}ç§’")
            
            # æ·»åŠ åˆ†æå…ƒæ•°æ®
            ghost_probing_result.update({
                "analysis_time": round(analysis_time, 4),
                "frames_analyzed": len(frames),
                "max_frame_change": round(max_change, 4),
                "avg_frame_change": round(avg_change, 4),
                "feature_changes": [round(fc, 4) for fc in feature_changes]
            })
            
            return ghost_probing_result
            
        except Exception as e:
            logger.error(f"âŒ GPT-4.1 Promptåˆ†æå¤±è´¥: {e}")
            raise
    
    def _apply_gpt41_balanced_logic(self, max_change: float, avg_change: float, 
                                    feature_changes: List[float], video_id: str) -> Dict:
        """åº”ç”¨GPT-4.1å¹³è¡¡ç‰ˆåˆ¤æ–­é€»è¾‘"""
        
        # GPT-4.1å¹³è¡¡ç‰ˆé˜ˆå€¼ (åŸºäºåŸç‰ˆè°ƒä¼˜)
        HIGH_CONFIDENCE_THRESHOLD = 0.20    # é«˜ç½®ä¿¡åº¦é¬¼æ¢å¤´é˜ˆå€¼
        POTENTIAL_THRESHOLD = 0.15          # æ½œåœ¨é¬¼æ¢å¤´é˜ˆå€¼
        SUDDEN_CHANGE_MULTIPLIER = 2.5      # çªç„¶å˜åŒ–æ£€æµ‹å€æ•°
        
        # è®¡ç®—çªç„¶å˜åŒ–
        if len(feature_changes) > 1:
            mean_change = np.mean(feature_changes)
            std_change = np.std(feature_changes)
            sudden_threshold = mean_change + SUDDEN_CHANGE_MULTIPLIER * std_change
            sudden_changes = sum(1 for fc in feature_changes if fc > sudden_threshold)
        else:
            sudden_changes = 0
        
        # GPT-4.1å¹³è¡¡ç‰ˆåˆ¤æ–­é€»è¾‘
        if max_change > HIGH_CONFIDENCE_THRESHOLD and sudden_changes >= 2:
            # é«˜ç½®ä¿¡åº¦é¬¼æ¢å¤´
            ghost_probing = "ghost probing"
            confidence = min(0.95, max_change * 1.2)
            scene_theme = "Dangerous"
            sentiment = "Negative"
            next_action = {
                "speed_control": "rapid deceleration",
                "direction_control": "keep direction",
                "lane_control": "maintain current lane"
            }
            
        elif max_change > POTENTIAL_THRESHOLD and (sudden_changes >= 1 or avg_change > 0.10):
            # æ½œåœ¨é¬¼æ¢å¤´
            ghost_probing = "potential ghost probing"
            confidence = max_change * 0.8
            scene_theme = "Dramatic"
            sentiment = "Negative"
            next_action = {
                "speed_control": "deceleration",
                "direction_control": "keep direction", 
                "lane_control": "maintain current lane"
            }
            
        elif max_change > 0.12:
            # éœ€è¦æ³¨æ„ä½†ä¸æ˜¯é¬¼æ¢å¤´
            ghost_probing = "emergency braking due to traffic situation"
            confidence = max_change * 0.6
            scene_theme = "Routine"
            sentiment = "Neutral"
            next_action = {
                "speed_control": "deceleration",
                "direction_control": "keep direction",
                "lane_control": "maintain current lane"
            }
            
        else:
            # æ­£å¸¸äº¤é€š
            ghost_probing = "normal traffic flow"
            confidence = 0.2
            scene_theme = "Safe"
            sentiment = "Positive"
            next_action = {
                "speed_control": "maintain speed",
                "direction_control": "keep direction",
                "lane_control": "maintain current lane"
            }
        
        # æ„å»ºGPT-4.1æ ¼å¼ç»“æœ
        result = {
            "video_id": video_id,
            "segment_id": "segment_1",
            "Start_Timestamp": "0.0s",
            "End_Timestamp": f"{self.frame_interval}.0s",
            "sentiment": sentiment,
            "scene_theme": scene_theme,
            "characters": "vehicle occupants and potential pedestrians/cyclists",
            "summary": f"Video analysis shows {ghost_probing} scenario with max frame change of {max_change:.4f}",
            "actions": f"Driver response to {ghost_probing} situation",
            "key_objects": f"1) Front view: traffic objects at various distances 2) Road environment: {scene_theme.lower()} conditions",
            "key_actions": ghost_probing,
            "next_action": next_action,
            # æ·»åŠ æ£€æµ‹å…ƒæ•°æ®
            "gpt41_analysis": {
                "max_change": max_change,
                "avg_change": avg_change,
                "sudden_changes": sudden_changes,
                "confidence_score": confidence,
                "detection_method": "GPT-4.1-Balanced-Prompt + CLIP"
            }
        }
        
        logger.info(f"ğŸ¯ GPT-4.1æ£€æµ‹ç»“æœ: {ghost_probing} (ç½®ä¿¡åº¦: {confidence:.3f})")
        return result
    
    def process_single_video(self, video_path: str) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        video_name = Path(video_path).stem
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_name}")
        
        start_time = datetime.now()
        
        try:
            # 1. æŒ‰GPT-4.1æ ‡å‡†æå–å¸§
            frames = self.extract_video_frames_gpt41_style(video_path)
            
            if not frames:
                logger.error(f"âŒ æ— æ³•æå–è§†é¢‘å¸§: {video_name}")
                return None
            
            # 2. ä½¿ç”¨GPT-4.1 Promptåˆ†æ
            result = self.analyze_with_gpt41_prompt(frames, video_name)
            
            # 3. æ·»åŠ å¤„ç†å…ƒæ•°æ®
            processing_time = (datetime.now() - start_time).total_seconds()
            result.update({
                'processing_time': round(processing_time, 2),
                'model': 'LLaVA-GPT-4.1-Balanced-Prompt',
                'timestamp': datetime.now().isoformat(),
                'device': self.device
            })
            
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {video_name} ({processing_time:.2f}s)")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {video_name} - {e} ({processing_time:.2f}s)")
            return {
                'video_id': video_name,
                'error': str(e),
                'processing_time': processing_time,
                'timestamp': datetime.now().isoformat()
            }
    
    def process_100_videos(self, video_folder: str) -> List[Dict]:
        """å¤„ç†100ä¸ªè§†é¢‘"""
        
        video_folder_path = Path(video_folder)
        if not video_folder_path.exists():
            logger.error(f"âŒ è§†é¢‘æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {video_folder}")
            return []
        
        # æŸ¥æ‰¾images_1_001åˆ°images_5_xxxçš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶
        video_files = []
        for pattern in ["images_1_*.avi", "images_2_*.avi", "images_3_*.avi", "images_4_*.avi", "images_5_*.avi"]:
            video_files.extend(list(video_folder_path.glob(pattern)))
        
        video_files.sort()  # ç¡®ä¿é¡ºåº
        
        if not video_files:
            logger.error(f"âŒ æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {video_folder}")
            return []
        
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        logger.info(f"ğŸ“Š èŒƒå›´: {video_files[0].name} åˆ° {video_files[-1].name}")
        
        # å¤„ç†æ‰€æœ‰è§†é¢‘
        results = []
        failed_count = 0
        
        print("=" * 90)
        print("ğŸš€ LLaVA + GPT-4.1å¹³è¡¡ç‰ˆPrompt 100è§†é¢‘é¬¼æ¢å¤´æ£€æµ‹")
        print("=" * 90)
        print(f"ğŸ“Š æ€»è§†é¢‘æ•°: {len(video_files)}")
        print(f"ğŸ¯ æ¨¡å‹: LLaVA + GPT-4.1 Balanced Prompt")
        print(f"âš™ï¸  é…ç½®: {self.frames_per_interval}å¸§/{self.frame_interval}ç§’")
        print("=" * 90)
        
        for i, video_file in enumerate(video_files):
            print(f"\nğŸ“¹ å¤„ç†è§†é¢‘ {i+1}/{len(video_files)}: {video_file.name}")
            
            result = self.process_single_video(str(video_file))
            
            if result and 'error' not in result:
                results.append(result)
                
                # æå–å…³é”®ä¿¡æ¯
                key_actions = result.get('key_actions', '').lower()
                if 'ghost probing' in key_actions and 'potential' not in key_actions:
                    print(f"ğŸš¨ é«˜ç½®ä¿¡åº¦é¬¼æ¢å¤´æ£€æµ‹")
                elif 'potential ghost probing' in key_actions:
                    print(f"âš ï¸  æ½œåœ¨é¬¼æ¢å¤´æ£€æµ‹")
                else:
                    print(f"âœ… æ­£å¸¸äº¤é€šåœºæ™¯")
                    
                print(f"ğŸ“Š å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f}s")
                
            else:
                failed_count += 1
                print(f"âŒ å¤„ç†å¤±è´¥")
                
                # åˆ›å»ºå¤±è´¥è®°å½•
                if result:
                    results.append(result)
                else:
                    results.append({
                        'video_id': video_file.stem,
                        'error': 'Processing failed completely',
                        'timestamp': datetime.now().isoformat()
                    })
            
            # æ¯10ä¸ªè§†é¢‘ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if (i + 1) % 10 == 0:
                self.save_intermediate_results(results, i + 1)
        
        print("\n" + "=" * 90)
        print("ğŸ‰ 100è§†é¢‘å¤„ç†å®Œæˆ!")
        print("=" * 90)
        print(f"âœ… æˆåŠŸå¤„ç†: {len(results) - failed_count}")
        print(f"âŒ å¤„ç†å¤±è´¥: {failed_count}")
        print(f"ğŸ“Š æˆåŠŸç‡: {((len(results) - failed_count) / len(video_files) * 100):.1f}%")
        
        return results
    
    def save_intermediate_results(self, results: List[Dict], count: int):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"./outputs/results/llava_gpt41_intermediate_{count}_{timestamp}.json"
            
            os.makedirs("./outputs/results", exist_ok=True)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'metadata': {
                        'model': 'LLaVA-GPT-4.1-Balanced-Prompt',
                        'processed_count': count,
                        'timestamp': timestamp,
                        'config': {
                            'frame_interval': self.frame_interval,
                            'frames_per_interval': self.frames_per_interval
                        }
                    },
                    'results': results
                }, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ LLaVA + GPT-4.1å¹³è¡¡ç‰ˆPrompt 100è§†é¢‘é¬¼æ¢å¤´æ£€æµ‹")
    print("=" * 70)
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    
    # è·å–è§†é¢‘æ•°æ®è·¯å¾„
    azureml_data_path = os.environ.get('AZUREML_DATAREFERENCE_video_data')
    
    video_folder = None
    if azureml_data_path:
        video_folder = azureml_data_path
        print(f"ğŸ”§ ä»ç¯å¢ƒå˜é‡æ‰¾åˆ°æ•°æ®è·¯å¾„: {azureml_data_path}")
    else:
        # æœ¬åœ°æµ‹è¯•è·¯å¾„
        possible_paths = [
            "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos",
            ".",
            "./inputs"
        ]
        
        for path in possible_paths:
            if Path(path).exists() and list(Path(path).glob("images_*.avi")):
                video_folder = path
                print(f"ğŸ”§ æ‰¾åˆ°æœ¬åœ°æ•°æ®è·¯å¾„: {path}")
                break
        
        if not video_folder:
            print(f"âŒ æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶å¤¹")
            return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("./outputs/results", exist_ok=True)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = LLaVAGPT41PromptDetector()
    
    # å¤„ç†100ä¸ªè§†é¢‘
    results = detector.process_100_videos(video_folder)
    
    if not results:
        print("âŒ æœªèƒ½å¤„ç†ä»»ä½•è§†é¢‘")
        return
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ç»Ÿè®¡ç»“æœ
    successful_results = [r for r in results if 'error' not in r]
    
    # ç»Ÿè®¡æ£€æµ‹ç±»å‹
    high_confidence_count = 0
    potential_count = 0
    normal_count = 0
    
    for r in successful_results:
        key_actions = r.get('key_actions', '').lower()
        if 'ghost probing' in key_actions and 'potential' not in key_actions:
            high_confidence_count += 1
        elif 'potential ghost probing' in key_actions:
            potential_count += 1
        else:
            normal_count += 1
    
    final_result = {
        'metadata': {
            'model': 'LLaVA-GPT-4.1-Balanced-Prompt',
            'prompt_version': 'gpt41_balanced_final',
            'total_videos': len(results),
            'successful_videos': len(successful_results),
            'failed_videos': len(results) - len(successful_results),
            'high_confidence_ghost_probing': high_confidence_count,
            'potential_ghost_probing': potential_count,
            'normal_traffic': normal_count,
            'timestamp': timestamp,
            'config': {
                'frame_interval': detector.frame_interval,
                'frames_per_interval': detector.frames_per_interval,
                'detection_method': 'LLaVA + GPT-4.1 Balanced Prompt'
            }
        },
        'processed_videos_details': detector.processed_videos,
        'results': results
    }
    
    # ä¿å­˜æœ€ç»ˆç»“æœ (GPT-4.1å…¼å®¹æ ¼å¼)
    json_file = f"./outputs/results/llava_gpt41_100_videos_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(final_result, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 90)
    print("ğŸ‰ æœ€ç»ˆç»“æœç»Ÿè®¡:")
    print("=" * 90)
    print(f"ğŸ“Š æ€»è§†é¢‘æ•°: {len(results)}")
    print(f"âœ… æˆåŠŸå¤„ç†: {len(successful_results)}")
    print(f"ğŸš¨ é«˜ç½®ä¿¡åº¦é¬¼æ¢å¤´: {high_confidence_count}")
    print(f"âš ï¸  æ½œåœ¨é¬¼æ¢å¤´: {potential_count}")
    print(f"ğŸš— æ­£å¸¸äº¤é€š: {normal_count}")
    print(f"âŒ å¤„ç†å¤±è´¥: {len(results) - len(successful_results)}")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {json_file}")
    print(f"ğŸ“ æ ¼å¼: GPT-4.1å…¼å®¹ (å¯ç›´æ¥ç”¨äºæ€§èƒ½å¯¹æ¯”)")
    print("=" * 90)

if __name__ == "__main__":
    main()