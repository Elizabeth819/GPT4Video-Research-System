# GPUæ˜¾å­˜ç›‘æ§LLaVAæ£€æµ‹ä½œä¸š
$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
type: command

# Job metadata
display_name: gpu-memory-monitor-llava
description: "LLaVA Ghost Probing Detection with detailed GPU memory monitoring"

# Compute configuration  
compute: azureml:llava-a100-low-priority
environment: azureml:AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu:10

# Resource requirements
resources:
  instance_count: 1

# Code source
code: .

# GPUæ˜¾å­˜ç›‘æ§ä½œä¸š
command: >
  echo "ğŸ–¥ï¸ å¼€å§‹GPUæ˜¾å­˜ç›‘æ§LLaVAä½œä¸š" &&
  python --version &&
  echo "ğŸ“‹ æ£€æŸ¥GPUå’ŒCUDAç¯å¢ƒ..." &&
  nvidia-smi &&
  python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}'); print(f'GPUæ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024:.0f}MB' if torch.cuda.is_available() else 'CPUæ¨¡å¼')" &&
  echo "ğŸ”§ å®‰è£…ä¾èµ–åŒ…..." &&
  pip install decord transformers accelerate Pillow &&
  echo "ğŸ“ æ£€æŸ¥è¾“å…¥æ•°æ®è·¯å¾„..." &&
  env | grep AZUREML_DATAREFERENCE_video_data &&
  echo "ğŸ” è¿è¡ŒGPUæ˜¾å­˜ç›‘æ§ç‰ˆæ£€æµ‹..." &&
  timeout 7200 python gpu_memory_monitor.py &&
  echo "âœ… GPUæ˜¾å­˜ç›‘æ§ä½œä¸šå®Œæˆ"

# Input data  
inputs:
  video_data:
    type: uri_folder
    path: azureml:dada-100-videos-fixed:1
    mode: ro_mount

# Output data
outputs:
  results:
    type: uri_folder
    mode: rw_mount

# Environment variables
environment_variables:
  CUDA_VISIBLE_DEVICES: "0"
  TRANSFORMERS_CACHE: "/tmp/transformers"
  TORCH_CUDA_ARCH_LIST: "7.0;7.5;8.0;8.6"
  PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128"

# Experiment settings
experiment_name: gpu-memory-monitor-experiment