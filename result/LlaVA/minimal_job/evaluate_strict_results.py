#!/usr/bin/env python3
"""
è¯„ä¼°ä¸¥æ ¼éªŒè¯LLaVAæ£€æµ‹ç»“æœ
è®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ç­‰æŒ‡æ ‡
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np

def load_ground_truth(csv_path: str) -> Dict[str, bool]:
    """åŠ è½½ground truthæ ‡ç­¾"""
    try:
        df = pd.read_csv(csv_path, sep='\t')  # ä½¿ç”¨tabåˆ†éš”ç¬¦
        print(f"ğŸ“Š è¯»å–ground truthæ–‡ä»¶: {csv_path}")
        print(f"ğŸ“Š åˆ—å: {df.columns.tolist()}")
        
        gt_labels = {}
        for _, row in df.iterrows():
            video_id = str(row['video_id']).replace('.avi', '')  # ç§»é™¤åç¼€ç»Ÿä¸€æ ¼å¼
            label = str(row['ground_truth_label']).lower()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºé¬¼æ¢å¤´
            has_ghost_probing = (
                'ghost probing' in label or 
                'ghost' in label or
                ('s:' in label and 'none' not in label and 'cut-in' not in label)
            )
            gt_labels[video_id] = has_ghost_probing
            
        print(f"ğŸ“Š åŠ è½½äº† {len(gt_labels)} ä¸ªground truthæ ‡ç­¾")
        
        # ç»Ÿè®¡ground truthåˆ†å¸ƒ
        ghost_count = sum(gt_labels.values())
        normal_count = len(gt_labels) - ghost_count
        print(f"ğŸ“Š Ground Truthåˆ†å¸ƒ:")
        print(f"   - é¬¼æ¢å¤´è§†é¢‘: {ghost_count}")
        print(f"   - æ­£å¸¸è§†é¢‘: {normal_count}")
        print(f"   - é¬¼æ¢å¤´æ¯”ä¾‹: {ghost_count/len(gt_labels)*100:.1f}%")
        
        return gt_labels
        
    except Exception as e:
        print(f"âŒ åŠ è½½ground truthå¤±è´¥: {e}")
        return {}

def load_strict_results(json_path: str) -> List[Dict]:
    """åŠ è½½ä¸¥æ ¼éªŒè¯æ£€æµ‹ç»“æœ"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        results = data.get('results', [])
        print(f"ğŸ“Š åŠ è½½äº† {len(results)} ä¸ªæ£€æµ‹ç»“æœ")
        
        return results
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ£€æµ‹ç»“æœå¤±è´¥: {e}")
        return []

def evaluate_performance(gt_labels: Dict[str, bool], detection_results: List[Dict]) -> Dict:
    """è¯„ä¼°æ£€æµ‹æ€§èƒ½"""
    
    # å‡†å¤‡è¯„ä¼°æ•°æ®
    matched_results = []
    
    for result in detection_results:
        video_id = result.get('video_id', '').replace('.avi', '')
        
        if video_id in gt_labels:
            # æ£€æµ‹ç»“æœ
            detected = result.get('ghost_probing_label', 'no').lower() == 'yes'
            confidence = result.get('confidence', 0.0)
            
            # Ground truth
            ground_truth = gt_labels[video_id]
            
            matched_results.append({
                'video_id': video_id,
                'ground_truth': ground_truth,
                'detected': detected,
                'confidence': confidence,
                'max_frame_change': result.get('max_frame_change', 0),
                'avg_frame_change': result.get('avg_frame_change', 0)
            })
    
    print(f"ğŸ“Š åŒ¹é…åˆ° {len(matched_results)} ä¸ªæœ‰ground truthçš„æ£€æµ‹ç»“æœ")
    
    if not matched_results:
        return {"error": "æ²¡æœ‰åŒ¹é…çš„ç»“æœ"}
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tp = sum(1 for r in matched_results if r['ground_truth'] and r['detected'])      # True Positive
    tn = sum(1 for r in matched_results if not r['ground_truth'] and not r['detected'])  # True Negative  
    fp = sum(1 for r in matched_results if not r['ground_truth'] and r['detected'])      # False Positive
    fn = sum(1 for r in matched_results if r['ground_truth'] and not r['detected'])      # False Negative
    
    total = len(matched_results)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    accuracy = (tp + tn) / total if total > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # ç»Ÿè®¡ç‰¹å¾å˜åŒ–
    ghost_videos = [r for r in matched_results if r['ground_truth']]
    normal_videos = [r for r in matched_results if not r['ground_truth']]
    
    ghost_max_changes = [r['max_frame_change'] for r in ghost_videos]
    normal_max_changes = [r['max_frame_change'] for r in normal_videos]
    
    results = {
        'total_videos': total,
        'confusion_matrix': {
            'true_positive': tp,
            'true_negative': tn, 
            'false_positive': fp,
            'false_negative': fn
        },
        'performance_metrics': {
            'accuracy': round(accuracy, 4),
            'precision': round(precision, 4),
            'recall': round(recall, 4),
            'specificity': round(specificity, 4),
            'f1_score': round(f1_score, 4)
        },
        'feature_analysis': {
            'ghost_videos_max_change': {
                'mean': round(np.mean(ghost_max_changes), 4) if ghost_max_changes else 0,
                'std': round(np.std(ghost_max_changes), 4) if ghost_max_changes else 0,
                'max': round(max(ghost_max_changes), 4) if ghost_max_changes else 0,
                'min': round(min(ghost_max_changes), 4) if ghost_max_changes else 0
            },
            'normal_videos_max_change': {
                'mean': round(np.mean(normal_max_changes), 4) if normal_max_changes else 0,
                'std': round(np.std(normal_max_changes), 4) if normal_max_changes else 0,
                'max': round(max(normal_max_changes), 4) if normal_max_changes else 0,
                'min': round(min(normal_max_changes), 4) if normal_max_changes else 0
            }
        },
        'detailed_results': matched_results
    }
    
    return results

def print_evaluation_report(eval_results: Dict):
    """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
    
    print("\n" + "="*80)
    print("ğŸ¯ ä¸¥æ ¼éªŒè¯LLaVAé¬¼æ¢å¤´æ£€æµ‹æ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
    print("="*80)
    
    if 'error' in eval_results:
        print(f"âŒ è¯„ä¼°é”™è¯¯: {eval_results['error']}")
        return
    
    # åŸºæœ¬ä¿¡æ¯
    total = eval_results['total_videos']
    cm = eval_results['confusion_matrix']
    metrics = eval_results['performance_metrics']
    
    print(f"ğŸ“Š è¯„ä¼°è§†é¢‘æ€»æ•°: {total}")
    print()
    
    # æ··æ·†çŸ©é˜µ
    print("ğŸ“‹ æ··æ·†çŸ©é˜µ:")
    print(f"   çœŸé˜³æ€§ (TP): {cm['true_positive']:3d}  |  å‡é˜³æ€§ (FP): {cm['false_positive']:3d}")
    print(f"   å‡é˜´æ€§ (FN): {cm['false_negative']:3d}  |  çœŸé˜´æ€§ (TN): {cm['true_negative']:3d}")
    print()
    
    # æ€§èƒ½æŒ‡æ ‡
    print("ğŸ¯ æ€§èƒ½æŒ‡æ ‡:")
    print(f"   å‡†ç¡®ç‡ (Accuracy):  {metrics['accuracy']:.1%}")
    print(f"   ç²¾ç¡®åº¦ (Precision): {metrics['precision']:.1%}")
    print(f"   å¬å›ç‡ (Recall):    {metrics['recall']:.1%}")
    print(f"   ç‰¹å¼‚æ€§ (Specificity): {metrics['specificity']:.1%}")
    print(f"   F1åˆ†æ•° (F1-Score):  {metrics['f1_score']:.1%}")
    print()
    
    # ç‰¹å¾åˆ†æ
    feature_analysis = eval_results['feature_analysis']
    ghost_stats = feature_analysis['ghost_videos_max_change']
    normal_stats = feature_analysis['normal_videos_max_change']
    
    print("ğŸ” ç‰¹å¾å˜åŒ–åˆ†æ:")
    print(f"   é¬¼æ¢å¤´è§†é¢‘æœ€å¤§å˜åŒ–: å‡å€¼={ghost_stats['mean']:.4f}, æ ‡å‡†å·®={ghost_stats['std']:.4f}")
    print(f"                     æœ€å¤§å€¼={ghost_stats['max']:.4f}, æœ€å°å€¼={ghost_stats['min']:.4f}")
    print(f"   æ­£å¸¸è§†é¢‘æœ€å¤§å˜åŒ–:   å‡å€¼={normal_stats['mean']:.4f}, æ ‡å‡†å·®={normal_stats['std']:.4f}")
    print(f"                     æœ€å¤§å€¼={normal_stats['max']:.4f}, æœ€å°å€¼={normal_stats['min']:.4f}")
    print()
    
    # é—®é¢˜åˆ†æ
    print("âš ï¸  é—®é¢˜åˆ†æ:")
    if metrics['recall'] < 0.1:
        print("   - å¬å›ç‡æä½ï¼Œå‡ ä¹æ— æ³•æ£€æµ‹åˆ°çœŸå®çš„é¬¼æ¢å¤´")
    if metrics['precision'] < 0.1:
        print("   - ç²¾ç¡®åº¦æä½ï¼Œå­˜åœ¨å¤§é‡è¯¯æŠ¥")
    if ghost_stats['max'] < 0.3:
        print("   - é¬¼æ¢å¤´è§†é¢‘çš„ç‰¹å¾å˜åŒ–ä»ç„¶å¾ˆå°ï¼Œå¯èƒ½éœ€è¦æ›´æ•æ„Ÿçš„æ£€æµ‹æ–¹æ³•")
    if normal_stats['max'] > 0.2:
        print("   - æ­£å¸¸è§†é¢‘ä¹Ÿæœ‰è¾ƒå¤§çš„ç‰¹å¾å˜åŒ–ï¼Œå¯èƒ½å½±å“åˆ¤æ–­é˜ˆå€¼è®¾ç½®")
    
    print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¼€å§‹è¯„ä¼°ä¸¥æ ¼éªŒè¯LLaVAæ£€æµ‹ç»“æœ...")
    
    # æ–‡ä»¶è·¯å¾„
    gt_file = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/groundtruth_labels.csv"
    strict_results_file = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/LlaVA/minimal_job/strict_validation_results/artifacts/outputs/results/strict_llava_results_20250722_021252.json"
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not Path(gt_file).exists():
        print(f"âŒ Ground truthæ–‡ä»¶ä¸å­˜åœ¨: {gt_file}")
        return
    
    if not Path(strict_results_file).exists():
        print(f"âŒ æ£€æµ‹ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {strict_results_file}")
        return
    
    # åŠ è½½æ•°æ®
    gt_labels = load_ground_truth(gt_file)
    if not gt_labels:
        print("âŒ æ— æ³•åŠ è½½ground truthæ ‡ç­¾")
        return
    
    detection_results = load_strict_results(strict_results_file)
    if not detection_results:
        print("âŒ æ— æ³•åŠ è½½æ£€æµ‹ç»“æœ")
        return
    
    # è¯„ä¼°æ€§èƒ½
    eval_results = evaluate_performance(gt_labels, detection_results)
    
    # æ‰“å°æŠ¥å‘Š
    print_evaluation_report(eval_results)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = "strict_validation_evaluation_report.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(eval_results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()