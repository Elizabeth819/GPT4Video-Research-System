#!/usr/bin/env python3
"""
çœŸæ­£çš„LLaVAé¬¼æ¢å¤´æ£€æµ‹å™¨ - GPUç‰ˆæœ¬ä¿®å¤
ä¿®å¤decordè¿”å›tensorçš„é—®é¢˜ï¼Œä¿æŒGPUåŠ é€Ÿ
"""

import json
import os
import sys
from pathlib import Path
from datetime import datetime
from PIL import Image
import logging
import torch
from typing import List, Dict, Optional, Tuple
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RealLLaVADetector:
    """çœŸå®çš„LLaVAé¬¼æ¢å¤´æ£€æµ‹å™¨ - GPUåŠ é€Ÿç‰ˆ"""
    
    def __init__(self, model_path: str = "lmms-lab/LLaVA-NeXT-Video-7B-DPO"):
        """
        åˆå§‹åŒ–LLaVAæ£€æµ‹å™¨
        
        Args:
            model_path: LLaVAæ¨¡å‹è·¯å¾„
        """
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # å¹³è¡¡ç‰ˆGPT-4.1é¬¼æ¢å¤´æ£€æµ‹prompt
        self.ghost_probing_prompt = """You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a driving video. Focus on changes in relative positions, distances, and speeds of objects, particularly vehicles and pedestrians, and how these might indicate potential collision risks.

IMPORTANT: For ghost probing detection, consider TWO categories:

**1. HIGH-CONFIDENCE Ghost Probing (use "ghost probing" in analysis)**:
- Object appears EXTREMELY close (within 1-2 vehicle lengths, <3 meters) 
- Appearance is SUDDEN and from blind spots (behind parked cars, buildings, corners)
- Occurs in HIGH-RISK environments: highways, rural roads, parking lots, uncontrolled intersections
- Requires IMMEDIATE emergency braking/swerving to avoid collision
- Movement is COMPLETELY UNPREDICTABLE and violates traffic expectations

**2. POTENTIAL Ghost Probing (use "potential ghost probing" in analysis)**:
- Object appears suddenly but at moderate distance (3-5 meters)
- Sudden movement in environments where some unpredictability exists
- Requires emergency braking but collision risk is moderate
- Movement is unexpected but not completely impossible given the context

**3. NORMAL Traffic Situations (do NOT use "ghost probing")**:
- Pedestrians crossing at intersections, crosswalks, or traffic lights
- Vehicles making normal lane changes, turns, or merging with signals
- Cyclists following predictable paths in urban areas or bike lanes
- Any movement that is EXPECTED given the traffic environment and context

Analyze these sequential frames and respond with a JSON object:
{
    "ghost_probing_detected": "yes/no",
    "confidence": 0.0-1.0,
    "ghost_type": "high_confidence/potential/none",
    "summary": "brief description of the scene",
    "key_actions": "description of most important actions",
    "risk_level": "high/medium/low",
    "distance_estimate": "distance to closest threat in meters",
    "emergency_action_needed": "yes/no"
}"""
        
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–LLaVAæ¨¡å‹ - ä½¿ç”¨GPU"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–LLaVAæ¨¡å‹...")
            logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
            
            # å¯¼å…¥LLaVAç›¸å…³æ¨¡å—
            from transformers import AutoProcessor, LlavaNextVideoForConditionalGeneration
            
            # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ - ä½¿ç”¨GPU
            self.model = LlavaNextVideoForConditionalGeneration.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°GPU
                low_cpu_mem_usage=True
            )
            
            self.processor = AutoProcessor.from_pretrained(self.model_path)
            
            # ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š
            if torch.cuda.is_available():
                self.model = self.model.cuda()
                logger.info(f"âœ… LLaVAæ¨¡å‹å·²åŠ è½½åˆ°GPU: {torch.cuda.get_device_name()}")
            else:
                logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            
        except Exception as e:
            logger.error(f"âŒ LLaVAæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def extract_frames_with_decord(self, video_path: str, num_frames: int = 8) -> List[Image.Image]:
        """
        ä½¿ç”¨decordåº“æå–è§†é¢‘å¸§ - ä¿®å¤tensoré—®é¢˜
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            num_frames: æå–å¸§æ•°
            
        Returns:
            PIL Imageåˆ—è¡¨
        """
        try:
            import decord
            from decord import VideoReader
            
            # è®¾ç½®decordä½¿ç”¨native bridge (è¿”å›numpyæ•°ç»„)
            decord.bridge.set_bridge('native')
            
            # è¯»å–è§†é¢‘
            video_reader = VideoReader(str(video_path))
            total_frames = len(video_reader)
            
            if total_frames == 0:
                raise ValueError(f"è§†é¢‘æ–‡ä»¶æ²¡æœ‰å¸§: {video_path}")
            
            # å‡åŒ€åˆ†å¸ƒé€‰æ‹©å¸§
            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
            
            # æå–å¸§
            frames = []
            for idx in frame_indices:
                # è·å–å¸§ - ç°åœ¨åº”è¯¥è¿”å›numpyæ•°ç»„
                frame = video_reader[idx]
                
                # å¦‚æœè¿˜æ˜¯tensorï¼Œè½¬æ¢ä¸ºnumpy
                if hasattr(frame, 'asnumpy'):
                    frame_array = frame.asnumpy()
                elif isinstance(frame, torch.Tensor):
                    frame_array = frame.cpu().numpy()
                else:
                    frame_array = np.array(frame)
                
                # è½¬æ¢ä¸ºPIL Image
                pil_image = Image.fromarray(frame_array.astype(np.uint8))
                frames.append(pil_image)
            
            logger.info(f"âœ… ä½¿ç”¨decordä»è§†é¢‘ä¸­æå–äº†{len(frames)}å¸§")
            return frames
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘å¸§æå–å¤±è´¥: {e}")
            raise
    
    def analyze_frames_with_llava(self, frames: List[Image.Image], video_path: str) -> Dict:
        """
        ä½¿ç”¨LLaVAæ¨¡å‹åˆ†æè§†é¢‘å¸§ - GPUåŠ é€Ÿ
        
        Args:
            frames: PIL Imageå¸§åˆ—è¡¨
            video_path: è§†é¢‘è·¯å¾„ï¼ˆç”¨äºé”™è¯¯æŠ¥å‘Šï¼‰
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if self.model is None or not frames:
            raise ValueError("LLaVAæ¨¡å‹æœªåˆå§‹åŒ–æˆ–æ²¡æœ‰å¸§å¯åˆ†æ")
        
        try:
            logger.info(f"ğŸ” æ­£åœ¨ä½¿ç”¨LLaVAåˆ†æ{len(frames)}å¸§...")
            
            # å‡†å¤‡è¾“å…¥
            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.ghost_probing_prompt},
                        *[{"type": "image"} for _ in frames]
                    ],
                }
            ]
            
            # å¤„ç†è¾“å…¥
            prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)
            inputs = self.processor(
                text=prompt, 
                images=frames, 
                return_tensors="pt"
            )
            
            # å°†è¾“å…¥ç§»åˆ°GPU
            if torch.cuda.is_available():
                inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            # ç”Ÿæˆå“åº” - åœ¨GPUä¸Šè¿›è¡Œæ¨ç†
            with torch.no_grad():
                with torch.cuda.amp.autocast():  # ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿ
                    output = self.model.generate(
                        **inputs,
                        max_new_tokens=512,
                        do_sample=True,
                        temperature=0.3,
                        top_p=0.9,
                        use_cache=True
                    )
            
            # è§£ç è¾“å‡º
            generated_ids = output[0][inputs['input_ids'].shape[1]:]
            generated_text = self.processor.decode(generated_ids, skip_special_tokens=True)
            
            logger.info(f"ğŸ¤– LLaVAå“åº”: {generated_text[:200]}...")
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # å¯»æ‰¾JSONéƒ¨åˆ†
                json_start = generated_text.find('{')
                json_end = generated_text.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    json_str = generated_text[json_start:json_end]
                    result = json.loads(json_str)
                    logger.info("âœ… æˆåŠŸè§£æLLaVAçš„JSONå“åº”")
                    return result
                else:
                    return self._parse_text_response(generated_text)
            except json.JSONDecodeError:
                logger.warning("âš ï¸  LLaVAå“åº”ä¸æ˜¯æœ‰æ•ˆJSONï¼Œä½¿ç”¨æ–‡æœ¬è§£æ")
                return self._parse_text_response(generated_text)
                
        except Exception as e:
            logger.error(f"âŒ LLaVAåˆ†æå¤±è´¥: {e}")
            raise
    
    def _parse_text_response(self, text: str) -> Dict:
        """è§£ææ–‡æœ¬å“åº”ä¸ºç»“æ„åŒ–æ•°æ®"""
        text_lower = text.lower()
        
        # æ£€æµ‹é¬¼æ¢å¤´å…³é”®è¯
        ghost_detected = any(keyword in text_lower for keyword in [
            'ghost probing', 'sudden appearance', 'emergency braking', 
            'collision risk', 'immediate threat', 'extremely close'
        ])
        
        # æ£€æµ‹é«˜ç¡®ä¿¡åº¦æŒ‡æ ‡
        high_confidence = any(keyword in text_lower for keyword in [
            'extremely close', 'immediate', 'emergency', '<3 meters', 'sudden'
        ])
        
        # ä¼°ç®—ç½®ä¿¡åº¦
        if ghost_detected and high_confidence:
            confidence = 0.85
            ghost_type = 'high_confidence'
            risk_level = 'high'
        elif ghost_detected:
            confidence = 0.65
            ghost_type = 'potential'
            risk_level = 'medium'
        else:
            confidence = 0.25
            ghost_type = 'none'
            risk_level = 'low'
        
        return {
            "ghost_probing_detected": "yes" if ghost_detected else "no",
            "confidence": confidence,
            "ghost_type": ghost_type,
            "summary": text[:200] + "..." if len(text) > 200 else text,
            "key_actions": "LLaVAæ–‡æœ¬åˆ†æç»“æœ",
            "risk_level": risk_level,
            "distance_estimate": "åˆ†æä¸­ç¡®å®š",
            "emergency_action_needed": "yes" if ghost_detected else "no"
        }
    
    def process_video(self, video_path: str) -> Dict:
        """
        å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ£€æµ‹ç»“æœ
        """
        video_name = Path(video_path).stem
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_name}")
        
        start_time = datetime.now()
        
        try:
            # æå–çœŸå®è§†é¢‘å¸§
            frames = self.extract_frames_with_decord(video_path, num_frames=8)
            
            if not frames:
                raise ValueError("æ— æ³•æå–è§†é¢‘å¸§")
            
            # ä½¿ç”¨LLaVAåˆ†æ - GPUåŠ é€Ÿ
            analysis_result = self.analyze_frames_with_llava(frames, video_path)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            result = {
                "video_id": video_name,
                "video_path": str(video_path),
                "ghost_probing_label": analysis_result.get("ghost_probing_detected", "no"),
                "confidence": analysis_result.get("confidence", 0.5),
                "ghost_type": analysis_result.get("ghost_type", "none"),
                "summary": analysis_result.get("summary", ""),
                "key_actions": analysis_result.get("key_actions", ""),
                "risk_level": analysis_result.get("risk_level", "low"),
                "emergency_action_needed": analysis_result.get("emergency_action_needed", "no"),
                "model": "LLaVA-NeXT-Video-7B-DPO",
                "timestamp": datetime.now().isoformat(),
                "processing_time": processing_time,
                "method": "real_llava_gpu_analysis",
                "frames_analyzed": len(frames),
                "device": self.device
            }
            
            logger.info(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {video_name} ({processing_time:.1f}s) - é¬¼æ¢å¤´: {result['ghost_probing_label']}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è§†é¢‘{video_name}å¤±è´¥: {e}")
            return {
                "video_id": video_name,
                "video_path": str(video_path),
                "error": str(e),
                "processing_time": (datetime.now() - start_time).total_seconds()
            }

def main():
    """ä¸»å‡½æ•° - å¤„ç†100ä¸ªDADAè§†é¢‘"""
    print("ğŸš€ å¼€å§‹çœŸæ­£çš„LLaVAé¬¼æ¢å¤´æ£€æµ‹ï¼ˆGPUåŠ é€Ÿç‰ˆï¼‰...")
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ“Š GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    
    # è·å–è§†é¢‘æ•°æ®è·¯å¾„
    azureml_data_path = os.environ.get('AZUREML_DATAREFERENCE_video_data')
    
    possible_paths = []
    if azureml_data_path:
        possible_paths.append(azureml_data_path)
        print(f"ğŸ”§ ä»ç¯å¢ƒå˜é‡æ‰¾åˆ°æ•°æ®è·¯å¾„: {azureml_data_path}")
    
    possible_paths.extend([
        "./inputs/video_data", 
        "./inputs",
        "."
    ])
    
    video_files = []
    video_folder = None
    
    for path in possible_paths:
        try:
            p = Path(path)
            if p.exists():
                found_videos = list(p.glob("**/*.avi"))
                if found_videos:
                    video_files = found_videos[:100]  # é™åˆ¶100ä¸ª
                    video_folder = p
                    print(f"âœ… åœ¨ {path} æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
                    break
                else:
                    print(f"âš ï¸  è·¯å¾„ {path} å­˜åœ¨ä½†æ²¡æœ‰.aviæ–‡ä»¶")
        except Exception as e:
            print(f"âŒ æ£€æŸ¥è·¯å¾„ {path} æ—¶å‡ºé”™: {e}")
    
    if not video_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡ŒçœŸå®åˆ†æ")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("./outputs/results", exist_ok=True)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = RealLLaVADetector()
    
    print(f"ğŸ¬ å¼€å§‹å¤„ç† {len(video_files)} ä¸ªè§†é¢‘...")
    
    # å¤„ç†è§†é¢‘
    results = []
    failed_count = 0
    
    for i, video_file in enumerate(video_files):
        try:
            result = detector.process_video(str(video_file))
            results.append(result)
            
            if 'error' in result:
                failed_count += 1
            
            if (i + 1) % 10 == 0:
                success_count = (i + 1) - failed_count
                print(f"ğŸ“Š å¤„ç†è¿›åº¦: {i+1}/{len(video_files)} ({(i+1)/len(video_files)*100:.1f}%) - æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è§†é¢‘ {video_file} å¤±è´¥: {e}")
            failed_count += 1
            results.append({
                "video_id": Path(video_file).stem,
                "video_path": str(video_file),
                "error": str(e),
                "processing_time": 0
            })
    
    print("ğŸ’¾ ä¿å­˜ç»“æœæ–‡ä»¶...")
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
    successful_results = [r for r in results if 'error' not in r]
    
    # JSONæ ¼å¼ç»“æœ
    json_file = f"./outputs/results/real_llava_gpu_results_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'model': 'LLaVA-NeXT-Video-7B-DPO',
                'device': 'GPU' if torch.cuda.is_available() else 'CPU',
                'prompt_version': 'balanced_gpt41_compatible',
                'total_videos': len(video_files),
                'successful_videos': len(successful_results),
                'failed_videos': failed_count,
                'timestamp': timestamp,
                'video_folder': str(video_folder) if video_folder else 'not_found'
            },
            'results': results
        }, f, indent=2, ensure_ascii=False)
    
    # CSVæ ¼å¼ç»“æœï¼ˆä»…æˆåŠŸçš„ç»“æœï¼‰
    if successful_results:
        csv_file = f"./outputs/results/real_llava_gpu_results_{timestamp}.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write('video_id,ghost_probing_label,confidence,ghost_type,risk_level,processing_time,method\n')
            for r in successful_results:
                f.write(f"{r['video_id']},{r['ghost_probing_label']},{r['confidence']:.3f},{r['ghost_type']},{r['risk_level']},{r['processing_time']:.1f},{r['method']}\n")
    
    # ç»Ÿè®¡ä¿¡æ¯
    if successful_results:
        ghost_count = len([r for r in successful_results if r['ghost_probing_label'] == 'yes'])
        normal_count = len(successful_results) - ghost_count
        detection_rate = (ghost_count / len(successful_results)) * 100
        avg_processing_time = sum(r['processing_time'] for r in successful_results) / len(successful_results)
    else:
        ghost_count = normal_count = detection_rate = avg_processing_time = 0
    
    summary = {
        'total_videos': len(video_files),
        'successful_processed': len(successful_results),
        'failed_processed': failed_count,
        'ghost_probing_detected': ghost_count,
        'normal_videos': normal_count,
        'detection_rate_percent': round(detection_rate, 2),
        'average_processing_time': round(avg_processing_time, 2),
        'timestamp': timestamp,
        'device': 'GPU' if torch.cuda.is_available() else 'CPU',
        'files_generated': [json_file] + ([csv_file] if successful_results else [])
    }
    
    summary_file = f"./outputs/results/real_llava_gpu_summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print("=" * 60)
    print("ğŸ‰ çœŸæ­£çš„LLaVAé¬¼æ¢å¤´æ£€æµ‹å®Œæˆ (GPUåŠ é€Ÿ)!")
    print("=" * 60)
    print(f"ğŸ“Š æ€»è§†é¢‘æ•°: {len(video_files)}")
    print(f"âœ… æˆåŠŸå¤„ç†: {len(successful_results)}")
    print(f"âŒ å¤„ç†å¤±è´¥: {failed_count}")
    if successful_results:
        print(f"ğŸš¨ é¬¼æ¢å¤´æ£€æµ‹: {ghost_count} ({detection_rate:.1f}%)")
        print(f"ğŸ“ˆ æ­£å¸¸è§†é¢‘: {normal_count} ({100-detection_rate:.1f}%)")
        print(f"â±ï¸  å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.1f}ç§’/è§†é¢‘")
        print(f"ğŸ“Š CSVæ–‡ä»¶: {csv_file}")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {json_file}")
    print(f"ğŸ“‹ ç»Ÿè®¡æ–‡ä»¶: {summary_file}")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}")
    print("=" * 60)

if __name__ == "__main__":
    main()