#!/usr/bin/env python3
"""æµ‹è¯•LLaVAæ¨¡å‹æ˜¯å¦å¯ç”¨"""

import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_llava_availability():
    """æµ‹è¯•LLaVAæ¨¡å‹å¯ç”¨æ€§"""
    
    print("ğŸ”§ æµ‹è¯•LLaVAæ¨¡å‹å¯ç”¨æ€§...")
    
    try:
        from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
        
        print("âœ… LLaVA transformerså¯å¯¼å…¥")
        
        # æµ‹è¯•åŠ è½½å°æ¨¡å‹
        model_id = "llava-hf/llava-v1.6-mistral-7b-hf"
        
        print(f"ğŸ“¥ å°è¯•åŠ è½½æ¨¡å‹: {model_id}")
        
        processor = LlavaNextProcessor.from_pretrained(model_id)
        print("âœ… ProcessoråŠ è½½æˆåŠŸ")
        
        model = LlavaNextForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("âœ… ModelåŠ è½½æˆåŠŸ")
        
        print("ğŸ‰ çœŸæ­£çš„LLaVAæ¨¡å‹å¯ç”¨!")
        return True
        
    except Exception as e:
        print(f"âŒ LLaVAæ¨¡å‹ä¸å¯ç”¨: {e}")
        return False

def test_clip_fallback():
    """æµ‹è¯•CLIPå›é€€"""
    
    try:
        from transformers import CLIPProcessor, CLIPModel
        
        processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        
        print("âœ… CLIPå›é€€æ¨¡å¼å¯ç”¨")
        return True
        
    except Exception as e:
        print(f"âŒ CLIPä¹Ÿä¸å¯ç”¨: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ æ¨¡å‹å¯ç”¨æ€§æµ‹è¯•")
    print("=" * 50)
    
    llava_ok = test_llava_availability()
    if not llava_ok:
        print("\nğŸ”„ æµ‹è¯•CLIPå›é€€...")
        clip_ok = test_clip_fallback()
        
        if not clip_ok:
            print("âŒ æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨")
        else:
            print("âš ï¸  å°†ä½¿ç”¨CLIPå›é€€æ¨¡å¼")
    
    print("=" * 50)