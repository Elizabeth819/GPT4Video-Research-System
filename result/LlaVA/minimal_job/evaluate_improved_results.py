#!/usr/bin/env python3
"""
è¯„ä¼°æ”¹è¿›ç‰ˆLLaVAæ£€æµ‹ç»“æœ
å¯¹æ¯”ground truthè®¡ç®—æ€§èƒ½æŒ‡æ ‡
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np

def load_ground_truth(csv_path: str) -> Dict[str, bool]:
    """åŠ è½½ground truthæ ‡ç­¾"""
    try:
        df = pd.read_csv(csv_path, sep='\t')
        print(f"ğŸ“Š è¯»å–ground truthæ–‡ä»¶: {csv_path}")
        
        gt_labels = {}
        for _, row in df.iterrows():
            video_id = str(row['video_id']).replace('.avi', '')
            label = str(row['ground_truth_label']).lower()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºé¬¼æ¢å¤´
            has_ghost_probing = (
                'ghost probing' in label or 
                'ghost' in label or
                ('s:' in label and 'none' not in label and 'cut-in' not in label)
            )
            gt_labels[video_id] = has_ghost_probing
            
        print(f"ğŸ“Š åŠ è½½äº† {len(gt_labels)} ä¸ªground truthæ ‡ç­¾")
        return gt_labels
        
    except Exception as e:
        print(f"âŒ åŠ è½½ground truthå¤±è´¥: {e}")
        return {}

def load_improved_results(json_path: str) -> List[Dict]:
    """åŠ è½½æ”¹è¿›ç‰ˆæ£€æµ‹ç»“æœ"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        results = data.get('results', [])
        print(f"ğŸ“Š åŠ è½½äº† {len(results)} ä¸ªæ”¹è¿›ç‰ˆæ£€æµ‹ç»“æœ")
        
        return results
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ£€æµ‹ç»“æœå¤±è´¥: {e}")
        return []

def compare_with_baseline(improved_results: List[Dict], baseline_results: List[Dict]) -> Dict:
    """å¯¹æ¯”æ”¹è¿›ç‰ˆä¸åŸºçº¿ç‰ˆæœ¬çš„ç»“æœ"""
    
    # åˆ›å»ºè§†é¢‘IDåˆ°ç»“æœçš„æ˜ å°„
    improved_map = {r['video_id']: r for r in improved_results}
    baseline_map = {r['video_id']: r for r in baseline_results}
    
    comparison = {
        'total_videos': len(improved_results),
        'changes': [],
        'detection_changes': {
            'baseline_detected': 0,
            'improved_detected': 0,
            'newly_detected': [],
            'no_longer_detected': []
        }
    }
    
    for video_id in improved_map:
        if video_id in baseline_map:
            improved = improved_map[video_id]
            baseline = baseline_map[video_id]
            
            improved_detected = improved.get('ghost_probing_label') == 'yes'
            baseline_detected = baseline.get('ghost_probing_label') == 'yes'
            
            if baseline_detected:
                comparison['detection_changes']['baseline_detected'] += 1
            if improved_detected:
                comparison['detection_changes']['improved_detected'] += 1
            
            # æ£€æµ‹å˜åŒ–
            if improved_detected and not baseline_detected:
                comparison['detection_changes']['newly_detected'].append({
                    'video_id': video_id,
                    'improved_confidence': improved.get('confidence', 0),
                    'baseline_confidence': baseline.get('confidence', 0)
                })
            elif baseline_detected and not improved_detected:
                comparison['detection_changes']['no_longer_detected'].append({
                    'video_id': video_id,
                    'improved_confidence': improved.get('confidence', 0),
                    'baseline_confidence': baseline.get('confidence', 0)
                })
            
            # è®°å½•è¯¦ç»†å˜åŒ–
            comparison['changes'].append({
                'video_id': video_id,
                'baseline_detected': baseline_detected,
                'improved_detected': improved_detected,
                'baseline_confidence': baseline.get('confidence', 0),
                'improved_confidence': improved.get('confidence', 0),
                'confidence_change': improved.get('confidence', 0) - baseline.get('confidence', 0),
                'max_frame_change': improved.get('temporal_analysis', {}).get('max_change', 0)
            })
    
    return comparison

def evaluate_improved_performance(gt_labels: Dict[str, bool], detection_results: List[Dict]) -> Dict:
    """è¯„ä¼°æ”¹è¿›ç‰ˆæ£€æµ‹æ€§èƒ½"""
    
    # å‡†å¤‡è¯„ä¼°æ•°æ®
    matched_results = []
    
    for result in detection_results:
        video_id = result.get('video_id', '').replace('.avi', '')
        
        if video_id in gt_labels:
            # æ£€æµ‹ç»“æœ
            detected = result.get('ghost_probing_label', 'no').lower() == 'yes'
            confidence = result.get('confidence', 0.0)
            
            # Ground truth
            ground_truth = gt_labels[video_id]
            
            # æ—¶åºåˆ†ææ•°æ®
            temporal_analysis = result.get('temporal_analysis', {})
            detection_scores = result.get('detection_scores', {})
            
            matched_results.append({
                'video_id': video_id,
                'ground_truth': ground_truth,
                'detected': detected,
                'confidence': confidence,
                'max_frame_change': temporal_analysis.get('max_change', 0),
                'avg_frame_change': temporal_analysis.get('mean_change', 0),
                'sudden_changes': temporal_analysis.get('sudden_change_count', 0),
                'continuous_regions': len(temporal_analysis.get('continuous_change_regions', [])),
                'detection_scores': detection_scores,
                'thresholds_used': result.get('thresholds_used', {})
            })
    
    print(f"ğŸ“Š åŒ¹é…åˆ° {len(matched_results)} ä¸ªæœ‰ground truthçš„æ£€æµ‹ç»“æœ")
    
    if not matched_results:
        return {"error": "æ²¡æœ‰åŒ¹é…çš„ç»“æœ"}
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tp = sum(1 for r in matched_results if r['ground_truth'] and r['detected'])
    tn = sum(1 for r in matched_results if not r['ground_truth'] and not r['detected'])
    fp = sum(1 for r in matched_results if not r['ground_truth'] and r['detected'])
    fn = sum(1 for r in matched_results if r['ground_truth'] and not r['detected'])
    
    total = len(matched_results)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    accuracy = (tp + tn) / total if total > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # åˆ†ææ£€æµ‹æ¡ˆä¾‹
    true_positives = [r for r in matched_results if r['ground_truth'] and r['detected']]
    false_positives = [r for r in matched_results if not r['ground_truth'] and r['detected']]
    false_negatives = [r for r in matched_results if r['ground_truth'] and not r['detected']]
    true_negatives = [r for r in matched_results if not r['ground_truth'] and not r['detected']]
    
    results = {
        'total_videos': total,
        'confusion_matrix': {
            'true_positive': tp,
            'true_negative': tn,
            'false_positive': fp,
            'false_negative': fn
        },
        'performance_metrics': {
            'accuracy': round(accuracy, 4),
            'precision': round(precision, 4),
            'recall': round(recall, 4),
            'specificity': round(specificity, 4),
            'f1_score': round(f1_score, 4)
        },
        'detailed_analysis': {
            'true_positives': true_positives,
            'false_positives': false_positives,
            'false_negatives': false_negatives,
            'true_negatives': true_negatives
        },
        'threshold_analysis': {
            'avg_tp_confidence': np.mean([r['confidence'] for r in true_positives]) if true_positives else 0,
            'avg_fp_confidence': np.mean([r['confidence'] for r in false_positives]) if false_positives else 0,
            'avg_tp_max_change': np.mean([r['max_frame_change'] for r in true_positives]) if true_positives else 0,
            'avg_fp_max_change': np.mean([r['max_frame_change'] for r in false_positives]) if false_positives else 0,
        }
    }
    
    return results

def print_improved_evaluation_report(eval_results: Dict, comparison: Dict = None):
    """æ‰“å°æ”¹è¿›ç‰ˆè¯„ä¼°æŠ¥å‘Š"""
    
    print("\n" + "="*80)
    print("ğŸš€ æ”¹è¿›ç‰ˆLLaVAé¬¼æ¢å¤´æ£€æµ‹æ€§èƒ½è¯„ä¼°æŠ¥å‘Š")
    print("="*80)
    
    if 'error' in eval_results:
        print(f"âŒ è¯„ä¼°é”™è¯¯: {eval_results['error']}")
        return
    
    # åŸºæœ¬ä¿¡æ¯
    total = eval_results['total_videos']
    cm = eval_results['confusion_matrix']
    metrics = eval_results['performance_metrics']
    threshold_analysis = eval_results['threshold_analysis']
    
    print(f"ğŸ“Š è¯„ä¼°è§†é¢‘æ€»æ•°: {total}")
    print()
    
    # æ··æ·†çŸ©é˜µ
    print("ğŸ“‹ æ··æ·†çŸ©é˜µ:")
    print(f"   çœŸé˜³æ€§ (TP): {cm['true_positive']:3d}  |  å‡é˜³æ€§ (FP): {cm['false_positive']:3d}")
    print(f"   å‡é˜´æ€§ (FN): {cm['false_negative']:3d}  |  çœŸé˜´æ€§ (TN): {cm['true_negative']:3d}")
    print()
    
    # æ€§èƒ½æŒ‡æ ‡
    print("ğŸ¯ æ€§èƒ½æŒ‡æ ‡:")
    print(f"   å‡†ç¡®ç‡ (Accuracy):  {metrics['accuracy']:.1%}")
    print(f"   ç²¾ç¡®åº¦ (Precision): {metrics['precision']:.1%}")
    print(f"   å¬å›ç‡ (Recall):    {metrics['recall']:.1%}")
    print(f"   ç‰¹å¼‚æ€§ (Specificity): {metrics['specificity']:.1%}")
    print(f"   F1åˆ†æ•° (F1-Score):  {metrics['f1_score']:.1%}")
    print()
    
    # é˜ˆå€¼åˆ†æ
    print("ğŸ” æ£€æµ‹ç½®ä¿¡åº¦åˆ†æ:")
    print(f"   çœŸé˜³æ€§å¹³å‡ç½®ä¿¡åº¦: {threshold_analysis['avg_tp_confidence']:.3f}")
    print(f"   å‡é˜³æ€§å¹³å‡ç½®ä¿¡åº¦: {threshold_analysis['avg_fp_confidence']:.3f}")
    print(f"   çœŸé˜³æ€§å¹³å‡å¸§å˜åŒ–: {threshold_analysis['avg_tp_max_change']:.4f}")
    print(f"   å‡é˜³æ€§å¹³å‡å¸§å˜åŒ–: {threshold_analysis['avg_fp_max_change']:.4f}")
    print()
    
    # è¯¦ç»†æ¡ˆä¾‹åˆ†æ
    detailed = eval_results['detailed_analysis']
    
    if detailed['true_positives']:
        print("âœ… æ­£ç¡®æ£€æµ‹å‡ºçš„é¬¼æ¢å¤´:")
        for tp in detailed['true_positives']:
            print(f"   - {tp['video_id']}: ç½®ä¿¡åº¦={tp['confidence']:.3f}, æœ€å¤§å˜åŒ–={tp['max_frame_change']:.4f}")
    
    if detailed['false_positives']:
        print("\nâŒ è¯¯æŠ¥çš„æ­£å¸¸è§†é¢‘:")
        for fp in detailed['false_positives']:
            print(f"   - {fp['video_id']}: ç½®ä¿¡åº¦={fp['confidence']:.3f}, æœ€å¤§å˜åŒ–={fp['max_frame_change']:.4f}")
    
    if detailed['false_negatives']:
        print("\nâš ï¸  æ¼æ£€çš„é¬¼æ¢å¤´:")
        for fn in detailed['false_negatives']:
            print(f"   - {fn['video_id']}: ç½®ä¿¡åº¦={fn['confidence']:.3f}, æœ€å¤§å˜åŒ–={fn['max_frame_change']:.4f}")
    print()
    
    # å¯¹æ¯”åˆ†æ
    if comparison:
        print("ğŸ“ˆ æ”¹è¿›æ•ˆæœåˆ†æ:")
        baseline_detected = comparison['detection_changes']['baseline_detected']
        improved_detected = comparison['detection_changes']['improved_detected']
        newly_detected = len(comparison['detection_changes']['newly_detected'])
        no_longer_detected = len(comparison['detection_changes']['no_longer_detected'])
        
        print(f"   åŸºçº¿ç‰ˆæœ¬æ£€æµ‹æ•°: {baseline_detected}")
        print(f"   æ”¹è¿›ç‰ˆæœ¬æ£€æµ‹æ•°: {improved_detected}")
        print(f"   æ–°å¢æ£€æµ‹: {newly_detected} ä¸ª")
        print(f"   ä¸å†æ£€æµ‹: {no_longer_detected} ä¸ª")
        
        if comparison['detection_changes']['newly_detected']:
            print("   æ–°å¢æ£€æµ‹çš„è§†é¢‘:")
            for item in comparison['detection_changes']['newly_detected']:
                print(f"     - {item['video_id']}: ç½®ä¿¡åº¦ {item['baseline_confidence']:.3f} â†’ {item['improved_confidence']:.3f}")
    
    print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¯„ä¼°æ”¹è¿›ç‰ˆLLaVAæ£€æµ‹ç»“æœ...")
    
    # æ–‡ä»¶è·¯å¾„
    gt_file = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/groundtruth_labels.csv"
    improved_results_file = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/LlaVA/minimal_job/improved_results/artifacts/outputs/results/improved_llava_results_20250722_025127.json"
    baseline_results_file = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/LlaVA/minimal_job/strict_validation_results/artifacts/outputs/results/strict_llava_results_20250722_021252.json"
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not Path(gt_file).exists():
        print(f"âŒ Ground truthæ–‡ä»¶ä¸å­˜åœ¨: {gt_file}")
        return
    
    if not Path(improved_results_file).exists():
        print(f"âŒ æ”¹è¿›ç‰ˆç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {improved_results_file}")
        return
    
    # åŠ è½½æ•°æ®
    gt_labels = load_ground_truth(gt_file)
    if not gt_labels:
        print("âŒ æ— æ³•åŠ è½½ground truthæ ‡ç­¾")
        return
    
    improved_results = load_improved_results(improved_results_file)
    if not improved_results:
        print("âŒ æ— æ³•åŠ è½½æ”¹è¿›ç‰ˆæ£€æµ‹ç»“æœ")
        return
    
    # åŠ è½½åŸºçº¿ç»“æœè¿›è¡Œå¯¹æ¯”
    comparison = None
    if Path(baseline_results_file).exists():
        try:
            with open(baseline_results_file, 'r', encoding='utf-8') as f:
                baseline_data = json.load(f)
            baseline_results = baseline_data.get('results', [])
            if baseline_results:
                comparison = compare_with_baseline(improved_results, baseline_results)
                print(f"ğŸ“Š åŠ è½½äº†åŸºçº¿ç»“æœè¿›è¡Œå¯¹æ¯”: {len(baseline_results)} ä¸ª")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åŠ è½½åŸºçº¿ç»“æœ: {e}")
    
    # è¯„ä¼°æ€§èƒ½
    eval_results = evaluate_improved_performance(gt_labels, improved_results)
    
    # æ‰“å°æŠ¥å‘Š
    print_improved_evaluation_report(eval_results, comparison)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = "improved_evaluation_report.json"
    report_data = {
        'evaluation_results': eval_results,
        'comparison_with_baseline': comparison,
        'timestamp': '20250722_025127'
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()