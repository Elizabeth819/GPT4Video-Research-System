#!/usr/bin/env python3
"""
GPUæ˜¾å­˜ç›‘æ§ç‰ˆLLaVAæ£€æµ‹å™¨
ç›‘æ§æ•´ä¸ªå¤„ç†è¿‡ç¨‹ä¸­çš„GPUæ˜¾å­˜æ¶ˆè€—
"""

import json
import os
import gc
from pathlib import Path
from datetime import datetime
from PIL import Image
import logging
import torch
from typing import List, Dict
import numpy as np
import time

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_gpu_memory_info():
    """è·å–GPUæ˜¾å­˜ä¿¡æ¯ (MB)"""
    if torch.cuda.is_available():
        return {
            'allocated': torch.cuda.memory_allocated() / 1024 / 1024,  # MB
            'reserved': torch.cuda.memory_reserved() / 1024 / 1024,   # MB
            'max_allocated': torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
        }
    return {'allocated': 0, 'reserved': 0, 'max_allocated': 0}

def log_memory_usage(stage: str):
    """è®°å½•å½“å‰é˜¶æ®µçš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    mem_info = get_gpu_memory_info()
    logger.info(f"ğŸ–¥ï¸  [{stage}] GPUæ˜¾å­˜ - å·²åˆ†é…: {mem_info['allocated']:.1f}MB, "
                f"å·²ä¿ç•™: {mem_info['reserved']:.1f}MB, å³°å€¼: {mem_info['max_allocated']:.1f}MB")
    return mem_info

class GPUMemoryLLaVADetector:
    """å¸¦GPUæ˜¾å­˜ç›‘æ§çš„LLaVAæ£€æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.memory_logs = []  # è®°å½•æ˜¾å­˜ä½¿ç”¨å†å²
        
        log_memory_usage("åˆå§‹åŒ–å¼€å§‹")
        self._initialize_model()
        log_memory_usage("åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–GPUæ˜¾å­˜ç›‘æ§ç‰ˆæ¨¡å‹...")
            logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
            
            if torch.cuda.is_available():
                # é‡ç½®GPUæ˜¾å­˜ç»Ÿè®¡
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.empty_cache()
                log_memory_usage("ç¼“å­˜æ¸…ç†å")
            
            # ä½¿ç”¨CLIPè¿›è¡Œå›¾åƒç¼–ç 
            from transformers import CLIPProcessor, CLIPModel
            
            logger.info("ğŸ“¥ æ­£åœ¨åŠ è½½CLIPæ¨¡å‹...")
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            log_memory_usage("CLIPæ¨¡å‹åŠ è½½å")
            
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            log_memory_usage("CLIPå¤„ç†å™¨åŠ è½½å")
            
            # ç§»åˆ°GPU
            if torch.cuda.is_available():
                self.clip_model = self.clip_model.cuda()
                log_memory_usage("æ¨¡å‹ç§»åˆ°GPUå")
                logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU: {torch.cuda.get_device_name()}")
            else:
                logger.warning("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def extract_frames_with_memory_monitor(self, video_path: str, num_frames: int = 8) -> List[Image.Image]:
        """å¸¦æ˜¾å­˜ç›‘æ§çš„å¸§æå–"""
        
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {Path(video_path).name}")
        mem_start = log_memory_usage("è§†é¢‘å¤„ç†å¼€å§‹")
        
        try:
            import decord
            from decord import VideoReader
            
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
                log_memory_usage("æ¸…ç†å")
            
            # è®¾ç½®decord
            decord.bridge.set_bridge('native')
            
            # è¯»å–è§†é¢‘
            logger.info("ğŸ“– æ­£åœ¨è¯»å–è§†é¢‘...")
            video_reader = VideoReader(str(video_path))
            total_frames = len(video_reader)
            log_memory_usage("è§†é¢‘è¯»å–å")
            
            logger.info(f"ğŸ“Š æ€»å¸§æ•°: {total_frames}")
            
            # å‡åŒ€åˆ†å¸ƒé€‰æ‹©å¸§
            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
            logger.info(f"ğŸ“Š é€‰æ‹©å¸§ç´¢å¼•: {frame_indices.tolist()}")
            
            # æå–å¸§
            frames = []
            for i, idx in enumerate(frame_indices):
                logger.info(f"ğŸ” æå–ç¬¬ {i+1}/{num_frames} å¸§ (ç´¢å¼•: {idx})")
                
                # è·å–å¸§å‰æ˜¾å­˜çŠ¶æ€
                mem_before = log_memory_usage(f"å¸§{i+1}æå–å‰")
                
                # è·å–å¸§
                frame = video_reader[idx]
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                if hasattr(frame, 'asnumpy'):
                    frame_array = frame.asnumpy()
                elif isinstance(frame, torch.Tensor):
                    frame_array = frame.cpu().numpy()
                else:
                    frame_array = np.array(frame)
                
                # è½¬æ¢ä¸ºPIL Image
                pil_image = Image.fromarray(frame_array.astype(np.uint8))
                frames.append(pil_image)
                
                # è·å–å¸§åæ˜¾å­˜çŠ¶æ€
                mem_after = log_memory_usage(f"å¸§{i+1}æå–å")
                
                # è®°å½•æ˜¾å­˜å˜åŒ–
                mem_change = mem_after['allocated'] - mem_before['allocated']
                if abs(mem_change) > 0.1:  # åªè®°å½•æ˜¾è‘—å˜åŒ–
                    logger.info(f"  ğŸ“Š å¸§{i+1}æ˜¾å­˜å˜åŒ–: {mem_change:+.1f}MB")
                
                # æ¸…ç†ä¸´æ—¶å¯¹è±¡
                del frame, frame_array
                gc.collect()
            
            mem_end = log_memory_usage("å¸§æå–å®Œæˆ")
            total_mem_change = mem_end['allocated'] - mem_start['allocated']
            logger.info(f"ğŸ“Š è§†é¢‘å¤„ç†æ€»æ˜¾å­˜å˜åŒ–: {total_mem_change:+.1f}MB")
            
            return frames
            
        except Exception as e:
            logger.error(f"âŒ å¸§æå–å¤±è´¥: {e}")
            raise
    
    def analyze_frames_with_memory_monitor(self, frames: List[Image.Image]) -> Dict:
        """å¸¦æ˜¾å­˜ç›‘æ§çš„å¸§åˆ†æ"""
        if not frames:
            raise ValueError("æ²¡æœ‰å¸§å¯åˆ†æ")
        
        try:
            logger.info(f"ğŸ” å¼€å§‹åˆ†æ{len(frames)}å¸§...")
            mem_start = log_memory_usage("CLIPåˆ†æå¼€å§‹")
            
            # CLIPå¤„ç†
            logger.info("ğŸ“ CLIPé¢„å¤„ç†...")
            inputs = self.clip_processor(images=frames, return_tensors="pt", padding=True)
            mem_after_preprocess = log_memory_usage("CLIPé¢„å¤„ç†å")
            
            if torch.cuda.is_available():
                inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
                mem_after_cuda = log_memory_usage("æ•°æ®ç§»åˆ°GPUå")
            
            # CLIPæ¨ç†
            logger.info("ğŸ§  CLIPç‰¹å¾æå–...")
            with torch.no_grad():
                image_features = self.clip_model.get_image_features(**inputs)
            
            mem_after_inference = log_memory_usage("CLIPæ¨ç†å")
            
            # åˆ†æå¸§é—´å˜åŒ–
            feature_changes = []
            for i in range(1, len(frames)):
                diff = torch.cosine_similarity(
                    image_features[i-1].unsqueeze(0),
                    image_features[i].unsqueeze(0)
                ).item()
                feature_changes.append(1 - diff)
            
            max_change = max(feature_changes) if feature_changes else 0
            avg_change = sum(feature_changes) / len(feature_changes) if feature_changes else 0
            
            mem_final = log_memory_usage("åˆ†æå®Œæˆ")
            
            # æ¸…ç†GPUæ˜¾å­˜
            del inputs, image_features
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            mem_after_cleanup = log_memory_usage("æ˜¾å­˜æ¸…ç†å")
            
            # åˆ¤æ–­é€»è¾‘
            if max_change > 0.5:
                ghost_detected = "yes"
                confidence = min(0.9, max_change)
                ghost_type = "high_confidence" if max_change > 0.7 else "potential"
                risk_level = "high" if max_change > 0.7 else "medium"
            elif max_change > 0.3:
                ghost_detected = "yes"
                confidence = max_change * 0.7
                ghost_type = "potential"
                risk_level = "medium"
            else:
                ghost_detected = "no"
                confidence = 0.2
                ghost_type = "none"
                risk_level = "low"
            
            # æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡
            memory_stats = {
                'start_allocated': mem_start['allocated'],
                'peak_allocated': mem_after_inference['allocated'],
                'final_allocated': mem_final['allocated'],
                'cleanup_allocated': mem_after_cleanup['allocated'],
                'peak_reserved': mem_after_inference['reserved'],
                'memory_increase': mem_after_inference['allocated'] - mem_start['allocated'],
                'memory_freed': mem_final['allocated'] - mem_after_cleanup['allocated']
            }
            
            result = {
                "ghost_probing_detected": ghost_detected,
                "confidence": round(confidence, 3),
                "ghost_type": ghost_type,
                "summary": f"GPUæ˜¾å­˜ç›‘æ§åˆ†æå®Œæˆï¼Œæœ€å¤§å¸§é—´å˜åŒ–: {max_change:.4f}",
                "key_actions": f"ç›‘æ§ç‰ˆæ£€æµ‹{len(frames)}å¸§ï¼ŒCLIPç‰¹å¾åˆ†æ",
                "risk_level": risk_level,
                "emergency_action_needed": "yes" if ghost_detected == "yes" else "no",
                "max_frame_change": round(max_change, 4),
                "avg_frame_change": round(avg_change, 4),
                "memory_stats": memory_stats
            }
            
            logger.info(f"ğŸ“Š æ˜¾å­˜å³°å€¼: {memory_stats['peak_allocated']:.1f}MB")
            logger.info(f"ğŸ“Š æ˜¾å­˜å¢é•¿: {memory_stats['memory_increase']:+.1f}MB")
            logger.info(f"âœ… åˆ†æå®Œæˆ - é¬¼æ¢å¤´: {ghost_detected} (ç½®ä¿¡åº¦: {confidence:.3f})")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
            raise
    
    def process_video(self, video_path: str) -> Dict:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        video_name = Path(video_path).stem
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_name}")
        
        start_time = datetime.now()
        mem_initial = log_memory_usage("è§†é¢‘å¤„ç†å¼€å§‹")
        
        try:
            # æå–è§†é¢‘å¸§
            frames = self.extract_frames_with_memory_monitor(video_path, num_frames=8)
            
            if not frames:
                raise ValueError("æ— æ³•æå–è§†é¢‘å¸§")
            
            # åˆ†æå¸§
            analysis_result = self.analyze_frames_with_memory_monitor(frames)
            
            # æœ€ç»ˆæ˜¾å­˜çŠ¶æ€
            mem_final = log_memory_usage("è§†é¢‘å¤„ç†å®Œæˆ")
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # æ„å»ºç»“æœ
            result = {
                "video_id": video_name,
                "video_path": str(video_path),
                "ghost_probing_label": analysis_result.get("ghost_probing_detected", "no"),
                "confidence": analysis_result.get("confidence", 0.5),
                "ghost_type": analysis_result.get("ghost_type", "none"),
                "summary": analysis_result.get("summary", ""),
                "key_actions": analysis_result.get("key_actions", ""),
                "risk_level": analysis_result.get("risk_level", "low"),
                "emergency_action_needed": analysis_result.get("emergency_action_needed", "no"),
                "model": "CLIP-GPU-Memory-Monitor",
                "timestamp": datetime.now().isoformat(),
                "processing_time": processing_time,
                "frames_analyzed": len(frames),
                "device": self.device,
                "max_frame_change": analysis_result.get("max_frame_change", 0),
                "avg_frame_change": analysis_result.get("avg_frame_change", 0),
                "memory_stats": analysis_result.get("memory_stats", {}),
                "total_memory_change": mem_final['allocated'] - mem_initial['allocated']
            }
            
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {video_name} ({processing_time:.2f}s)")
            logger.info(f"ğŸ“Š è§†é¢‘æ€»æ˜¾å­˜å˜åŒ–: {result['total_memory_change']:+.1f}MB")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {video_name} - {e}")
            return {
                "video_id": video_name,
                "video_path": str(video_path),
                "error": str(e),
                "processing_time": (datetime.now() - start_time).total_seconds()
            }

def main():
    """GPUæ˜¾å­˜ç›‘æ§ä¸»å‡½æ•°"""
    print("ğŸ–¥ï¸  GPUæ˜¾å­˜ç›‘æ§ç‰ˆLLaVAé¬¼æ¢å¤´æ£€æµ‹")
    print("=" * 60)
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ“Š GPUæ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 / 1024:.0f}MB")
        log_memory_usage("ç¨‹åºå¼€å§‹")
    else:
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        return
    
    # è·å–è§†é¢‘è·¯å¾„
    azureml_data_path = os.environ.get('AZUREML_DATAREFERENCE_video_data')
    
    possible_paths = []
    if azureml_data_path:
        possible_paths.append(azureml_data_path)
        print(f"ğŸ”§ ä»ç¯å¢ƒå˜é‡æ‰¾åˆ°æ•°æ®è·¯å¾„: {azureml_data_path}")
    
    possible_paths.extend(["./inputs/video_data", "./inputs", "."])
    
    video_files = []
    for path in possible_paths:
        try:
            p = Path(path)
            if p.exists():
                found_videos = list(p.glob("**/*.avi"))
                if found_videos:
                    # åªå–å‰3ä¸ªè§†é¢‘è¿›è¡Œæ˜¾å­˜ç›‘æ§æµ‹è¯•
                    video_files = found_videos[:3]
                    print(f"âœ… åœ¨ {path} æ‰¾åˆ° {len(found_videos)} ä¸ªè§†é¢‘ï¼Œç›‘æ§å‰ {len(video_files)} ä¸ª")
                    break
        except Exception as e:
            print(f"âŒ æ£€æŸ¥è·¯å¾„ {path} æ—¶å‡ºé”™: {e}")
    
    if not video_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("./outputs/results", exist_ok=True)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = GPUMemoryLLaVADetector()
    
    print(f"ğŸ¬ å¼€å§‹GPUæ˜¾å­˜ç›‘æ§å¤„ç† {len(video_files)} ä¸ªè§†é¢‘...")
    print("=" * 60)
    
    # å¤„ç†è§†é¢‘
    results = []
    
    for i, video_file in enumerate(video_files):
        try:
            print(f"\nğŸ“¹ å¤„ç†è§†é¢‘ {i+1}/{len(video_files)}: {Path(video_file).name}")
            result = detector.process_video(str(video_file))
            results.append(result)
            
            # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æ‘˜è¦
            if 'memory_stats' in result:
                mem_stats = result['memory_stats']
                print(f"ğŸ“Š å³°å€¼æ˜¾å­˜: {mem_stats.get('peak_allocated', 0):.1f}MB")
                print(f"ğŸ“Š æ˜¾å­˜å¢é•¿: {mem_stats.get('memory_increase', 0):+.1f}MB")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è§†é¢‘å¤±è´¥: {e}")
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜GPUæ˜¾å­˜ç›‘æ§ç»“æœ...")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    memory_result = {
        'metadata': {
            'model': 'CLIP-GPU-Memory-Monitor',
            'device': 'GPU',
            'gpu_name': torch.cuda.get_device_name() if torch.cuda.is_available() else 'N/A',
            'total_gpu_memory': torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 if torch.cuda.is_available() else 0,
            'total_videos': len(video_files),
            'timestamp': timestamp,
            'test_type': 'gpu_memory_monitoring'
        },
        'results': results
    }
    
    json_file = f"./outputs/results/gpu_memory_llava_results_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(memory_result, f, indent=2, ensure_ascii=False)
    
    print("=" * 60)
    print("ğŸ‰ GPUæ˜¾å­˜ç›‘æ§å®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ“Š æµ‹è¯•è§†é¢‘æ•°: {len(video_files)}")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {json_file}")
    print("=" * 60)

if __name__ == "__main__":
    main()