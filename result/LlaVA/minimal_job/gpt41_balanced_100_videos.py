#!/usr/bin/env python3
"""
GPT-4.1å¹³è¡¡ç‰ˆ100è§†é¢‘é¬¼æ¢å¤´æ£€æµ‹
å¤„ç†images_1_001åˆ°images_5_xxxçš„æ‰€æœ‰100ä¸ªè§†é¢‘
ä½¿ç”¨ä¸GPT-4.1ç›¸åŒçš„promptå’Œæ ¼å¼
"""

import json
import os
import sys
from pathlib import Path
from datetime import datetime
import logging
import time
import hashlib
from typing import Dict, List, Optional
import requests
import base64
from PIL import Image
import cv2
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GPT41BalancedDetector:
    """GPT-4.1å¹³è¡¡ç‰ˆé¬¼æ¢å¤´æ£€æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        # Azure OpenAIé…ç½® (ä½¿ç”¨ç¯å¢ƒå˜é‡)
        self.api_key = os.getenv('AZURE_VISION_KEY', 'placeholder-key')
        self.endpoint = os.getenv('VISION_ENDPOINT', 'https://placeholder.openai.azure.com/')
        self.deployment_name = os.getenv('VISION_DEPLOYMENT_NAME', 'gpt-4o-vision')
        self.api_version = "2024-02-15-preview"
        
        # é…ç½®å‚æ•° (ä¸GPT-4.1ä¿æŒä¸€è‡´)
        self.frame_interval = 10  # æ¯æ®µ10ç§’
        self.frames_per_interval = 10  # æ¯æ®µ10å¸§
        self.max_tokens = 2000
        self.temperature = 0
        self.max_retry_attempts = 3
        
        self.processed_videos = []
        
        logger.info("ğŸ”§ GPT-4.1å¹³è¡¡ç‰ˆæ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š é…ç½®: {self.frames_per_interval}å¸§/{self.frame_interval}ç§’, æ¸©åº¦={self.temperature}")
    
    def encode_image_to_base64(self, image_path: str) -> str:
        """å°†å›¾åƒç¼–ç ä¸ºbase64"""
        try:
            with open(image_path, 'rb') as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            logger.error(f"âŒ å›¾åƒç¼–ç å¤±è´¥ {image_path}: {e}")
            return ""
    
    def extract_video_frames(self, video_path: str) -> List[str]:
        """æå–è§†é¢‘å…³é”®å¸§"""
        logger.info(f"ğŸ¬ æå–è§†é¢‘å¸§: {Path(video_path).name}")
        
        try:
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = Path("./temp_frames")
            temp_dir.mkdir(exist_ok=True)
            
            # ä½¿ç”¨OpenCVè¯»å–è§†é¢‘
            cap = cv2.VideoCapture(str(video_path))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            logger.info(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.2f}fps, {duration:.2f}ç§’")
            
            # è®¡ç®—éœ€è¦æå–çš„å¸§ç´¢å¼•
            if duration <= self.frame_interval:
                # è§†é¢‘çŸ­äºé—´éš”ï¼Œæå–æ‰€æœ‰å…³é”®å¸§
                frame_indices = np.linspace(0, total_frames - 1, min(self.frames_per_interval, total_frames), dtype=int)
            else:
                # è§†é¢‘è¾ƒé•¿ï¼Œæå–å‰frame_intervalç§’çš„å¸§
                target_frames = int(fps * self.frame_interval)
                frame_indices = np.linspace(0, min(target_frames - 1, total_frames - 1), self.frames_per_interval, dtype=int)
            
            # æå–å¸§
            frame_paths = []
            for i, frame_idx in enumerate(frame_indices):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                
                if ret:
                    frame_path = temp_dir / f"frame_{i:03d}.jpg"
                    cv2.imwrite(str(frame_path), frame)
                    frame_paths.append(str(frame_path))
                    
                    # éªŒè¯å¸§è´¨é‡
                    if frame.mean() < 10:  # æ£€æŸ¥æ˜¯å¦ä¸ºé»‘å¸§
                        logger.warning(f"âš ï¸  å¸§ {i} å¯èƒ½ä¸ºé»‘å¸§")
            
            cap.release()
            logger.info(f"âœ… æˆåŠŸæå– {len(frame_paths)} å¸§")
            
            return frame_paths
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘å¸§æå–å¤±è´¥: {e}")
            return []
    
    def create_balanced_gpt41_prompt(self, video_id: str, trans: str = "æ— éŸ³é¢‘") -> str:
        """åˆ›å»ºå¹³è¡¡ç‰ˆGPT-4.1 prompt"""
        
        system_content = f"""You are VideoAnalyzerGPT analyzing a series of SEQUENTIAL images taken from a video, where each image represents a consecutive moment in time. Focus on the changes in the relative positions, distances, and speeds of objects, particularly the car in front and self vehicle, and how these might indicate a potential need for braking or collision avoidance. Based on the sequence of images, predict the next action that the observer vehicle should take.

Your job is to take in as an input a transcription of {self.frame_interval} seconds of audio from a video,
as well as {self.frames_per_interval} frames split evenly throughout {self.frame_interval} seconds.
You are to generate and provide a Current Action Summary of the video you are considering ({self.frames_per_interval}
frames over {self.frame_interval} seconds), which is generated from your analysis of each frame ({self.frames_per_interval} in total),
as well as the in-between audio, until we have a full action summary of the video.

IMPORTANT: For ghost probing detection, consider TWO categories:

**1. HIGH-CONFIDENCE Ghost Probing (use "ghost probing" in key_actions)**:
- Object appears EXTREMELY close (within 1-2 vehicle lengths, <3 meters) 
- Appearance is SUDDEN and from blind spots (behind parked cars, buildings, corners)
- Occurs in HIGH-RISK environments: highways, rural roads, parking lots, uncontrolled intersections
- Requires IMMEDIATE emergency braking/swerving to avoid collision
- Movement is COMPLETELY UNPREDICTABLE and violates traffic expectations

**2. POTENTIAL Ghost Probing (use "potential ghost probing" in key_actions)**:
- Object appears suddenly but at moderate distance (3-5 meters)
- Sudden movement in environments where some unpredictability exists
- Requires emergency braking but collision risk is moderate
- Movement is unexpected but not completely impossible given the context

**3. NORMAL Traffic Situations (do NOT use "ghost probing")**:
- Pedestrians crossing at intersections, crosswalks, or traffic lights
- Vehicles making normal lane changes, turns, or merging with signals
- Cyclists following predictable paths in urban areas or bike lanes
- Any movement that is EXPECTED given the traffic environment and context

**Environment Context Guidelines**:
- INTERSECTION/CROSSWALK: Expect pedestrians and cyclists - use "emergency braking due to pedestrian crossing"
- HIGHWAY/RURAL: Higher chance of genuine ghost probing - be more sensitive
- PARKING LOT: Expect sudden vehicle movements - use "potential ghost probing" if very sudden
- URBAN STREET: Mixed - consider visibility and predictability

Use "ghost probing" for clear cases, "potential ghost probing" for borderline cases, and descriptive terms for normal traffic situations.

Your response should be a valid JSON object with the following EXACT structure (match this format precisely):
{{
    "video_id": "{video_id}",
    "segment_id": "segment_1",
    "Start_Timestamp": "0.0s",
    "End_Timestamp": "{self.frame_interval}.0s",
    "sentiment": "Positive/Negative/Neutral",
    "scene_theme": "Dramatic/Routine/Dangerous/Safe",
    "characters": "brief description of people in the scene",
    "summary": "comprehensive summary of the scene and what happens",
    "actions": "actions taken by the vehicle and driver responses",
    "key_objects": "numbered list: 1) Position: object description, distance, behavior impact 2) Position: object description, distance, behavior impact",
    "key_actions": "brief description of most important actions (use 'ghost probing', 'potential ghost probing', or descriptive terms as appropriate)",
    "next_action": {{
        "speed_control": "rapid deceleration/deceleration/maintain speed/acceleration",
        "direction_control": "keep direction/turn left/turn right",
        "lane_control": "maintain current lane/change left/change right"
    }}
}}

Audio Transcription: {trans}"""

        return system_content
    
    def call_gpt41_vision_api(self, system_prompt: str, frame_paths: List[str]) -> Optional[Dict]:
        """è°ƒç”¨GPT-4.1 Vision API"""
        
        try:
            # å‡†å¤‡å›¾åƒæ•°æ®
            images_data = []
            for frame_path in frame_paths:
                base64_image = self.encode_image_to_base64(frame_path)
                if base64_image:
                    images_data.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    })
            
            if not images_data:
                logger.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å›¾åƒæ•°æ®")
                return None
            
            # æ„å»ºè¯·æ±‚
            headers = {
                "Content-Type": "application/json",
                "api-key": self.api_key
            }
            
            # æ¶ˆæ¯æ ¼å¼
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user", 
                    "content": images_data
                }
            ]
            
            payload = {
                "messages": messages,
                "max_tokens": self.max_tokens,
                "temperature": self.temperature,
                "stream": False
            }
            
            # APIè°ƒç”¨
            url = f"{self.endpoint}/openai/deployments/{self.deployment_name}/chat/completions?api-version={self.api_version}"
            
            logger.info(f"ğŸ” è°ƒç”¨GPT-4.1 API: {len(images_data)}å¼ å›¾åƒ")
            start_time = time.time()
            
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            
            api_time = time.time() - start_time
            logger.info(f"â±ï¸  APIè°ƒç”¨æ—¶é—´: {api_time:.2f}ç§’")
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                
                # å°è¯•è§£æJSON
                try:
                    # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
                    if '```json' in content:
                        content = content.split('```json')[1].split('```')[0].strip()
                    elif '```' in content:
                        content = content.split('```')[1].strip()
                    
                    parsed_result = json.loads(content)
                    logger.info("âœ… GPT-4.1 åˆ†æå®Œæˆ")
                    return parsed_result
                    
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                    logger.error(f"åŸå§‹å“åº”: {content[:500]}...")
                    return None
            else:
                logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                logger.error(f"é”™è¯¯ä¿¡æ¯: {response.text[:500]}...")
                return None
                
        except Exception as e:
            logger.error(f"âŒ GPT-4.1 APIè°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def process_single_video(self, video_path: str) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        video_name = Path(video_path).stem
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {video_name}")
        
        start_time = time.time()
        
        try:
            # 1. æå–è§†é¢‘å¸§
            frame_paths = self.extract_video_frames(video_path)
            if not frame_paths:
                logger.error(f"âŒ æ— æ³•æå–è§†é¢‘å¸§: {video_name}")
                return None
            
            # 2. åˆ›å»ºprompt
            system_prompt = self.create_balanced_gpt41_prompt(video_name)
            
            # 3. è°ƒç”¨GPT-4.1 API
            result = None
            for attempt in range(self.max_retry_attempts):
                logger.info(f"ğŸ”„ å°è¯• {attempt + 1}/{self.max_retry_attempts}")
                result = self.call_gpt41_vision_api(system_prompt, frame_paths)
                
                if result:
                    break
                    
                if attempt < self.max_retry_attempts - 1:
                    wait_time = 2 ** attempt
                    logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
            
            # 4. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for frame_path in frame_paths:
                try:
                    Path(frame_path).unlink()
                except:
                    pass
            
            processing_time = time.time() - start_time
            
            if result:
                # æ·»åŠ å¤„ç†å…ƒæ•°æ®
                result.update({
                    'processing_time': round(processing_time, 2),
                    'model': 'GPT-4.1-Balanced',
                    'timestamp': datetime.now().isoformat(),
                    'frames_analyzed': len(frame_paths),
                    'api_config': {
                        'frame_interval': self.frame_interval,
                        'frames_per_interval': self.frames_per_interval,
                        'temperature': self.temperature,
                        'max_tokens': self.max_tokens
                    }
                })
                
                logger.info(f"âœ… å¤„ç†å®Œæˆ: {video_name} ({processing_time:.2f}s)")
                return result
            else:
                logger.error(f"âŒ å¤„ç†å¤±è´¥: {video_name}")
                return None
                
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ å¤„ç†å¼‚å¸¸: {video_name} - {e} ({processing_time:.2f}s)")
            return None
    
    def process_100_videos(self, video_folder: str) -> List[Dict]:
        """å¤„ç†100ä¸ªè§†é¢‘"""
        
        video_folder_path = Path(video_folder)
        if not video_folder_path.exists():
            logger.error(f"âŒ è§†é¢‘æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {video_folder}")
            return []
        
        # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘æ–‡ä»¶ (images_1_001 åˆ° images_5_xxx)
        video_files = []
        for pattern in ["images_1_*.avi", "images_2_*.avi", "images_3_*.avi", "images_4_*.avi", "images_5_*.avi"]:
            video_files.extend(list(video_folder_path.glob(pattern)))
        
        video_files.sort()  # ç¡®ä¿é¡ºåº
        
        if not video_files:
            logger.error(f"âŒ æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {video_folder}")
            return []
        
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        logger.info(f"ğŸ“Š èŒƒå›´: {video_files[0].name} åˆ° {video_files[-1].name}")
        
        # å¤„ç†æ‰€æœ‰è§†é¢‘
        results = []
        failed_count = 0
        
        print("=" * 80)
        print("ğŸš€ GPT-4.1å¹³è¡¡ç‰ˆ100è§†é¢‘é¬¼æ¢å¤´æ£€æµ‹")
        print("=" * 80)
        print(f"ğŸ“Š æ€»è§†é¢‘æ•°: {len(video_files)}")
        print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: GPT-4.1 Balanced Prompt")
        print(f"âš™ï¸  é…ç½®: {self.frames_per_interval}å¸§/{self.frame_interval}ç§’")
        print("=" * 80)
        
        for i, video_file in enumerate(video_files):
            print(f"\nğŸ“¹ å¤„ç†è§†é¢‘ {i+1}/{len(video_files)}: {video_file.name}")
            
            result = self.process_single_video(str(video_file))
            
            if result:
                results.append(result)
                
                # æå–å…³é”®ä¿¡æ¯
                ghost_probing = "ghost probing" in result.get('key_actions', '').lower()
                potential_ghost = "potential ghost probing" in result.get('key_actions', '').lower()
                
                if ghost_probing:
                    print(f"ğŸš¨ é«˜ç½®ä¿¡åº¦é¬¼æ¢å¤´æ£€æµ‹")
                elif potential_ghost:
                    print(f"âš ï¸  æ½œåœ¨é¬¼æ¢å¤´æ£€æµ‹") 
                else:
                    print(f"âœ… æ­£å¸¸äº¤é€šåœºæ™¯")
                    
                print(f"ğŸ“Š å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f}s")
                
            else:
                failed_count += 1
                print(f"âŒ å¤„ç†å¤±è´¥")
                
                # åˆ›å»ºå¤±è´¥è®°å½•
                results.append({
                    'video_id': video_file.stem,
                    'error': 'Processing failed',
                    'timestamp': datetime.now().isoformat()
                })
            
            # æ¯10ä¸ªè§†é¢‘ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if (i + 1) % 10 == 0:
                self.save_intermediate_results(results, i + 1)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ 100è§†é¢‘å¤„ç†å®Œæˆ!")
        print("=" * 80)
        print(f"âœ… æˆåŠŸå¤„ç†: {len(results) - failed_count}")
        print(f"âŒ å¤„ç†å¤±è´¥: {failed_count}")
        print(f"ğŸ“Š æˆåŠŸç‡: {((len(results) - failed_count) / len(video_files) * 100):.1f}%")
        
        return results
    
    def save_intermediate_results(self, results: List[Dict], count: int):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"./outputs/results/gpt41_balanced_intermediate_{count}_{timestamp}.json"
            
            os.makedirs("./outputs/results", exist_ok=True)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'metadata': {
                        'model': 'GPT-4.1-Balanced',
                        'processed_count': count,
                        'timestamp': timestamp,
                        'config': {
                            'frame_interval': self.frame_interval,
                            'frames_per_interval': self.frames_per_interval,
                            'temperature': self.temperature
                        }
                    },
                    'results': results
                }, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ GPT-4.1å¹³è¡¡ç‰ˆ100è§†é¢‘é¬¼æ¢å¤´æ£€æµ‹")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env_vars = ['AZURE_VISION_KEY', 'VISION_ENDPOINT', 'VISION_DEPLOYMENT_NAME']
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {missing_vars}")
        print("è¯·è®¾ç½®å¿…è¦çš„Azure OpenAIé…ç½®")
        return
    
    # è·å–è§†é¢‘æ•°æ®è·¯å¾„
    azureml_data_path = os.environ.get('AZUREML_DATAREFERENCE_video_data')
    
    video_folder = None
    if azureml_data_path:
        video_folder = azureml_data_path
        print(f"ğŸ”§ ä»ç¯å¢ƒå˜é‡æ‰¾åˆ°æ•°æ®è·¯å¾„: {azureml_data_path}")
    else:
        # æœ¬åœ°æµ‹è¯•è·¯å¾„
        video_folder = "/Users/wanmeng/repository/GPT4Video-cobra-auto/result/DADA-100-videos"
        if not Path(video_folder).exists():
            print(f"âŒ è§†é¢‘æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {video_folder}")
            return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("./outputs/results", exist_ok=True)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = GPT41BalancedDetector()
    
    # å¤„ç†100ä¸ªè§†é¢‘
    results = detector.process_100_videos(video_folder)
    
    if not results:
        print("âŒ æœªèƒ½å¤„ç†ä»»ä½•è§†é¢‘")
        return
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ç»Ÿè®¡ç»“æœ
    successful_results = [r for r in results if 'error' not in r]
    ghost_probing_count = sum(1 for r in successful_results 
                             if 'ghost probing' in r.get('key_actions', '').lower())
    potential_ghost_count = sum(1 for r in successful_results 
                               if 'potential ghost probing' in r.get('key_actions', '').lower() 
                               and 'ghost probing' not in r.get('key_actions', '').lower())
    
    final_result = {
        'metadata': {
            'model': 'GPT-4.1-Balanced',
            'prompt_version': 'balanced_final',
            'total_videos': len(results),
            'successful_videos': len(successful_results),
            'failed_videos': len(results) - len(successful_results),
            'ghost_probing_detected': ghost_probing_count,
            'potential_ghost_probing_detected': potential_ghost_count,
            'normal_traffic': len(successful_results) - ghost_probing_count - potential_ghost_count,
            'timestamp': timestamp,
            'config': {
                'frame_interval': detector.frame_interval,
                'frames_per_interval': detector.frames_per_interval,
                'temperature': detector.temperature,
                'max_tokens': detector.max_tokens
            }
        },
        'results': results
    }
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    json_file = f"./outputs/results/gpt41_balanced_100_videos_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(final_result, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æœ€ç»ˆç»“æœç»Ÿè®¡:")
    print("=" * 80)
    print(f"ğŸ“Š æ€»è§†é¢‘æ•°: {len(results)}")
    print(f"âœ… æˆåŠŸå¤„ç†: {len(successful_results)}")
    print(f"ğŸš¨ é«˜ç½®ä¿¡åº¦é¬¼æ¢å¤´: {ghost_probing_count}")
    print(f"âš ï¸  æ½œåœ¨é¬¼æ¢å¤´: {potential_ghost_count}")
    print(f"ğŸš— æ­£å¸¸äº¤é€š: {len(successful_results) - ghost_probing_count - potential_ghost_count}")
    print(f"âŒ å¤„ç†å¤±è´¥: {len(results) - len(successful_results)}")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {json_file}")
    print("=" * 80)

if __name__ == "__main__":
    main()