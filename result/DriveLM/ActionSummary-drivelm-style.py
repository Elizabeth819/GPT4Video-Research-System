#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DriveLMé£æ ¼çš„Graph VQA promptå¤„ç†DADA-2000è§†é¢‘
æ¨¡æ‹ŸDriveLMçš„Graph Visual Question Answeringæ–¹æ³•è¿›è¡ŒGhost Probingæ£€æµ‹
"""

import os
import json
import cv2
import base64
import requests
import subprocess
from typing import List, Dict, Any
import logging
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DriveLMStyleProcessor:
    def __init__(self):
        # ä½¿ç”¨OpenAI APIè€Œä¸æ˜¯Azureï¼Œæ›´ç®€å•ç¨³å®š
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.model = "gpt-4o"  # ä½¿ç”¨GPT-4oè¿›è¡Œè§†è§‰åˆ†æ
        self.output_dir = "result/drivelm_style_results"
        os.makedirs(self.output_dir, exist_ok=True)
        
    def get_drivelm_style_prompt(self) -> str:
        """
        è®¾è®¡DriveLMé£æ ¼çš„Graph Visual Question Answering prompt
        åŸºäºDriveLMçš„æ–¹æ³•è®ºï¼šGraphç»“æ„åŒ–æ¨ç† + å¤šæ­¥VQA
        """
        return """You are DriveLM, an advanced Graph Visual Question Answering system for autonomous driving scene understanding.

**GRAPH VQA METHODOLOGY:**
You will analyze driving scenarios through structured graph reasoning, connecting perception, prediction, planning, and behavioral elements.

**GRAPH ELEMENTS TO IDENTIFY:**
1. **EGO VEHICLE**: The autonomous vehicle's current state and trajectory
2. **TRAFFIC PARTICIPANTS**: Vehicles, pedestrians, cyclists in the scene  
3. **ROAD INFRASTRUCTURE**: Lanes, traffic signs, intersections
4. **DYNAMIC RELATIONSHIPS**: Spatial and temporal relationships between elements
5. **RISK FACTORS**: Potential collision points and safety-critical events

**MULTI-STEP REASONING PROCESS:**

**STEP 1: Scene Graph Construction**
Build a structured representation of the current driving scenario identifying all key entities and their relationships.

**STEP 2: Temporal Analysis** 
Analyze the sequence of frames to understand motion patterns and predict future states.

**STEP 3: Risk Assessment**
Evaluate potential safety-critical events, particularly focusing on sudden appearances (ghost probing).

**STEP 4: Graph-based Decision Making**
Use the constructed graph to make reasoned decisions about scene understanding.

**PRIMARY TASK: Ghost Probing Detection**
Ghost probing refers to the sudden appearance of pedestrians, vehicles, or objects that create immediate collision risk for the ego vehicle.

**DETECTION CRITERIA:**
- Sudden appearance within ego vehicle's trajectory
- Objects appearing from blind spots (behind parked cars, buildings, etc.)
- Rapid movement into the vehicle's path
- High collision risk scenarios requiring emergency response

**OUTPUT FORMAT:**
Provide your analysis in this structured format:

```json
{
    "scene_graph": {
        "ego_vehicle": "description of ego vehicle state",
        "traffic_participants": ["list of detected vehicles, pedestrians, etc."],
        "infrastructure": "road layout and traffic elements",
        "relationships": "spatial and temporal relationships"
    },
    "temporal_analysis": {
        "motion_patterns": "observed movement patterns",
        "trajectory_predictions": "predicted future states",
        "scene_evolution": "how the scene changes over time"
    },
    "risk_assessment": {
        "ghost_probing_detected": "YES/NO",
        "risk_level": "LOW/MEDIUM/HIGH/CRITICAL", 
        "risk_factors": ["list of identified risk factors"],
        "collision_probability": "assessment of collision likelihood"
    },
    "graph_reasoning": {
        "key_connections": "important graph relationships",
        "decision_logic": "reasoning process",
        "confidence_level": "HIGH/MEDIUM/LOW"
    },
    "final_decision": {
        "ghost_probing": "YES/NO",
        "explanation": "detailed reasoning for the decision",
        "recommended_action": "suggested vehicle response"
    }
}
```

**IMPORTANT NOTES:**
- Focus on sudden appearances and unexpected movements
- Consider the ego vehicle's trajectory and reaction time
- Evaluate visibility constraints and blind spots
- Prioritize safety-critical event detection
- Use graph-based reasoning to connect multiple evidence sources

Analyze the provided video frames using this Graph VQA methodology and determine if ghost probing occurs."""

    def extract_frames(self, video_path: str, interval: int = 10, max_frames: int = 10) -> List[str]:
        """æå–è§†é¢‘å…³é”®å¸§"""
        logger.info(f"æå–è§†é¢‘å¸§: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            return []
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        logger.info(f"è§†é¢‘ä¿¡æ¯: {duration:.1f}ç§’, {total_frames}å¸§, {fps:.1f}fps")
        
        frame_paths = []
        temp_dir = "frames_temp"
        os.makedirs(temp_dir, exist_ok=True)
        
        # è®¡ç®—é‡‡æ ·é—´éš”
        if duration > 0:
            time_interval = min(interval, duration / max_frames)
            frame_interval = max(1, int(fps * time_interval))
        else:
            frame_interval = max(1, total_frames // max_frames)
        
        frame_count = 0
        extracted_count = 0
        
        while cap.isOpened() and extracted_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
                
            if frame_count % frame_interval == 0:
                frame_path = os.path.join(temp_dir, f"frame_{extracted_count:03d}.jpg")
                cv2.imwrite(frame_path, frame)
                frame_paths.append(frame_path)
                extracted_count += 1
                
            frame_count += 1
        
        cap.release()
        logger.info(f"æå–äº† {len(frame_paths)} å¸§")
        return frame_paths

    def encode_image(self, image_path: str) -> str:
        """å°†å›¾åƒç¼–ç ä¸ºbase64"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def call_vision_api(self, frame_paths: List[str]) -> Dict[str, Any]:
        """è°ƒç”¨OpenAI Vision APIè¿›è¡ŒDriveLMé£æ ¼åˆ†æ"""
        logger.info(f"è°ƒç”¨OpenAI Vision APIåˆ†æ {len(frame_paths)} å¸§")
        
        # å‡†å¤‡æ¶ˆæ¯
        messages = [
            {
                "role": "system",
                "content": self.get_drivelm_style_prompt()
            },
            {
                "role": "user", 
                "content": [
                    {
                        "type": "text",
                        "text": "Please analyze these sequential video frames using Graph VQA methodology to detect ghost probing events."
                    }
                ]
            }
        ]
        
        # æ·»åŠ å›¾åƒ
        for i, frame_path in enumerate(frame_paths):
            base64_image = self.encode_image(frame_path)
            messages[1]["content"].append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            })
        
        # APIè°ƒç”¨
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": 2000,
            "temperature": 0
        }
        
        url = "https://api.openai.com/v1/chat/completions"
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            logger.info("APIè°ƒç”¨æˆåŠŸ")
            return {
                "success": True,
                "content": content,
                "usage": result.get("usage", {})
            }
            
        except Exception as e:
            logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def parse_drivelm_response(self, response_content: str) -> Dict[str, Any]:
        """è§£æDriveLMé£æ ¼çš„å“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            if "```json" in response_content:
                json_start = response_content.find("```json") + 7
                json_end = response_content.find("```", json_start)
                json_content = response_content[json_start:json_end].strip()
            else:
                json_content = response_content
            
            parsed = json.loads(json_content)
            
            # æå–å…³é”®ä¿¡æ¯
            ghost_probing = "NO"
            confidence = "LOW"
            
            if "risk_assessment" in parsed:
                ghost_probing = parsed["risk_assessment"].get("ghost_probing_detected", "NO")
            elif "final_decision" in parsed:
                ghost_probing = parsed["final_decision"].get("ghost_probing", "NO")
                
            if "graph_reasoning" in parsed:
                confidence = parsed["graph_reasoning"].get("confidence_level", "LOW")
            
            return {
                "parsed_response": parsed,
                "ghost_probing_detected": ghost_probing,
                "confidence_level": confidence,
                "success": True
            }
            
        except Exception as e:
            logger.warning(f"JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬åˆ†æ: {e}")
            
            # å›é€€åˆ°æ–‡æœ¬åˆ†æ
            content_lower = response_content.lower()
            
            # æ£€æµ‹å…³é”®è¯
            ghost_indicators = ["ghost probing", "sudden appearance", "collision risk", "emergency"]
            positive_indicators = ["yes", "detected", "critical", "high risk"]
            
            ghost_detected = any(indicator in content_lower for indicator in ghost_indicators)
            positive_response = any(indicator in content_lower for indicator in positive_indicators)
            
            ghost_probing = "YES" if (ghost_detected and positive_response) else "NO"
            
            return {
                "parsed_response": {"raw_content": response_content},
                "ghost_probing_detected": ghost_probing,
                "confidence_level": "MEDIUM",
                "success": False,
                "note": "Fallback text analysis used"
            }

    def process_video(self, video_path: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        video_name = os.path.basename(video_path).replace('.avi', '')
        logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘: {video_name}")
        
        result = {
            "video_id": video_name,
            "video_path": video_path,
            "timestamp": datetime.now().isoformat(),
            "method": "DriveLM_Style_Graph_VQA",
            "status": "processing"
        }
        
        try:
            # æå–å¸§
            frame_paths = self.extract_frames(video_path, interval=10, max_frames=10)
            
            if not frame_paths:
                result.update({
                    "status": "error",
                    "error": "No frames extracted",
                    "ghost_probing_detected": "UNKNOWN"
                })
                return result
            
            result["frames_extracted"] = len(frame_paths)
            
            # è°ƒç”¨API
            api_response = self.call_vision_api(frame_paths)
            
            if not api_response["success"]:
                result.update({
                    "status": "error", 
                    "error": api_response["error"],
                    "ghost_probing_detected": "UNKNOWN"
                })
                return result
            
            # è§£æå“åº”
            parsed = self.parse_drivelm_response(api_response["content"])
            
            result.update({
                "status": "completed",
                "raw_response": api_response["content"],
                "parsed_analysis": parsed["parsed_response"],
                "ghost_probing_detected": parsed["ghost_probing_detected"],
                "confidence_level": parsed["confidence_level"],
                "api_usage": api_response.get("usage", {}),
                "parsing_success": parsed["success"]
            })
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for frame_path in frame_paths:
                try:
                    os.remove(frame_path)
                except:
                    pass
            
            logger.info(f"å®Œæˆå¤„ç†: {video_name} - Ghost Probing: {parsed['ghost_probing_detected']}")
            
        except Exception as e:
            logger.error(f"å¤„ç†è§†é¢‘æ—¶å‡ºé”™: {e}")
            result.update({
                "status": "error",
                "error": str(e),
                "ghost_probing_detected": "UNKNOWN"
            })
        
        return result

    def process_video_list(self, video_list: List[str], start_from: int = 0) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†è§†é¢‘åˆ—è¡¨"""
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(video_list)} ä¸ªè§†é¢‘ï¼Œä»ç¬¬ {start_from} ä¸ªå¼€å§‹")
        
        results = []
        
        for i, video_path in enumerate(video_list[start_from:], start_from):
            logger.info(f"è¿›åº¦: {i+1}/{len(video_list)}")
            
            result = self.process_video(video_path)
            results.append(result)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            output_file = os.path.join(self.output_dir, f"drivelm_style_results_partial.json")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ä¸­é—´ç»“æœå·²ä¿å­˜: {output_file}")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='DriveLMé£æ ¼Ghost Probingæ£€æµ‹')
    parser.add_argument('--folder', default='DADA-2000-videos', help='è§†é¢‘æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--single', help='å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶')
    parser.add_argument('--start-from', type=int, default=0, help='ä»ç¬¬Nä¸ªè§†é¢‘å¼€å§‹å¤„ç†')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†è§†é¢‘æ•°é‡')
    
    args = parser.parse_args()
    
    processor = DriveLMStyleProcessor()
    
    if args.single:
        # å¤„ç†å•ä¸ªè§†é¢‘
        if not os.path.exists(args.single):
            logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.single}")
            return
            
        result = processor.process_video(args.single)
        
        output_file = os.path.join(processor.output_dir, f"single_video_result.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… å•ä¸ªè§†é¢‘å¤„ç†å®Œæˆ: {output_file}")
        
    else:
        # æ‰¹é‡å¤„ç†
        if not os.path.exists(args.folder):
            logger.error(f"è§†é¢‘æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {args.folder}")
            return
        
        # è·å–è§†é¢‘åˆ—è¡¨
        video_files = [f for f in os.listdir(args.folder) 
                      if f.endswith('.avi') and f.startswith('images_')]
        video_files.sort()
        
        if args.limit:
            video_files = video_files[:args.limit]
        
        logger.info(f"æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        # è½¬æ¢ä¸ºå®Œæ•´è·¯å¾„
        video_paths = [os.path.join(args.folder, f) for f in video_files]
        
        # æ‰¹é‡å¤„ç†
        results = processor.process_video_list(video_paths, args.start_from)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_file = os.path.join(processor.output_dir, f"drivelm_style_final_results.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ç»Ÿè®¡ç»“æœ
        completed = sum(1 for r in results if r['status'] == 'completed')
        ghost_detected = sum(1 for r in results 
                           if r.get('ghost_probing_detected') == 'YES')
        
        print(f"\nğŸ“Š DriveLMé£æ ¼å¤„ç†ç»“æœ:")
        print(f"  æ€»è§†é¢‘æ•°: {len(results)}")
        print(f"  æˆåŠŸå¤„ç†: {completed}")
        print(f"  æ£€æµ‹åˆ°Ghost Probing: {ghost_detected}")
        print(f"  æ£€æµ‹ç‡: {ghost_detected/completed*100:.1f}%" if completed > 0 else "N/A")
        print(f"  ç»“æœä¿å­˜åœ¨: {output_file}")

if __name__ == "__main__":
    main()